# -*- coding: utf-8 -*-
"""Twitter Sentimental Analysis_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GV7b7VOXsOIAyMRK62bnHdKPNr4DCaDW
"""

pip install pyspark

# Commented out IPython magic to ensure Python compatibility.
!pip install ipython-autotime
# %load_ext autotime

"""**Importing Libraries**"""

import tweepy
from textblob import TextBlob
from wordcloud import WordCloud
import pandas as pd
import numpy as np
import re
import string

from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover
from pyspark.sql import SparkSession
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import json, nltk
import seaborn as sns

"""# **STREAMING TWEETS**

**Credentials**
"""

consumer_key = 'zfARqZoJyVLxbNThLR6u5NnyG'
consumer_secret = '8UZNalbKOtNB1Sa4652JxHzAKK7iMODxXmuAYrKns2YuO2efiq'
access_token = '1388116161955565571-kisxSWseogrcpsTipfyJN1TlpCm8Cz'
access_secret = 'hLRwyaP7pdqoU9c8iC3DQfiwRd9SCMoCNsLRoOAf7C1AE'

"""**Authentication**"""

authenticate = tweepy.OAuthHandler(consumer_key, consumer_secret)
authenticate.set_access_token(access_token, access_secret) 
api = tweepy.API(authenticate, wait_on_rate_limit = True)

"""**Extracting Tweets**"""

posts = api.user_timeline(screen_name = "BillGates", count = 1000, lang = "en", tweet_mode = "extended" )

print("Five latest tweets: \n")
i = 1
for tweet in posts[0:5]:
  print(str(i) + ')' + tweet.full_text + '\n')
  i = i + 1

"""**Loading Tweets into a DataFrame**"""

df = pd.DataFrame([tweet.full_text for tweet in posts], columns=['Tweets'])
df.head()

"""**Filtering / Cleaning / Preprocessing the Tweets**"""

def prepTxt(text):
  text = re.sub(r'@[A-Za-z0-9]+', '', text)
  text = re.sub(r'#','', text)
  text = re.sub(r'RT[\s]+', '', text)
  text = re.sub(r'https?:\/\/\S+', '', text)
  text = re.sub(r':', '', text)

  return text

df['Tweets'] = df['Tweets'].apply(prepTxt)

df

"""**Getting Subjectivity & Polarity of the given Tweets**"""

def generateSubjectivity(text):
  return TextBlob(text).sentiment.subjectivity

def generatePolarity(text):
  return TextBlob(text).sentiment.polarity 

df['Subjectivity'] = df['Tweets'].apply(generateSubjectivity)
df['Polarity'] = df['Tweets'].apply(generatePolarity)

df

"""**Generating a WordCloud**"""

frequentWords = ''.join([twts for twts in df['Tweets']])
wordCloud = WordCloud(width = 600, height=400, random_state  = 0, max_font_size = 110).generate(frequentWords)

plt.imshow(wordCloud, interpolation = "bilinear")
plt.axis('off')
plt.show()

"""**Segregating Postive(1) & Negative(0) tweets based on the Polarity Analysis**"""

def generateAnalysis(score):
  if score < 0:
    return '-1'
  elif score == 0:
    return '0'
  else:
    return '1'

df['Analysis'] = df['Polarity'].apply(generateAnalysis)
df

"""**Sorting Values by Polarity**"""

sortedDF = df.sort_values(by=['Polarity'])

"""**Printing Positive, Negative and Neutral Tweets**"""

j = 1
for i in range (0, sortedDF.shape[0]):
  if(sortedDF['Analysis'][i] == '1'):
    #print(str(j) + ')'+sortedDF['Tweets'][i])
    #print()
    j = j+1

j = 1
for i in range (0, sortedDF.shape[0]):
  if(sortedDF['Analysis'][i] == '-1'):
    #print(str(j) + ')'+sortedDF['Tweets'][i])
    #print()
    j = j+1

j = 1
for i in range (0, sortedDF.shape[0]):
  if(sortedDF['Analysis'][i] == '0'):
    #print(str(j) + ')'+sortedDF['Tweets'][i])
    #print()
    j = j+1

"""**Generating a Scatter Plot for Polarity vs Subjectivity**"""

plt.figure(figsize=(8,6))
for i in range (0, df.shape[0]):
  plt.scatter(df['Polarity'][i], df['Subjectivity'][i], color = 'Blue')

plt.title('Sentiment Analysis')
plt.xlabel('Polarity')
plt.ylabel('Subjectivity') 
plt.show()

"""**Getting the Percentage of Positive, Negative and Neutral Tweets**"""

posTweets = df[df.Analysis == '1']
posTweets = posTweets['Tweets']

(posTweets.shape[0] / df.shape[0]) * 100

negTweets = df[df.Analysis == '-1']
negTweets = negTweets['Tweets']

(negTweets.shape[0] / df.shape[0]) * 100

neuTweets = df[df.Analysis == '0']
neuTweets = neuTweets['Tweets']

(neuTweets.shape[0] / df.shape[0])*100

"""**Generating a Histogram for the Sentiments**"""

df['Analysis'].value_counts()
plt.title('Sentiment Analysis')
plt.xlabel('sentiment')
plt.ylabel('Counts')
df['Analysis'].value_counts().plot(kind='bar')
plt.show()

df1 = df
df1

def cleanVal(text):
  text = re.sub(r'-1', '0', text)
  return text

df1['Analysis'] = df1['Analysis'].apply(cleanVal)

df1

"""**Saving the dataframe as a .CSV**"""

df1.to_csv('bill_gates.csv')

"""# **Building a Machine Learning Classification model via Spark for Sentimental Analysis of Future Tweets using the Current One's Collected**

**Creating a Spark Session**
"""

appName = "Sentiment Analysis in Spark"
spark = SparkSession.builder.appName(appName).config("spark.some.config.option", "some-value").getOrCreate()

"""**Uploading the Data that is to be Trained and Tested**"""

tweets_csv = spark.read.csv('training.csv', inferSchema=True, header=True)
tweets_csv.show(truncate=False, n=3)

"""**Extracting the necessary Independant and Dependant Variables**"""

data = tweets_csv.select("Tweets", col("Analysis").cast("Int").alias("sentiment"))
data.show(truncate = False,n=5)

"""**Training & Testing Split**"""

dividedData = data.randomSplit([0.3, 0.7]) 
trainingData = dividedData[0] 
testingData = dividedData[1] 
train_rows = trainingData.count()
test_rows = testingData.count()
print ("Training data rows:", train_rows, "; Testing data rows:", test_rows)

"""**Preporcessing the Training Data**"""

tokenizer = Tokenizer(inputCol="Tweets", outputCol="SentimentWords")
tokenizedTrain = tokenizer.transform(trainingData).filter("sentiment IS NOT NULL")
tokenizedTrain.show()

swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol="MeaningfulWords")
SwRemovedTrain = swr.transform(tokenizedTrain)
SwRemovedTrain.show(truncate=False, n=5)

hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol="features")
numericTrainData = hashTF.transform(SwRemovedTrain).select(
    'sentiment', 'MeaningfulWords', 'features') #.filter("sentiment IS NOT NULL")
numericTrainData.show(truncate=False, n=20)
#numericTrainData1 = numericTrainData.filter("sentiment IS NOT NULL")

"""**Applying Logistic Regression to Train our Data**"""

lr = LogisticRegression(labelCol="sentiment", featuresCol="features", maxIter=10, regParam=0.01)
model = lr.fit(numericTrainData)
print ("Training is done!") # 5.3 (90), 3.79(70), 3.2(50), 2.93(30)

"""**Prepearing the Test Data**"""

tokenizedTest = tokenizer.transform(testingData).filter("sentiment IS NOT NULL")
SwRemovedTest = swr.transform(tokenizedTest)
numericTest = hashTF.transform(SwRemovedTest).select(
    'Sentiment', 'MeaningfulWords', 'features')
numericTest.show(truncate=False, n=2)

"""**Generating the Accuracy Score of our Model**"""

prediction = model.transform(numericTest)
predictionFinal = prediction.select(
    "MeaningfulWords", "prediction", "Sentiment")
predictionFinal.show(n=4, truncate = False)
correctPrediction = predictionFinal.filter(
    predictionFinal['prediction'] == predictionFinal['Sentiment']).count()
totalData = predictionFinal.count()
print("correct prediction:", correctPrediction, ", total data:", totalData, ", accuracy:", (correctPrediction/totalData)*100,"%")

"""**Naive Bayes (Spark)**"""

from pyspark.mllib.classification import NaiveBayes
import shutil

"""**Training & Testing**"""

model=NaiveBayes.train(train, 5.0)
predictionAndLabel=test.map(lambda x: (x.label, model.predict(x.features)))
accuracy=1.0*predictionAndLabel.filter(lambda x: x[0]==x[1]).count()/test.count()
print("Model Accuracy is ", accuracy)

"""# **Building a Machine Learning Classification model via Scikit Learn for Sentimental Analysis of Future Tweets using the Current One's Collected**"""

total_data = pd.read_csv("bill_gates.csv")
#total_data

tweet = total_data.columns.values[1]
sentiment = total_data.columns.values[4]
tweet, sentiment

#total_data.info()

"""# **Preprocessing**

**Define a function which handles emoji classifications**
"""

def emoji(tweet):
    
    tweet = re.sub(r'(:\s?\)|:-\)|\(\s?:|\(-:|:\'\)|:O)', ' positiveemoji ', tweet)
    tweet = re.sub(r'(:\s?D|:-D|x-?D|X-?D)', ' positiveemoji ', tweet)
    tweet = re.sub(r'(<3|:\*)', ' positiveemoji ', tweet)
    tweet = re.sub(r'(;-?\)|;-?D|\(-?;|@-\))', ' positiveemoji ', tweet)
    tweet = re.sub(r'(:\s?\(|:-\(|\)\s?:|\)-:|:-/|:-\|)', ' negetiveemoji ', tweet)
    tweet = re.sub(r'(:,\(|:\'\(|:"\()', ' negetiveemoji ', tweet)
    return tweet

"""**Define a function which will preprocess the tweets**"""

import re

def process_tweet(tweet):
    tweet = tweet.lower()                                             
    tweet = re.sub('@[^\s]+', '', tweet)                              
    tweet = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', ' ', tweet)   
    tweet = re.sub(r"\d+", " ", str(tweet))                         
    tweet = re.sub('&quot;'," ", tweet)                              
    tweet = emoji(tweet)                                              
    tweet = re.sub(r"\b[a-zA-Z]\b", "", str(tweet))                   
    #for word in tweet.split():
        #if word.lower() in contractions:
            #tweet = tweet.replace(word, contractions[word.lower()])   
    tweet = re.sub(r"[^\w\s]", " ", str(tweet))                       
    tweet = re.sub(r'(.)\1+', r'\1\1', tweet)                      
    tweet = re.sub(r"\s+", " ", str(tweet))                              
    return tweet

"""**Stemming**"""

from nltk.stem.porter import *
stemmer = PorterStemmer()

tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])
tokenized_tweet.head()

"""**Lemmatization**"""

from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

tokenized_tweet = tokenized_tweet.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])
tokenized_tweet.head()

"""**Now make a new column for side by side comparison of new tweets vs old**"""

total_data['processed_tweet'] = np.vectorize(process_tweet)(total_data[tweet])
#total_data.head(10)

""" **Count vectorizer**"""

from sklearn.feature_extraction.text import CountVectorizer

count_vectorizer = CountVectorizer(ngram_range=(1,2))    # Unigram and Bigram
final_vectorized_data = count_vectorizer.fit_transform(total_data['processed_tweet'])  
final_vectorized_data

"""# **Train_Test Split**





"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(final_vectorized_data, total_data[sentiment],test_size=0.3, random_state=69)

"""# **Training_Testing the model.**

**NAIVE BAYES**
"""

from sklearn.naive_bayes import MultinomialNB  

NB = MultinomialNB()
NB.fit(X_train, y_train)

predicted_nb = NB.predict(X_test)

from sklearn.metrics import accuracy_score

score_naive = accuracy_score(predicted_nb, y_test)
print("Accuracy with Naive-bayes: ",score_naive)

"""**SUPPORT VECTOR MACHINE(SVM)**"""

from sklearn.svm import SVC

SVM = SVC(kernel = 'linear', random_state=0)
SVM.fit(X_train, y_train)

predicted_svm = SVM.predict(X_test)

score_svm = accuracy_score(predicted_svm, y_test)
print("Accuracy with SVM: ",score_svm)

"""**DECISION TREE**"""

from sklearn.tree import DecisionTreeClassifier

DT = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
DT.fit(X_train,y_train)

predicted_dt = DT.predict(X_test)

score_dt = accuracy_score(predicted_dt, y_test)
print("Accuracy with Decision_Tree: ",score_dt)