{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PytorchBasics_classifn.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"14d608d909bb4b2e9f4a557ad60bf44c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4002b355594f42d38562b0c68a8bb06b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f5f9f76206464274acb872b745849a01","IPY_MODEL_34e6eed69f4a42358e76ea5c178ce84d","IPY_MODEL_69292a3af3fd411fac34f9bc98e22aec"]}},"4002b355594f42d38562b0c68a8bb06b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f5f9f76206464274acb872b745849a01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aa3b5f4efd09469ea70bc99805e8a381","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4053228330b24fc79188f3dcdc95cf7e"}},"34e6eed69f4a42358e76ea5c178ce84d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7fc7544437e6493fb7ff37cc2b6ae203","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":9912422,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9912422,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e7fbe4d1fa454ed18e0457d4b6ac8152"}},"69292a3af3fd411fac34f9bc98e22aec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_610ac0be7ab84dd8988a4d40db0b62d5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9913344/? [00:00&lt;00:00, 58622658.23it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_20388ee2c6564a78a099165e55808615"}},"aa3b5f4efd09469ea70bc99805e8a381":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4053228330b24fc79188f3dcdc95cf7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7fc7544437e6493fb7ff37cc2b6ae203":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e7fbe4d1fa454ed18e0457d4b6ac8152":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"610ac0be7ab84dd8988a4d40db0b62d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"20388ee2c6564a78a099165e55808615":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6e7f577ea5b94a8a942ba6b70c85ea35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7dc576dbc2d14411b698f9673360a520","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dfcdec0ca07d41abafbcc44eb4439a1a","IPY_MODEL_14c85abd7bef4246a305f54461e01388","IPY_MODEL_111dae9471a6467d8bfca7b2ee075408"]}},"7dc576dbc2d14411b698f9673360a520":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dfcdec0ca07d41abafbcc44eb4439a1a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_67b2eb43ba2b4b76bfcd1e130b43c3b6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ac1824cbd37449d492b746e0e6eca5c9"}},"14c85abd7bef4246a305f54461e01388":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b7927d8c329640c6841f58bf2c1e2981","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_72ad2471de3942928045904055004782"}},"111dae9471a6467d8bfca7b2ee075408":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6e237d2ebd874a758c6c444b676a2e2e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29696/? [00:00&lt;00:00, 1057281.05it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6c96ec10563547f68a0e21d6a1448b1b"}},"67b2eb43ba2b4b76bfcd1e130b43c3b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ac1824cbd37449d492b746e0e6eca5c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b7927d8c329640c6841f58bf2c1e2981":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"72ad2471de3942928045904055004782":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6e237d2ebd874a758c6c444b676a2e2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6c96ec10563547f68a0e21d6a1448b1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e4b4e35bf7754b3a88b2f100f78e6229":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_60dc04698fa24bfebbf2424f8a81e733","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_94cc889fe0f642f2b08205567485e015","IPY_MODEL_dcb898a0cddb48998ee34dcecc99db73","IPY_MODEL_c10f3c1d90e249ffb454c4802b62beab"]}},"60dc04698fa24bfebbf2424f8a81e733":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"94cc889fe0f642f2b08205567485e015":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7959021db93c416da3ab05d6575e9b7d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3825172febb34bb5a5ccd81f7e2e7aee"}},"dcb898a0cddb48998ee34dcecc99db73":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7d2f6db50f0d42d493a2c2e344578392","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1648877,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1648877,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_68939098c18a48e59cc010e917e8ff70"}},"c10f3c1d90e249ffb454c4802b62beab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ac7b73c5c5674e2fb5731034537f50d9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1649664/? [00:00&lt;00:00, 29458539.56it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aa7d79c98e0d4ea6b172e1383ce9c3ec"}},"7959021db93c416da3ab05d6575e9b7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3825172febb34bb5a5ccd81f7e2e7aee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7d2f6db50f0d42d493a2c2e344578392":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"68939098c18a48e59cc010e917e8ff70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac7b73c5c5674e2fb5731034537f50d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"aa7d79c98e0d4ea6b172e1383ce9c3ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"634c1ae8a86047d6b78fc68253d4a71c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5e1e2bea26704bceb268658dee20cb97","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e117ebfefe1b45f08c3ff5cb50009b97","IPY_MODEL_7541a092fe784c548279dad714f28962","IPY_MODEL_9de6d939cb4247a1b545a616e71bc908"]}},"5e1e2bea26704bceb268658dee20cb97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e117ebfefe1b45f08c3ff5cb50009b97":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_81b69f39b4fe447d91b32910cc3028e3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_301a594f45bc49a6b2e783263cc89d38"}},"7541a092fe784c548279dad714f28962":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d17fb150a0ee44d990ff01e2f9a1ab1e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4542,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4542,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c838c42f7bb542ad83deae620c06c4e9"}},"9de6d939cb4247a1b545a616e71bc908":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ad1f284ac9c0486989559d82756f94a3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5120/? [00:00&lt;00:00, 185811.88it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a3a9e752bff945c497e5df0938aeca99"}},"81b69f39b4fe447d91b32910cc3028e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"301a594f45bc49a6b2e783263cc89d38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d17fb150a0ee44d990ff01e2f9a1ab1e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c838c42f7bb542ad83deae620c06c4e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad1f284ac9c0486989559d82756f94a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a3a9e752bff945c497e5df0938aeca99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"4KoygRmrTgT7"},"source":["# Pytorch Lab: Basic Image Classification"]},{"cell_type":"markdown","metadata":{"id":"YG10CCTSW8C-"},"source":["Haohang Li  \n","03/18/2021"]},{"cell_type":"markdown","metadata":{"id":"hRyUEmV1XCSJ"},"source":["Dependencies:"]},{"cell_type":"code","metadata":{"id":"pxpUQKM-XBq4","executionInfo":{"status":"ok","timestamp":1638136771583,"user_tz":300,"elapsed":6195,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}}},"source":["import os\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aNpRpoGuc_71"},"source":["## Colab setting"]},{"cell_type":"markdown","metadata":{"id":"LJanYyxldIxu"},"source":["Because we are gonna to use the gpu instance in this notebook, so please set the following in your colab:\n","\n","\n","```\n","Runtime -> Change runtime type -> Hardware accelerator: GPU\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z5XuNKpQUwfi"},"source":["## Short Introduction to Tensor"]},{"cell_type":"markdown","metadata":{"id":"hb9Wn9a0bM4o"},"source":["### Tensor Operations"]},{"cell_type":"markdown","metadata":{"id":"Qqu-8rqLWQE6"},"source":["The basic building block in Pytorch is the tensor. Tensors are very similar to the narray in the numpy and it has many pre-defined operations:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTabk5E4W2RD","executionInfo":{"status":"ok","timestamp":1638136775148,"user_tz":300,"elapsed":250,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"9c1122e0-a381-40b5-f682-fa1113771ddb"},"source":["# define a new tensor is similar to define a numpy array\n","a_numpy = np.array([7, 7, 7])\n","a_tensor = torch.Tensor([7, 7, 7])\n","print('Tensor and Numpy:')\n","print(a_numpy)\n","print(a_tensor)\n","print()\n","\n","# tensor support basic operations\n","tensor1 = torch.Tensor([1, 2, 3])\n","tensor2 = torch.Tensor([2, 3, 4])\n","print('Basic Operations')\n","print('Add')\n","print(tensor1 + tensor2)\n","print('Multiply')\n","print(tensor1 * tensor2)\n","print()\n","\n","# tensor object also has its own method\n","print('Tensor Oject method')\n","print(tensor1.add(tensor2))\n","print()\n","\n","# function operates on tensor\n","print('Function operates on tensor')\n","max_value, max_index = torch.max(tensor1, dim=0) # pytorch max val -> val, index\n","max_value, _ = torch.max(tensor1, dim=0) \n","print(\"modification\",max_value)\n","print(f'Max value: {max_value}, Max Index: {max_index}')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor and Numpy:\n","[7 7 7]\n","tensor([7., 7., 7.])\n","\n","Basic Operations\n","Add\n","tensor([3., 5., 7.])\n","Multiply\n","tensor([ 2.,  6., 12.])\n","\n","Tensor Oject method\n","tensor([3., 5., 7.])\n","\n","Function operates on tensor\n","modification tensor(3.)\n","Max value: 3.0, Max Index: 2\n"]}]},{"cell_type":"markdown","metadata":{"id":"JI7WyUe6bXv1"},"source":["Please note these opeartions will return a new tensor as result and the variables participating operations will not be modified. Alternatively, we can make these operations happens in-place. Instead of returing a extra tensor as result, in-place operations will directly change the content of given tensor. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sapPNjSxdS1g","executionInfo":{"status":"ok","timestamp":1638136779116,"user_tz":300,"elapsed":215,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"b2002dad-47ad-49d3-d59c-4153e5f93f26"},"source":["# not in-plcae\n","tensor1 = torch.Tensor([1, 2, 3])\n","tensor2 = torch.Tensor([2, 3, 4])\n","tensor1.add(tensor2)\n","print(tensor1)  # the content in tensor1 is not modified\n","\n","# in-place\n","tensor1.add_(tensor2)\n","print(tensor1) # the content in tensor1 is modified"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 2., 3.])\n","tensor([3., 5., 7.])\n"]}]},{"cell_type":"markdown","metadata":{"id":"6EXSZQNGdxFq"},"source":["In general, an in-place is the normal operation with extra '_' at the end. For example,\n","\n","\n","```\n","tensor.add() # not in-place\n","tensor.add_() # in-place\n","\n","tensor.abs() # not in-place\n","tensor.abs_() # in-place\n","```\n","\n","The in-place operation will be useful when you have limitation on memory. For example, if you have a huge tensor representation for a high resolution image, it may be costly to keep an another copy. For the full list of operations, please see [here](https://pytorch.org/docs/stable/tensors.html).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_lIS7GhOgxDn"},"source":["### Change the shape of the tensor\n","To change the shape of a tensor, we can use the \n","\n","```\n","Tensor.view()\n","```\n","which is similar to numpy.reshape(). Also, we can use -1 as place holder to let the pytorch find the correct shape for us.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMRod11XhXQq","executionInfo":{"status":"ok","timestamp":1638136782933,"user_tz":300,"elapsed":344,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"92c7b61f-12cb-4199-c37f-ccf8261bbb6f"},"source":["# initialize a tensor with random numbers\n","a_tensor = torch.randn((100, 33, 22, 11), dtype=torch.float)\n","print(a_tensor.size())  # use Tensor.size() to find the shape\n","print()\n","\n","\n","# reshape to (33, 22, 11, 100)\n","reshaped1 = a_tensor.view(33, 22, 11, 100)\n","print('After reshape')\n","print(reshaped1.size())\n","print()\n","\n","# use -1 as place holder\n","reshaped2 = a_tensor.view(33, 11, 10, -1)  # let pytorch calculate the last dimension for us\n","print('After reshape')\n","print(reshaped2.size())\n","print()\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([100, 33, 22, 11])\n","\n","After reshape\n","torch.Size([33, 22, 11, 100])\n","\n","After reshape\n","torch.Size([33, 11, 10, 220])\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"6_L2obaaivA8"},"source":["### Move Tensor to GPU/CPU\n","You may have heard deep learning models are training fatser on GPUs. To do this, we will need to move our data, which is represents as tensors, to GPU."]},{"cell_type":"code","metadata":{"id":"ofiYLx0-jMp3"},"source":["# get the device on your machine\n","cpu = torch.device('cpu')\n","gpu = torch.device('cuda')\n","\n","# often we use the following one liner to help us choose device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# move tensor to gpu\n","a_tensor = torch.Tensor([1, 2, 3])\n","a_tensor = a_tensor.to(gpu)  # you might get an error if you did not activate the gpu in your colab, see: https://colab.research.google.com/notebooks/gpu.ipynb\n","print('GPU tensor:')\n","print(a_tensor)\n","print()\n","\n","# move back to cpu\n","a_tensor = a_tensor.to(cpu)\n","print('CPU tensor:')\n","print(a_tensor)\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zKVzrZsllCrm"},"source":["The \n","```\n","tensor(...., device='cuda:0')\n","```\n","menas we have successfully moved tensor to GPU:0.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WA6Lz6hvUefo"},"source":["## MNIST Dataset"]},{"cell_type":"markdown","metadata":{"id":"AD_qc3UAot37"},"source":["#### *Download data*\n","Load the MINIST dataset:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"9FXfeIYEpkov","colab":{"base_uri":"https://localhost:8080/","height":421,"referenced_widgets":["14d608d909bb4b2e9f4a557ad60bf44c","4002b355594f42d38562b0c68a8bb06b","f5f9f76206464274acb872b745849a01","34e6eed69f4a42358e76ea5c178ce84d","69292a3af3fd411fac34f9bc98e22aec","aa3b5f4efd09469ea70bc99805e8a381","4053228330b24fc79188f3dcdc95cf7e","7fc7544437e6493fb7ff37cc2b6ae203","e7fbe4d1fa454ed18e0457d4b6ac8152","610ac0be7ab84dd8988a4d40db0b62d5","20388ee2c6564a78a099165e55808615","6e7f577ea5b94a8a942ba6b70c85ea35","7dc576dbc2d14411b698f9673360a520","dfcdec0ca07d41abafbcc44eb4439a1a","14c85abd7bef4246a305f54461e01388","111dae9471a6467d8bfca7b2ee075408","67b2eb43ba2b4b76bfcd1e130b43c3b6","ac1824cbd37449d492b746e0e6eca5c9","b7927d8c329640c6841f58bf2c1e2981","72ad2471de3942928045904055004782","6e237d2ebd874a758c6c444b676a2e2e","6c96ec10563547f68a0e21d6a1448b1b","e4b4e35bf7754b3a88b2f100f78e6229","60dc04698fa24bfebbf2424f8a81e733","94cc889fe0f642f2b08205567485e015","dcb898a0cddb48998ee34dcecc99db73","c10f3c1d90e249ffb454c4802b62beab","7959021db93c416da3ab05d6575e9b7d","3825172febb34bb5a5ccd81f7e2e7aee","7d2f6db50f0d42d493a2c2e344578392","68939098c18a48e59cc010e917e8ff70","ac7b73c5c5674e2fb5731034537f50d9","aa7d79c98e0d4ea6b172e1383ce9c3ec","634c1ae8a86047d6b78fc68253d4a71c","5e1e2bea26704bceb268658dee20cb97","e117ebfefe1b45f08c3ff5cb50009b97","7541a092fe784c548279dad714f28962","9de6d939cb4247a1b545a616e71bc908","81b69f39b4fe447d91b32910cc3028e3","301a594f45bc49a6b2e783263cc89d38","d17fb150a0ee44d990ff01e2f9a1ab1e","c838c42f7bb542ad83deae620c06c4e9","ad1f284ac9c0486989559d82756f94a3","a3a9e752bff945c497e5df0938aeca99"]},"executionInfo":{"status":"ok","timestamp":1638136799909,"user_tz":300,"elapsed":1433,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"2883a108-f8a1-42cb-8944-ebe253c20581"},"source":["# load from torch vision package\n","train_dataset = torchvision.datasets.MNIST('./data',\n","                                           train=True,\n","                                           transform=transforms.ToTensor(),\n","                                           download=True)\n","\n","test_dataset = torchvision.datasets.MNIST('./data',\n","                                          train=False,\n","                                          transform=transforms.ToTensor(),\n","                                          download=True)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14d608d909bb4b2e9f4a557ad60bf44c","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/9912422 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e7f577ea5b94a8a942ba6b70c85ea35","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/28881 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4b4e35bf7754b3a88b2f100f78e6229","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1648877 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"634c1ae8a86047d6b78fc68253d4a71c","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/4542 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"AZYhyDx7s8u-"},"source":["Visulize it:"]},{"cell_type":"code","metadata":{"id":"-jqqMl5Ts_Oj"},"source":["one_pic, _ = train_dataset[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OSNm2se-tD73","executionInfo":{"status":"ok","timestamp":1634250599907,"user_tz":240,"elapsed":18,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"4909b525-f64f-4dc9-c132-c315d8dd5d0b"},"source":["one_pic.size() # (channel, width, height)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 28, 28])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"6fq8oZQ3t9lb"},"source":["one_pic"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CcYlzx8G6dGy","executionInfo":{"status":"ok","timestamp":1634250600311,"user_tz":240,"elapsed":7,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"15b266b2-275d-4071-af7d-e8ffbdb667c1"},"source":["print(\"row:\",len(array[0]))\n","print(\"col:\",len(array[0][0]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["row: 28\n","col: 28\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":284},"id":"NGIT0OCFtcbQ","executionInfo":{"status":"ok","timestamp":1638136805332,"user_tz":300,"elapsed":916,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"576cc7fc-c1c7-44a3-9d0d-09b3da6a2c37"},"source":["import matplotlib.pyplot as plt\n","\n","fig = plt.figure()\n","for i in range(6):\n","    cur_data, cur_label = train_dataset[i]\n","    plt.subplot(2,3,i+1)\n","    plt.tight_layout()\n","    plt.imshow(cur_data[0], cmap='gray', interpolation='none')\n","    plt.title(\"Ground Truth: {}\".format(cur_label))\n","    plt.xticks([])\n","    plt.yticks([])"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 6 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Zq7MnvAksi1l"},"source":["For the load MNIST dataset function:\n","\n","\n","```\n","train_dataset = torchvision.datasets.MNIST('./data',\n","                                           train=True,\n","                                           transform=transforms.ToTensor(),\n","                                           download=True)\n","```\n","\n","The first argument specify the path where you want to save your data, *train = True* means we want to download the train data, *download=True* means to download the data on local directory, *transform=transforms.ToTensor()* will help us convert the data in Tensor format so we don't have to convert the image to tensor manually by ourself.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xYfFt-Hft_DR"},"source":["#### *Batch Training*\n","We now have the entire training dataset. But in deep learning, we will need to divide the dataset into different mini-batches to help us update the gradient more efficiently. To do this in Pytorch, we just simply: "]},{"cell_type":"code","metadata":{"id":"_lI31MO5u6dm","executionInfo":{"status":"ok","timestamp":1638136809004,"user_tz":300,"elapsed":242,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}}},"source":["train_loader = DataLoader(dataset=train_dataset,\n","                          batch_size=128,\n","                          shuffle=True)\n","valid_loader = DataLoader(dataset=test_dataset,\n","                          batch_size=128,\n","                          shuffle=False)\n","# 128 images"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gMoyrAg5vRcH"},"source":["The \n","\n","```\n","DataLoader(dataset=train_dataset,\n","          batch_size=128,\n","          shuffle=True)\n","```\n","takes the dataset object as input. In *batch_size* arugemnt, we specify the batch size of our mini batch to 128. And in *shuffle* argument, we tell the *DataLoader* we wish to random shuffle the dataset before slicing it to different batches.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5d-oQbqFwUgM"},"source":["The *DataLoader()* will return a generator and we can get our mini batch by iterating the iterator:"]},{"cell_type":"code","metadata":{"id":"Unde2owDwlDx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638136811878,"user_tz":300,"elapsed":283,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"de0dbd9b-82c4-4d08-c01e-26dd37b1d0b1"},"source":["# train: 1 mini batch contains 128 observations\n","train_iterator = train_loader.__iter__()\n","train_sample_image, train_sample_label = next(train_iterator)\n","\n","# test: 1 mini batch contains 128 observations\n","valid_iterator = valid_loader.__iter__()\n","valid_sample_image, valid_sample_label = next(valid_iterator)\n","\n","# print\n","print('Train Sample')\n","print('1 of 128 train sample', train_sample_image.shape)\n","print('1 of 128 train label', train_sample_label.shape)\n","print('Test Sample')\n","print('1 of 128 test sample', valid_sample_image.shape)\n","print('1 of 128 test label', valid_sample_label.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Sample\n","1 of 128 train sample torch.Size([128, 1, 28, 28])\n","1 of 128 train label torch.Size([128])\n","Test Sample\n","1 of 128 test sample torch.Size([128, 1, 28, 28])\n","1 of 128 test label torch.Size([128])\n"]}]},{"cell_type":"markdown","metadata":{"id":"sSsulIllVGZB"},"source":["## A Naive Classifier"]},{"cell_type":"markdown","metadata":{"id":"ouoMSzEnt-SL"},"source":["### *Define a model*\n","With the datase, let's build a a deep learning model to classify the MINIST dataset. For simplicity, our deep learning model will only have:\n","- One input layer\n","- One hidden layer with relu as our activation function\n","- One output layer\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"_SyLlTIN0IzI","executionInfo":{"status":"ok","timestamp":1638136814919,"user_tz":300,"elapsed":268,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}}},"source":["class FeedForward(nn.Module):  # just add (nn.Module) after your model name\n","    # model = FeedForward(input_size=28 * 28, hidden_size=512, num_classes=10)\n","    def __init__(self, input_size, hidden_size, num_classes):  # define the layers we need to construct the model\n","        super(FeedForward, self).__init__()  # initilize the parent nn.Module class\n","        self.Linear1 = nn.Linear(input_size, hidden_size)  # define our hidden layer here, the input shape of the layer is input_size and out shape is hidden_size\n","        self.act = nn.ReLU()  # define the relu activation function\n","        self.Linear2 = nn.Linear(hidden_size, num_classes) # define our output layer. The input shape should be the output shape of hiddden layer\n","       \n","    def forward(self, x):  # define the how the input tensors flows the different layers\n","        out1 = self.Linear1(x)\n","        out = self.act(out1)\n","        out = self.Linear2(out)\n","        \n","        # tensor flow path: Input layer -> Relu activation function -> Output layers\n","\n","        return out "],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-7xfRm731_cO"},"source":["Few things to note:  \n","1. The model in Pytorch are defined in class. Our model need to inherit from the nn.Module parent class. If you are not familar with the object programming in python, just make sure you add the *(nn.Module)* after your model name.\n","2. In our model, we will need to implement two functions:\n","\n","\n","```\n","def __init__()\n","```\n","In general, the `__init__` function holds the follows:\n","-  Initialize layers with parameters which should be trained, e.g. `nn.Linear` \n","- Other layers, although without parameters, need to be initiated as class objects, e.g. `nn.ReLU`. Alternatively, you can use `torch.nn.functional.relu` directly in forward function instead of class nn.ReLU\n","- Stateful layers such as `nn.Dropout`. When Dropout is placed in `__init__`, it is enabled in training but disabled in evaluation mode.\n","\n","\n","```\n","def forward(self, x)\n","```\n","to define how the tensor are passed through different layers. In our case, the data will be first pass though the hidden layer. The output of hidden layer will pass though the *relu* activation function. And then the output of *relu* function will be passed though output layer.\n","3. Please do not forget to initialize the parent class, just add\n","\n","\n","```\n","super(FeedForward, self).__init__()\n","```\n","in your\n","\n","\n","```\n","def __init__()\n","```\n","function\n","4. In the Pytorch the *Dense* layer is called *Linear*. However, different from Keras, *Linear* layer does not come with an activation function. Activation functions need to be defined separately. The *Linear* layer takes three arguments:\n","\n","\n","```\n","torch.nn.Linear(in_features, out_features, bias=True)\n","```\n","The *Linear* layer will perform the following transorm on tensor:\n","$$y = xA^T + b$$\n","\n","The *in_features* will be the shape of input tensor($x$), the *out_features*($y$) will be the shape of output tensor, the *bias* means whether we want to include bias($b$).\n","\n","\n","5. You might have noticed we did not add the *softmax()* function after the output layer to convert the output nubmer to probability for each class. Later, we will use \n","```\n","torch.nn.CrossEntropyLoss()\n","```\n","which contains a softmax function inside of it.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vf4PVAW0gbNg"},"source":["### *Train the model*\n","Let's define the training function of the model:"]},{"cell_type":"code","metadata":{"id":"35UTP9cZ5b71","executionInfo":{"status":"ok","timestamp":1638136821167,"user_tz":300,"elapsed":228,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}}},"source":["def train(batch_size, num_epochs, learning_rate, model, train_loader, valid_loader, device):\n","  # move the model to device\n","  model = model.to(device)  # move the model to gpu or cpu\n","\n","  # set up loss function and optimizer\n","  criterion = nn.CrossEntropyLoss()  # the CrossEntropyLoss() will provide the softmax for us\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # pass in the parameters to be updated and learning rate\n","\n","  # traning loop\n","  print('Training Starts:')\n","  num_total_steps = len(train_loader)\n","  for epoch in range(num_epochs):\n","      model.train()  # start to train the model, activate training behavior\n","      train_loss = 0\n","      for i, (images, labels) in enumerate(train_loader):\n","          # reshape images\n","          images = images.reshape(-1, 28 * 28).to(device)  # reshape: from (128, 1, 28, 28) -> (128, 28 * 28) = (128, 284), move batch to device\n","          labels = labels.to(device)  # move to device\n","          \n","          # forward\n","          outputs = model(images)  # forward\n","          cur_train_loss = criterion(outputs, labels)  # loss\n","\n","          # backward\n","          cur_train_loss.backward()  # run back propagation\n","          optimizer.step()  # optimizer update all model parameters\n","          optimizer.zero_grad()  # set gradient to zero, avoid gradient accumulating\n","\n","          # loss\n","          train_loss += cur_train_loss\n","\n","\n","\n","      # valid\n","      model.eval()  # start to train the model, activate training behavior\n","      with torch.no_grad():  # tell pytorch not to update parameters\n","          val_loss = 0\n","          for images, labels in valid_loader:\n","              # calculate validation loss\n","              images = images.reshape(-1, 28 * 28).to(device)\n","              labels = labels.to(device)\n","              outputs = model(images)\n","              cur_valid_loss = criterion(outputs, labels)\n","              val_loss += cur_valid_loss\n","      \n","      # print\n","      print(f\"Epoch:{epoch + 1} / {num_epochs}, train loss:{(train_loss/len(train_loader)).item():.5f}, valid loss:{(val_loss/len(valid_loader)).item():.5f}\")"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"z38atuuBlz_h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aBBvY5rskl-b"},"source":["Let's breakdown the function:\n","1. In order to use GPU to train our model, we will need to move the model and the training data to the correspond device\n","\n","\n","```\n","model.to(device)\n","....\n","images = images.reshape(-1, 28 * 28).to(device) \n","```\n","2. The loss function *CrossEntropyLoss* will provide us a softmax function and it expectes a class index $[0, C - 1]$ where the $C$ is the number of classes in the training data. In other words, we don't need to apply one-hot encoding in pytorch before using the cross entropy function, the Pytorch will handel the conversion for us. Full document can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n","3. We are using the Adam optimizer and the optimizer takes two inputs:\n","\n","\n","```\n","torch.optim.Adam(params, lr)\n","params: the parameters to be updated by the optimizer\n","lr: learning rate\n","```\n","In our case, we want the optimizer to track all parameters in the model. Other optimizers are also available, [see](https://pytorch.org/docs/stable/optim.html)\n","\n","4. *model.train()* and *model.eval()*: with the *model.train()*, it will activate some training behavior in layers(For exampel, to activate the dropdout behavior to let *dropout* layer random sample neurons in training session). With the *model.eval()*, it will activate some evaluation behavor in layers(For example, in evaluation session, we wish the output of dropout layers to multiply the drop ratio and stop randomly sampling the neurons)\n","5. Batch training:\n","\n","\n","```\n","for epoch in range(num_epochs):\n","      for i, (images, labels) in enumerate(train_loader):\n","```\n","At each epoch, we wish the data is passed in batches.\n","6. A typical training procedure for pytorch is:\n","\n","a forward seesion to calculate the loss of current batch\n","```\n","outputs = model(images)  # forward\n","train_loss = criterion(outputs, labels)  # loss\n","```\n","then we call back probagation to calculate the gradient for each parameters\n","\n","\n","```\n","train_loss.backward()\n","```\n","with the calculated gradients, call the optimizer to update each parameters\n","```\n","optimizer.step()\n","```\n","and please do not forget to call\n","```\n","optimizer.zero_grad()\n","```\n","to zero the gradients in optimizer. In the default behavior, the optimizer will try to remember all grident history and accumulate the gradients. Usually it is not the behavior we desired, so we manually clear the gradient history to prevent gradient accumulation.\n","\n","7. When we evluate the model, we will need to use\n","```\n","with torch.no_grad():\n","  ...\n","```\n","to tell pytorch stop calculating the gridents in the evaluation session. We don't want evaluation data to have any interaction with our model so we explictly tell Pytorch no gradients.\n","\n","8. In training seesion, we reshped the training batch data:\n","\n","```\n","images = images.reshape(-1, 28 * 28).to(device)\n","```\n","This is because: in default, the Pytorch assume the image data will have the format:\n","```\n","(num_batch, channels, width, height) -> (128, 1, 28, 28)\n","```\n","We wish to use a feed forward neural network to classify the image, so we will need to flatten the data to\n","```\n","(num_batch, width * height) -> (128, 28 * 28)\n","``` \n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"earRia8Pspu4"},"source":["All set and ready to go, let's call the train function to train our model:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"riUYHlj4s1cD","executionInfo":{"status":"ok","timestamp":1638137014037,"user_tz":300,"elapsed":187415,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"d3f1627c-68b6-4211-b9f5-f87ce2cb5d35"},"source":["# define model\n","model = FeedForward(input_size=28 * 28, hidden_size=512, num_classes=10)\n","\n","# train\n","train(batch_size=128,\n","      num_epochs=20,\n","      learning_rate=0.01,\n","      model=model,\n","      train_loader=train_loader,\n","      valid_loader=valid_loader, \n","      device=device)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Starts:\n","Epoch:1 / 20, train loss:0.20848, valid loss:0.12331\n","Epoch:2 / 20, train loss:0.11178, valid loss:0.11772\n","Epoch:3 / 20, train loss:0.08768, valid loss:0.11638\n","Epoch:4 / 20, train loss:0.07790, valid loss:0.12986\n","Epoch:5 / 20, train loss:0.06659, valid loss:0.13298\n","Epoch:6 / 20, train loss:0.05989, valid loss:0.14194\n","Epoch:7 / 20, train loss:0.06205, valid loss:0.15494\n","Epoch:8 / 20, train loss:0.05110, valid loss:0.17525\n","Epoch:9 / 20, train loss:0.05552, valid loss:0.15248\n","Epoch:10 / 20, train loss:0.05065, valid loss:0.19719\n","Epoch:11 / 20, train loss:0.05306, valid loss:0.23335\n","Epoch:12 / 20, train loss:0.04322, valid loss:0.21652\n","Epoch:13 / 20, train loss:0.03858, valid loss:0.22559\n","Epoch:14 / 20, train loss:0.04830, valid loss:0.25715\n","Epoch:15 / 20, train loss:0.05331, valid loss:0.29230\n","Epoch:16 / 20, train loss:0.03694, valid loss:0.27267\n","Epoch:17 / 20, train loss:0.03803, valid loss:0.29438\n","Epoch:18 / 20, train loss:0.03588, valid loss:0.23054\n","Epoch:19 / 20, train loss:0.03335, valid loss:0.24708\n","Epoch:20 / 20, train loss:0.03595, valid loss:0.26795\n"]}]},{"cell_type":"markdown","metadata":{"id":"EnaaBe67VcrQ"},"source":["## Early Stopping\n","To add the early stopping into our training, we just need to slightly modify our train function:"]},{"cell_type":"code","metadata":{"id":"s-mujl2EVluO","executionInfo":{"status":"ok","timestamp":1638137014038,"user_tz":300,"elapsed":16,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}}},"source":["def train_earlyStopping(batch_size, num_epochs, learning_rate, model, train_loader, valid_loader, device, no_improve_eopchs):\n","  # move the model to device\n","  model.to(device)\n","\n","  # new: early stopping\n","  min_val_loss = 10000\n","  epochs_no_improvement = 0\n","\n","  # set up loss function and optimizer\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  # traning loop\n","  print('Training Starts:')\n","  model.train()\n","  num_total_steps = len(train_loader)\n","  for epoch in range(num_epochs):\n","      train_loss = 0\n","      for i, (images, labels) in enumerate(train_loader):\n","          # reshape images\n","          images = images.reshape(-1, 28 * 28).to(device)\n","          labels = labels.to(device)\n","          \n","          # forward\n","          outputs = model(images)\n","          cur_train_loss = criterion(outputs, labels)\n","\n","          # backward\n","          cur_train_loss.backward()\n","          optimizer.step()\n","          optimizer.zero_grad()\n","\n","          # loss\n","          train_loss += cur_train_loss\n","      train_loss /= len(train_loader)\n","\n","\n","\n","      # valid\n","      model.eval()\n","      with torch.no_grad():\n","          val_loss = 0\n","          for images, labels in valid_loader:\n","              # calculate validation loss\n","              images = images.reshape(-1, 28 * 28).to(device)\n","              labels = labels.to(device)\n","              outputs = model(images)\n","              cur_valid_loss = criterion(outputs, labels)\n","              val_loss += cur_valid_loss\n","          val_loss /= len(valid_loader)\n","      \n","      # early stopping\n","      if val_loss < min_val_loss:\n","        torch.save(model.state_dict(), 'model.pth')\n","        epochs_no_improvement = 0\n","        min_val_loss = val_loss\n","      else:\n","        epochs_no_improvement += 1\n","      \n","      if epoch > 5 and epochs_no_improvement > no_improve_eopchs:\n","        print(\"Early Stopping\")\n","        break\n","      \n","      # print\n","      print(f\"Epoch:{epoch + 1} / {num_epochs}, train loss:{train_loss:.5f}, valid loss:{val_loss:.5f}\")"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OX2Np3ZuT8q2"},"source":["In this new fucntion, we add the code block:\n","\n","\n","```\n","if val_loss < min_val_loss:\n","        torch.save(model.state_dict(), 'model.pth')\n","        epochs_no_improvement = 0\n","        min_val_loss = val_loss\n","      else:\n","        epochs_no_improvement += 1\n","      \n","      if epoch > 5 and epochs_no_improvement > no_improve_eopchs:\n","        print(\"Early Stopping\")\n","        break\n","```\n","If the current validation loss is less than the minimal validation history, we will save the model. If the model does not improve its performance after *no_improve_eopchs* epochs, then we will stop the training process.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-N0utDfvVcj6"},"source":["Train the model and load early stopped model:"]},{"cell_type":"code","metadata":{"id":"XpE90wvpJ7FC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638137166426,"user_tz":300,"elapsed":63113,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"d7437da6-371a-4faf-be9b-bcff588491cf"},"source":["# define model\n","model = FeedForward(input_size=28 * 28, hidden_size=512, num_classes=10)\n","\n","# train\n","train_earlyStopping(batch_size=128,\n","                    num_epochs=20,\n","                    learning_rate=0.01,\n","                    model=model,\n","                    train_loader=train_loader,\n","                    valid_loader=valid_loader, \n","                    device=device,\n","                    no_improve_eopchs=3)\n","\n","# load the early stopped model\n","early_stopped_model = FeedForward(input_size=28 * 28, hidden_size=512, num_classes=10)\n","early_stopped_model.load_state_dict(torch.load('model.pth', map_location=device))  # <All keys matched successfully> means the model is loaded"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Starts:\n","Epoch:1 / 20, train loss:0.20526, valid loss:0.12547\n","Epoch:2 / 20, train loss:0.11103, valid loss:0.10471\n","Epoch:3 / 20, train loss:0.08791, valid loss:0.11859\n","Epoch:4 / 20, train loss:0.07353, valid loss:0.12165\n","Epoch:5 / 20, train loss:0.07048, valid loss:0.14656\n","Epoch:6 / 20, train loss:0.06542, valid loss:0.12438\n","Early Stopping\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":14}]}]}