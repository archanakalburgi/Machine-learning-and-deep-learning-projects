{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" NTM_Torch(SeqToSeq).ipynb","provenance":[{"file_id":"1a3h3DefIKqxemcX5DJ8EOr7XprXKkxiS","timestamp":1636656594129}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kGnrBg46TDtR"},"source":["## Seq2Seq Model - Neural Machine Translation\n","- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n","- https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n","- https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/"]},{"cell_type":"markdown","metadata":{"id":"MGNubSuHTDtV"},"source":["<img width=\"60%\" src=\"https://machinetalk.org/wp-content/uploads/2019/03/attention.gif\" class=\"img-responsive wp-post-image\" alt=\"\" data-pagespeed-url-hash=\"1740859731\" onload=\"pagespeed.CriticalImages.checkImageForCriticality(this);\">"]},{"cell_type":"markdown","metadata":{"id":"fAY9Y6n4TDtV"},"source":["### What is <b>sequence-to-sequence learning<b>?"]},{"cell_type":"markdown","metadata":{"id":"S2BLsnmlTDtW"},"source":["Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).\n","\n","\"the cat sat on the mat\" -> <b>[Seq2Seq model] </b>-> \"le chat etait assis sur le tapis\"\n","\n","This can be used for machine translation or for free-from question answering (generating a natural language answer given a natural language question) -- in general, it is applicable any time you need to generate text."]},{"cell_type":"markdown","metadata":{"id":"fKyBJ-dxTDtW"},"source":["### The general case: canonical sequence-to-sequence"]},{"cell_type":"markdown","metadata":{"id":"ESisiihHTDtX"},"source":["In the general case, input sequences and output sequences have different lengths (e.g. machine translation) and the entire input sequence is required in order to start predicting the target. This requires a more advanced setup, which is what people commonly refer to when mentioning \"sequence to sequence models\" with no further context. Here's how it works:"]},{"cell_type":"markdown","metadata":{"id":"bUxYgiCBTDtX"},"source":["<img alt=\"Seq2seq inference\" src=\"https://machinetalk.org/wp-content/uploads/2019/04/input.png\" width=\"60%\">\n"]},{"cell_type":"markdown","metadata":{"id":"Cmp6CVjxTDtX"},"source":["- A RNN layer (or stack thereof) acts as <b>\"encoder\"</b>: it processes the input sequence and returns its own internal state. The encoder, which is on the left-hand side, requires only sequences from source language as inputs. Note that we discard the outputs of the encoder RNN, only recovering the state. This state will serve as the \"context\", or \"conditioning\", of the decoder in the next step.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bMTFFF09TDtY"},"source":["- Another RNN layer (or stack thereof) acts as <b>\"decoder\"</b>: it is trained to predict the next word of the target sequence, given previous words of the target sequence. \n","    - Specifically, it is trained to `turn the target sequences into the same sequences but offset by one timestep in the future`, a training process called `teacher forcing` in this context. \n","    - Importantly, `the encoder uses as initial state the state vectors from the encoder`, which is how the decoder obtains information about what it is supposed to generate.\n","    - Effectively, the decoder learns to generate target at $t+1$ given target at $t$, conditioned on the input sequence.\n","    - The same process can also be used to train a Seq2Seq network `without teacher forcing`, i.e. by reinjecting the decoder's predictions into the decoder."]},{"cell_type":"markdown","metadata":{"id":"Jm9fbUP_TDtY"},"source":["## Neural Translation Machine"]},{"cell_type":"markdown","metadata":{"id":"HtJLiaGtTDtZ"},"source":["Let's illustrate these ideas with actual code.\n","\n","For our example implementation, we will use a dataset of pairs of English sentences and their French translation, which you can download from <a href=\"http://www.manythings.org/anki/\">manythings.org/anki</a>. The file to download is called `fra-eng.zip` (English/French). We will implement a word-level model sequence-to-sequence model, processing the input word-by-word and generating the output word-by-word. \n","\n","Here's a summary of our process:\n","\n","- 1) Turn the sentences into 3 Numpy arrays, `encoder_input_data`, `decoder_input_data`, `decoder_target_data`:\n","    - `encoder_input_data` is a 3D array of shape (`num_pairs`, `max_english_sentence_length`, `num_english_characters`) containing a one-hot vectorization of the English sentences.\n","    - `decoder_input_data` is a 3D array of shape (`num_pairs`, `max_french_sentence_length`, `num_french_characters`) containg a one-hot vectorization of the French sentences.\n","    - `decoder_target_data` is the same as decoder_input_data but offset by one timestep. `decoder_target_data[:, t, :]` will be the same as `decoder_input_data[:, t + 1, :]`.\n","- 2) Train a basic LSTM-based Seq2Seq model to predict `decoder_target_data` given `encoder_input_data` and `decoder_input_data`. Our model uses `teacher forcing`.\n","- 3) Decode some sentences to check if the model is working (i.e. turn samples from `encoder_input_data` into corresponding samples from `decoder_target_data`).\n","\n","Because the `training process` and `inference process` (decoding sentences) are quite different, `we use different models for both, albeit they all leverage the same inner layers`.\n","\n","Note that the encoder and decoder are connected by RNN states:\n","\n","- `encoder states`: This is used to store the states of the encoder.\n","- `inital_state of decoder`: we pass the encoder states to the decoder as initial states.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xkP-yOF6UFUR","executionInfo":{"status":"ok","timestamp":1636035516801,"user_tz":240,"elapsed":4068,"user":{"displayName":"Ziyi Xiong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02400447891473674622"}},"outputId":"021fbf95-543e-46a8-98d3-3572a0380c61"},"source":["!pip install torchinfo"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.5.3-py3-none-any.whl (19 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.5.3\n"]}]},{"cell_type":"code","metadata":{"id":"j0G2VAsTTDta"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","import numpy as np\n","import unicodedata\n","import re\n","from tensorflow import keras\n","import pandas as pd\n","from sklearn.utils import shuffle"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"898_SKUGTDtb"},"source":["### (1) Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"vlxqKSoMTDtb"},"source":["1) Turn the sentences into 3 Numpy arrays, `data_en`, `data_fr_in`, `data_fr_out`:\n","- `data_en`: is a 2D array of shape (`num_samples`, `max_en_words_per_sentence`) containing a tokenized sentences after preprocessing.\n","- `data_fr_in`: is a 2D array of shape (`num_samples`, `max_fr_words_per_sentence`) containing a tokenized sentences after preprocessing.\n","- `data_fr_out`: the same as decoder_input_data but offset by one timestep, i.e.  `data_fr_in [:, t]` will be the same as `data_fr_out [:, t+1]`.  "]},{"cell_type":"markdown","metadata":{"id":"22OH8g2dTDtc"},"source":["Note that, this is a demo version of the NTM. We will train the model using a small dataset. To make the model work realistically, you need to train the model with a larget collection of training samples\n","\n","\n","We'll use Keras for simple data preprocessing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jwQ1aLeaWCZ7","executionInfo":{"status":"ok","timestamp":1636035570391,"user_tz":240,"elapsed":36983,"user":{"displayName":"Ziyi Xiong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02400447891473674622"}},"outputId":"031cf173-bd41-4cd5-ddaa-20094cf3519f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path_to_glove_file = \"drive/My Drive/TA 667/fra-eng/fra.txt\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"pxGrDvhrTDtc","executionInfo":{"status":"ok","timestamp":1636035581195,"user_tz":240,"elapsed":2825,"user":{"displayName":"Ziyi Xiong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02400447891473674622"}},"outputId":"d1a410f2-920b-4371-95d6-85ac565ac7f6"},"source":["# Read data\n","#text = pd.read_csv('fra.txt', sep=\"\\t\", header=None, usecols=[0,1])\n","text = pd.read_csv(path_to_glove_file, sep=\"\\t\", header=None, usecols=[0,1])\n","text.columns =['en','fr']\n","text.head()\n","len(text)\n","\n","# Take a small set to save training time\n","text = shuffle(text)\n","raw_data=text.iloc[0:10000]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>en</th>\n","      <th>fr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>Salut !</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Hi.</td>\n","      <td>Salut.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     en        fr\n","0   Go.      Va !\n","1   Hi.   Salut !\n","2   Hi.    Salut.\n","3  Run!   Cours !\n","4  Run!  Courez !"]},"metadata":{},"execution_count":5},{"output_type":"execute_result","data":{"text/plain":["175623"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"APHjwbk3TDte"},"source":["# clean up text\n","\n","def unicode_to_ascii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","\n","def normalize_string(s):\n","    s = unicode_to_ascii(s)\n","    s = re.sub(r'([!.?])', r' \\1', s)\n","    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n","    s = re.sub(r'\\s+', r' ', s)\n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mn_5sRfKTDte"},"source":["# clean up text\n","raw_data_en = [normalize_string(data) for data in raw_data[\"en\"]]\n","\n","# add special token <start>/<end> to indicate the beginning and end of a sentence\n","raw_data_fr_in = ['<start> ' + normalize_string(data) for data in raw_data[\"fr\"]]\n","raw_data_fr_out = [normalize_string(data) + ' <end>' for data in raw_data[\"fr\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0_K5J-mTDte","executionInfo":{"status":"ok","timestamp":1636035587707,"user_tz":240,"elapsed":851,"user":{"displayName":"Ziyi Xiong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02400447891473674622"}},"outputId":"f4dcaec0-8240-42bd-e57a-4b2ac43a4e3b"},"source":["# Tokenize each sentence and index each word\n","max_en_words = 5000\n","max_en_len = 10\n","en_tokenizer = keras.preprocessing.text.Tokenizer(filters='', \\\n","                                                  num_words=max_en_words )\n","en_tokenizer.fit_on_texts(raw_data_en)\n","print(\"Total number of English words: \", len(en_tokenizer.word_index))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of English words:  4511\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"peO39yU1TDtf","executionInfo":{"status":"ok","timestamp":1636035589761,"user_tz":240,"elapsed":523,"user":{"displayName":"Ziyi Xiong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02400447891473674622"}},"outputId":"68d37aa6-398a-45a7-9a2a-40f149fa0b6b"},"source":["data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n","data_en = keras.preprocessing.sequence.pad_sequences(data_en,\\\n","                                                     maxlen=max_en_len, \\\n","                                                     padding='post')\n","# print a sample sentence after preprocessing\n","print(data_en[:3])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[  20    8   44   10   13    7   61   81  190    1]\n"," [   2  826  437  647    1    0    0    0    0    0]\n"," [  14  351    4 2393   43 1172    1    0    0    0]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sc0cEYKJTDtf","executionInfo":{"status":"ok","timestamp":1636035591960,"user_tz":240,"elapsed":768,"user":{"displayName":"Ziyi Xiong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02400447891473674622"}},"outputId":"ad264be3-b262-48e4-a282-00a16da08185"},"source":["# Process French sentences in the same way\n","\n","max_fr_words = 5000\n","max_fr_len = 10\n","fr_tokenizer = keras.preprocessing.text.Tokenizer(filters='', num_words = max_fr_words)\n","\n","# ATTENTION: always finish with fit_on_texts before moving on\n","fr_tokenizer.fit_on_texts(raw_data_fr_in)\n","fr_tokenizer.fit_on_texts(raw_data_fr_out)\n","print(\"Total number of French words: \", len(fr_tokenizer.word_index))\n","\n","data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)\n","data_fr_in = keras.preprocessing.sequence.pad_sequences(data_fr_in,\\\n","                                                        maxlen=max_fr_len, \\\n","                                                        padding='post')\n","\n","data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)\n","data_fr_out = keras.preprocessing.sequence.pad_sequences(data_fr_out,\\\n","                                                            maxlen=max_fr_len, \\\n","                                                            padding='post')\n","# print a sample sentence after preprocessing\n","data_fr_in[:3]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of French words:  6299\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[  66,    7,    7,   39,  114,   26,   77,  181,  182,    1],\n","       [   2,   17,   97,  170,   48,  815,    1,    0,    0,    0],\n","       [   2,   12,    5,  360,    6, 1566,   95,  361,    1,    0]],\n","      dtype=int32)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPCKtESXTDtg","executionInfo":{"status":"ok","timestamp":1636035594820,"user_tz":240,"elapsed":183,"user":{"displayName":"Ziyi Xiong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02400447891473674622"}},"outputId":"1e385d53-b735-4608-d056-678c6aa9e47a"},"source":["# Create the reversal mapping between indexes and words\n","reverse_fr_word_index ={fr_tokenizer.word_index[w] : w \\\n","                        for w in  fr_tokenizer.word_index}\n","print(\"index of symbol <start> :\", fr_tokenizer.word_index[\"<start>\"])\n","print(\"index of symbol <end> :\", fr_tokenizer.word_index[\"<end>\"])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["index of symbol <start> : 2\n","index of symbol <end> : 3\n"]}]},{"cell_type":"markdown","metadata":{"id":"Upb8hh2UTDtg"},"source":["### (2) Define \"Teacher Forcing\" Model for Training Process"]},{"cell_type":"markdown","metadata":{"id":"EtQPaUm2TDth"},"source":["2) Train a basic LSTM-based Seq2Seq model to predict `decoder_outputs` given `encoder_inputs` and `decoder_inputs`. Our model uses `teacher forcing`.\n","\n","\n","And here is how the data’s shape changes at each layer. Often keeping track of the data’s shape is extremely helpful not to make silly mistakes, just like stacking up Lego pieces.\n","\n","Here we start with a simple encoder: only one layer LSTM, unidirectional.\n","\n","Task for you:\n","`Can you modify the model to allow multiple layers and bidirectional?`"]},{"cell_type":"code","metadata":{"id":"SEprU1ODTDth"},"source":["import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i2j_DkOHWk0z"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NyDE9y-3TDti"},"source":["max_en_words = 5000\n","max_en_len = 10\n","max_fr_len = 10\n","max_fr_words = 5000\n","latent_dim = 100   #i.e. rnn_size\n","batch_size = 32"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YTX3BZz3TDti"},"source":["#### Encoder ####\n","<img width=\"60%\" alt=\"Seq2seq inference\" src=\"https://machinetalk.org/wp-content/uploads/2019/03/data_shapes-1.png\" width=\"80%\"> \n"]},{"cell_type":"code","metadata":{"id":"c3huf1cNTDti"},"source":["# vocab_size: the total number of words in vocabulary\n","# embedding_size: word embedding dimension\n","# latent_dim: RNN hidden state dimension\n","\n","class EncoderLSTM(nn.Module):\n","    \n","  def __init__(self, vocab_size, embedding_size, hidden_size):\n","    \n","    super(EncoderLSTM, self).__init__()\n","    \n","    self.vocab_size = vocab_size\n","\n","    self.embedding_size = embedding_size\n","    \n","    self.hidden_size = hidden_size\n","\n","    self.embedding = nn.Embedding(self.vocab_size, self.embedding_size,padding_idx=0)\n","    \n","    # The input to LSTM should be [batch_size, seq_length, embedding_size]\n","    self.LSTM = nn.LSTM(input_size = self.embedding_size, \\\n","                        hidden_size = self.hidden_size,\n","                        batch_first = True)\n","\n","  \n","  def forward(self, x):\n","    \n","    # the shape of x is [batch_size, seq_length]\n","    x = self.embedding(x)  \n","    # after embeding, the shape of x is: [batch_size, seq_length, embedding_size]\n","    \n","    # We don't care about output. We only need states\n","    outputs, (hidden_state, cell_state) = self.LSTM(x)\n","    # hidden_state shape: [1, batch_size, hidden_size]\n","    # cell_state shape: [1, batch_size, hidden_size]\n","    \n","    #print(hidden_state.shape)\n","    #print(cell_state.shape)\n","    \n","    return outputs, hidden_state, cell_state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtwjkm3UTDtj","executionInfo":{"status":"ok","timestamp":1636035657549,"user_tz":240,"elapsed":423,"user":{"displayName":"Ziyi Xiong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02400447891473674622"}},"outputId":"ba37c877-36b7-4478-9370-92aade0de452"},"source":["encoder = EncoderLSTM(vocab_size=max_en_words,\n","                      embedding_size=latent_dim,\n","                      hidden_size=latent_dim)\n","\n","from torchinfo import summary \n","summary(encoder,input_size=(batch_size,max_en_len),\n","       dtypes=[torch.long])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","EncoderLSTM                              --                        --\n","├─Embedding: 1-1                         [32, 10, 100]             500,000\n","├─LSTM: 1-2                              [32, 10, 100]             80,800\n","==========================================================================================\n","Total params: 580,800\n","Trainable params: 580,800\n","Non-trainable params: 0\n","Total mult-adds (M): 41.86\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.51\n","Params size (MB): 2.32\n","Estimated Total Size (MB): 2.84\n","=========================================================================================="]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"az0K4J_LTDtj"},"source":["#### decoder ####\n","<img width=\"60%\" alt=\"Seq2seq inference\" src=\"https://machinetalk.org/wp-content/uploads/2019/03/data_shapes-2.png\" width=\"80%\"> \n","\n"]},{"cell_type":"code","metadata":{"id":"MNMtvBiDTDtj"},"source":["# vocab_size: the total number of words in vocabulary\n","# embedding_size: word embedding dimension\n","# latent_dim: RNN hidden state dimension\n","\n","class DecoderLSTM(nn.Module):\n","    \n","  def __init__(self, vocab_size, embedding_size, hidden_size):\n","    \n","    super(DecoderLSTM, self).__init__()\n","    \n","    self.vocab_size = vocab_size\n","\n","    self.embedding_size = embedding_size\n","    \n","    self.hidden_size = hidden_size\n","\n","    self.embedding = nn.Embedding(self.vocab_size, self.embedding_size,padding_idx=0)\n","    \n","    # The input to LSTM should be [batch_size, seq_length, embedding_size]\n","    self.LSTM = nn.LSTM(input_size = self.embedding_size, \\\n","                        hidden_size = self.hidden_size,\n","                        batch_first = True)\n","\n","    self.dense = nn.Linear(in_features = self.hidden_size, \n","                        out_features = self.vocab_size)\n","\n","\n","  def forward(self, x, hidden_state, cell_state):\n","    \n","    \n","    x = self.embedding(x)\n","\n","    # LSTM will be initialized with encoder states\n","    outputs, (h, c) = self.LSTM(x, (hidden_state, cell_state))\n","    # outputs shape is [batch, seq_length, hidden_state]\n","    \n","    predictions = self.dense(outputs)\n","    # prediction shape is [batch, seq_length, vocab_size]\n","\n","    return predictions, h, c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R10iQbTfTDtk","executionInfo":{"status":"ok","timestamp":1636035664271,"user_tz":240,"elapsed":156,"user":{"displayName":"Ziyi Xiong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02400447891473674622"}},"outputId":"b5728396-232f-4de1-a94a-259da3555e12"},"source":["decoder = DecoderLSTM(vocab_size=max_fr_words,\\\n","                      embedding_size=latent_dim,\\\n","                      hidden_size=latent_dim)\n","\n","summary(decoder,input_size=[(batch_size,max_fr_len),\\\n","                            (1,batch_size,latent_dim),\\\n","                            (1,batch_size,latent_dim)],\\\n","        dtypes=[torch.long,torch.float,torch.float])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","DecoderLSTM                              --                        --\n","├─Embedding: 1-1                         [32, 10, 100]             500,000\n","├─LSTM: 1-2                              [32, 10, 100]             80,800\n","├─Linear: 1-3                            [32, 10, 5000]            505,000\n","==========================================================================================\n","Total params: 1,085,800\n","Trainable params: 1,085,800\n","Non-trainable params: 0\n","Total mult-adds (M): 58.02\n","==========================================================================================\n","Input size (MB): 0.03\n","Forward/backward pass size (MB): 13.31\n","Params size (MB): 4.34\n","Estimated Total Size (MB): 17.68\n","=========================================================================================="]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"zZvPsEMfTDtm"},"source":["#### Connect Encoder and Decoder to Create Seq2Seq Model ####"]},{"cell_type":"code","metadata":{"id":"DdRFQ_KSTDtn"},"source":["class Seq2Seq(nn.Module):\n","    \n","  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n","    \n","    super(Seq2Seq, self).__init__()\n","    \n","    self.Encoder = Encoder_LSTM\n","    self.Decoder = Decoder_LSTM\n","\n","  def forward(self, encoder_input, decoder_input):\n","    \n","    _, hidden_state, cell_state = self.Encoder(encoder_input)\n","    \n","    predictions, _, _ = self.Decoder(decoder_input, hidden_state, cell_state)\n","        \n","    return predictions\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"402ZnbfOTDto"},"source":["model = Seq2Seq(encoder, decoder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lPEF4p9STDtp"},"source":["#### Create Dataset and Training Function ####"]},{"cell_type":"markdown","metadata":{"id":"d8Iu7R0OTDtp"},"source":["Now you can compile and fit the model as usual. Note the matching between tensors and variables:\n","- `encoder_inputs` <-> `data_en`\n","- `decoder_inputs` <-> `data_fr_in`\n","- `decoder_outputs` <-> `data_fr_out`"]},{"cell_type":"code","metadata":{"id":"lxlSiFSETDtq"},"source":["class NTM_dataset(Dataset):\n","    \n","    def __init__(self,data_en, data_fr_in,data_fr_out):\n","        \n","        self.length = len(data_en)\n","        \n","        self.encoder_input = torch.IntTensor(data_en)\n","        self.decoder_input = torch.IntTensor(data_fr_in)\n","        \n","        # for CrossEntropyLoss, decoder_output must have Long data type\n","        self.decoder_output = torch.LongTensor(data_fr_out)\n","    \n","    def __getitem__(self, index):\n","        return self.encoder_input[index], \\\n","               self.decoder_input[index],\\\n","               self.decoder_output[index]\n","    \n","    def __len__(self):\n","        return self.length "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4x0TxU7TDtq"},"source":["dataset = NTM_dataset(data_en, data_fr_in, data_fr_out)\n","\n","test_size = int(len(data_en) * 0.2)\n","train_size = len(data_en) - test_size\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, \\\n","                                                            [train_size, test_size])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofAcEfmWTDtq","executionInfo":{"status":"ok","timestamp":1636035679536,"user_tz":240,"elapsed":4,"user":{"displayName":"Ziyi Xiong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02400447891473674622"}},"outputId":"88e66965-605e-466f-ec57-a056c2c3e669"},"source":["len(train_dataset)\n","len(test_dataset)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8000"]},"metadata":{},"execution_count":23},{"output_type":"execute_result","data":{"text/plain":["2000"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"2nkhNF_4TDtr"},"source":["# Define a function to train the model \n","def train_model(model, train_dataset, test_dataset, device, lr=0.0005, epochs=20, batch_size=32):\n","    \n","    # construct dataloader\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","    # move model to device\n","    model = model.to(device)\n","\n","    # history\n","    history = {'train_loss': [],\n","               'train_acc': [],\n","               'test_loss': [],\n","               'test_acc': []}\n","    # setup loss function and optimizer\n","    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # training loop\n","    print('Training Start')\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0\n","        train_acc = 0\n","        test_loss = 0\n","        test_acc = 0\n","\n","        for encoder_input, decoder_input, decoder_output in train_loader:\n","            \n","            # move data to device\n","            encoder_input = encoder_input.to(device)\n","            decoder_input = decoder_input.to(device)\n","            decoder_output = decoder_output.to(device)\n","            \n","            # forward\n","            outputs = model(encoder_input, decoder_input)  # batch_size, max_fr_len (i.e. seq_len), fr_vocab_size\n","            \n","            _, pred = torch.max(outputs, dim = -1)\n","            \n","            #reshape output to batch_size * seq_len, fr_vocab_size since the loss looks for 2-dimensional input                \n","            cur_train_loss = criterion(outputs.view(-1 , max_fr_words), decoder_output.view(-1))\n","            \n","            # reshape pred & decoder ouput t to calculate acc of each predicted words\n","            cur_train_acc = (pred.view(-1) == decoder_output.view(-1)) \n","            cur_train_acc = cur_train_acc.sum().item()/len(cur_train_acc)\n","                \n","            # backward\n","            cur_train_loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            # loss and acc\n","            train_loss += cur_train_loss\n","            train_acc += cur_train_acc\n","\n","        # test start\n","        model.eval()\n","        with torch.no_grad():\n","            \n","            for encoder_input, decoder_input, decoder_output in test_loader:\n","            \n","                # move data to device\n","                encoder_input = encoder_input.to(device)\n","                decoder_input = decoder_input.to(device)\n","                decoder_output = decoder_output.to(device)\n","            \n","                # forward\n","                outputs = model(encoder_input, decoder_input)  # batch_size, max_fr_len (i.e. seq_len), fr_vocab_size\n","            \n","                _, pred = torch.max(outputs, dim = -1)\n","            \n","                #reshape output to batch_size, seq_len * fr_vocab_size since the loss looks for 2-dimensional input                \n","                cur_test_loss = criterion(outputs.view(-1, max_fr_words), decoder_output.view(-1))\n","                \n","                cur_test_acc = (pred.view(-1) == decoder_output.view(-1)) \n","                cur_test_acc = cur_test_acc.sum().item()/len(cur_test_acc)\n","                \n","                # loss and acc\n","                test_loss += cur_test_loss\n","                test_acc += cur_test_acc\n","\n","        # epoch output\n","        train_loss = (train_loss/len(train_loader)).item()\n","        train_acc = train_acc/len(train_loader)\n","        val_loss = (test_loss/len(test_loader)).item()\n","        val_acc = test_acc/len(test_loader)\n","        history['train_loss'].append(train_loss)\n","        history['train_acc'].append(train_acc)\n","        history['test_loss'].append(val_loss)\n","        history['test_acc'].append(val_acc)\n","        print(f\"Epoch:{epoch + 1} / {epochs}, train loss:{train_loss:.4f} train_acc:{train_acc:.4f}, valid loss:{val_loss:.4f} valid acc:{val_acc:.4f}\")\n","    \n","    return history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59FZGTObTDts","executionInfo":{"status":"ok","timestamp":1636036288617,"user_tz":240,"elapsed":598515,"user":{"displayName":"Ziyi Xiong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02400447891473674622"}},"outputId":"c606f4c0-1a23-4e57-9572-f0be3d6d1e38"},"source":["history = train_model(model=model,\n","                      train_dataset = train_dataset,\n","                      test_dataset = test_dataset,\n","                      device=device,\n","                      epochs=50,\n","                      batch_size=64)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Start\n","Epoch:1 / 50, train loss:4.6214 train_acc:0.3248, valid loss:4.0966 valid acc:0.3900\n","Epoch:2 / 50, train loss:3.9483 train_acc:0.3983, valid loss:3.8776 valid acc:0.4153\n","Epoch:3 / 50, train loss:3.7353 train_acc:0.4239, valid loss:3.7202 valid acc:0.4441\n","Epoch:4 / 50, train loss:3.5653 train_acc:0.4442, valid loss:3.5986 valid acc:0.4556\n","Epoch:5 / 50, train loss:3.4249 train_acc:0.4578, valid loss:3.5166 valid acc:0.4640\n","Epoch:6 / 50, train loss:3.3050 train_acc:0.4661, valid loss:3.4335 valid acc:0.4723\n","Epoch:7 / 50, train loss:3.1983 train_acc:0.4740, valid loss:3.3893 valid acc:0.4771\n","Epoch:8 / 50, train loss:3.0998 train_acc:0.4840, valid loss:3.3198 valid acc:0.4868\n","Epoch:9 / 50, train loss:3.0084 train_acc:0.4932, valid loss:3.2747 valid acc:0.4925\n","Epoch:10 / 50, train loss:2.9244 train_acc:0.5002, valid loss:3.2428 valid acc:0.4972\n","Epoch:11 / 50, train loss:2.8442 train_acc:0.5063, valid loss:3.2301 valid acc:0.4996\n","Epoch:12 / 50, train loss:2.7707 train_acc:0.5126, valid loss:3.1817 valid acc:0.5049\n","Epoch:13 / 50, train loss:2.7019 train_acc:0.5179, valid loss:3.1711 valid acc:0.5053\n","Epoch:14 / 50, train loss:2.6367 train_acc:0.5243, valid loss:3.1418 valid acc:0.5103\n","Epoch:15 / 50, train loss:2.5752 train_acc:0.5300, valid loss:3.1140 valid acc:0.5126\n","Epoch:16 / 50, train loss:2.5145 train_acc:0.5356, valid loss:3.1116 valid acc:0.5138\n","Epoch:17 / 50, train loss:2.4583 train_acc:0.5408, valid loss:3.0900 valid acc:0.5170\n","Epoch:18 / 50, train loss:2.4028 train_acc:0.5458, valid loss:3.0638 valid acc:0.5209\n","Epoch:19 / 50, train loss:2.3510 train_acc:0.5507, valid loss:3.0665 valid acc:0.5227\n","Epoch:20 / 50, train loss:2.3000 train_acc:0.5564, valid loss:3.0384 valid acc:0.5248\n","Epoch:21 / 50, train loss:2.2515 train_acc:0.5612, valid loss:3.0628 valid acc:0.5258\n","Epoch:22 / 50, train loss:2.2035 train_acc:0.5664, valid loss:3.0242 valid acc:0.5294\n","Epoch:23 / 50, train loss:2.1568 train_acc:0.5715, valid loss:3.0402 valid acc:0.5283\n","Epoch:24 / 50, train loss:2.1132 train_acc:0.5771, valid loss:3.0287 valid acc:0.5300\n","Epoch:25 / 50, train loss:2.0676 train_acc:0.5826, valid loss:3.0205 valid acc:0.5293\n","Epoch:26 / 50, train loss:2.0259 train_acc:0.5866, valid loss:3.0287 valid acc:0.5325\n","Epoch:27 / 50, train loss:1.9841 train_acc:0.5925, valid loss:3.0036 valid acc:0.5333\n","Epoch:28 / 50, train loss:1.9435 train_acc:0.5975, valid loss:3.0105 valid acc:0.5336\n","Epoch:29 / 50, train loss:1.9048 train_acc:0.6020, valid loss:3.0021 valid acc:0.5332\n","Epoch:30 / 50, train loss:1.8666 train_acc:0.6077, valid loss:3.0113 valid acc:0.5346\n","Epoch:31 / 50, train loss:1.8268 train_acc:0.6127, valid loss:2.9917 valid acc:0.5389\n","Epoch:32 / 50, train loss:1.7905 train_acc:0.6184, valid loss:3.0093 valid acc:0.5380\n","Epoch:33 / 50, train loss:1.7540 train_acc:0.6236, valid loss:3.0108 valid acc:0.5363\n","Epoch:34 / 50, train loss:1.7167 train_acc:0.6301, valid loss:3.0382 valid acc:0.5372\n","Epoch:35 / 50, train loss:1.6840 train_acc:0.6356, valid loss:3.0077 valid acc:0.5347\n","Epoch:36 / 50, train loss:1.6478 train_acc:0.6420, valid loss:2.9933 valid acc:0.5383\n","Epoch:37 / 50, train loss:1.6137 train_acc:0.6477, valid loss:3.0223 valid acc:0.5375\n","Epoch:38 / 50, train loss:1.5818 train_acc:0.6544, valid loss:3.0142 valid acc:0.5383\n","Epoch:39 / 50, train loss:1.5490 train_acc:0.6612, valid loss:3.0151 valid acc:0.5390\n","Epoch:40 / 50, train loss:1.5161 train_acc:0.6673, valid loss:3.0442 valid acc:0.5332\n","Epoch:41 / 50, train loss:1.4839 train_acc:0.6743, valid loss:3.0359 valid acc:0.5380\n","Epoch:42 / 50, train loss:1.4550 train_acc:0.6801, valid loss:3.0438 valid acc:0.5403\n","Epoch:43 / 50, train loss:1.4227 train_acc:0.6865, valid loss:3.0538 valid acc:0.5380\n","Epoch:44 / 50, train loss:1.3942 train_acc:0.6933, valid loss:3.0572 valid acc:0.5365\n","Epoch:45 / 50, train loss:1.3645 train_acc:0.6990, valid loss:3.0686 valid acc:0.5385\n","Epoch:46 / 50, train loss:1.3370 train_acc:0.7048, valid loss:3.0436 valid acc:0.5400\n","Epoch:47 / 50, train loss:1.3067 train_acc:0.7116, valid loss:3.0751 valid acc:0.5369\n","Epoch:48 / 50, train loss:1.2812 train_acc:0.7175, valid loss:3.0887 valid acc:0.5355\n","Epoch:49 / 50, train loss:1.2512 train_acc:0.7250, valid loss:3.0957 valid acc:0.5391\n","Epoch:50 / 50, train loss:1.2258 train_acc:0.7306, valid loss:3.0845 valid acc:0.5370\n"]}]},{"cell_type":"markdown","metadata":{"id":"kKmoE7FpTDtt"},"source":["### Define a model for Inference (i.e. Testing Translation)"]},{"cell_type":"markdown","metadata":{"id":"E7n9m2Z5TDtt"},"source":["3) Decode some sentences to check that the model is working (i.e. turn samples from `data_en` into corresponding samples from `data_fr_out`).\n","\n","Because the training process and `inference process` (decoding sentences) are quite different, we use different models for both, albeit they all leverage the same inner layers where **all the weights have been trained**. \n"]},{"cell_type":"markdown","metadata":{"id":"nejfMQbBTDtt"},"source":["#### Inference without \"teacher forcing\"\n","<img width=\"60%\" alt=\"Seq2seq inference\" src=\"https://blog.keras.io/img/seq2seq/seq2seq-inference.png\" width=\"80%\"> \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1__vDoeLTDtt"},"source":["Now we define a function which returns the translated sentences given an input sentence. \n","- The decoder translates word by word starting with an decoder input `[ <start> ]`\n","- When a word, say $w_t$ is translated, the next decode input at $t+1$ is $[ w_t ]$\n","- It continues until either `<end>` is generated or the `max_fr_len` number of words are generated."]},{"cell_type":"code","metadata":{"id":"8Mpqsli0TDtt"},"source":["def decode_sequence(model, input_seq, device):  # input_seq is a English sentence\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","        # convert input_seq to tensor\n","        input_seq = torch.IntTensor(input_seq).to(device)\n","\n","        # Encode the input as state vectors.\n","        hidden_state, cell_state = model.Encoder(input_seq)\n","\n","\n","        # Generate empty target sequence of (batch=1, length=1). (i.e. we translate word by word)\n","        target_seq = torch.empty((1,1), dtype=torch.int32, device = device)\n","\n","        # Populate the start symbol of target sequence with the start character.\n","        target_seq = target_seq.fill_(fr_tokenizer.word_index[\"<start>\"])\n","\n","        target_seq = target_seq.to(device)\n","\n","        # Generate word by word using the encode state and the last \n","        # generated word\n","\n","        decoded_sentence = []\n","\n","        while True:\n","\n","            # get decode ouput and hidden states, output shape is [1,1,5000]\n","            output_tokens, h, c = model.Decoder(target_seq, hidden_state, cell_state)\n","\n","\n","            # Get the most likely word\n","            _, sampled_token_index = torch.max(output_tokens, dim = -1)\n","\n","            # flatten the token_index and convert to numpy number\n","            sampled_token_index = sampled_token_index.view(-1).item()\n","\n","            # Look up the word by id\n","            sampled_word = reverse_fr_word_index[sampled_token_index]\n","\n","            # append the word to decoded sentence\n","            decoded_sentence.append(sampled_word)\n","\n","            # Exit condition: either hit max length\n","            # or find stop character.\n","            if (sampled_word == '<end>' or len(decoded_sentence) == max_fr_len):\n","                break\n","\n","            # Update the target sequence with newly generated word.\n","            target_seq = target_seq.fill_(sampled_token_index)\n","\n","            # Update states\n","            hidden_state, cell_state = h, c\n","\n","    return ' '.join(decoded_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"na1hHqKtTDtu"},"source":["# Now let's test\n","\n","test = text.iloc[10000:10010]\n","test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IeIZqwfrTDtu"},"source":["# preprocess test data\n","data_test = en_tokenizer.texts_to_sequences(test[\"en\"])\n","data_test = keras.preprocessing.sequence.pad_sequences(data_test,\\\n","                                                     maxlen=max_en_len, \\\n","                                                     padding='post')\n","print(data_test[:3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3WclY3WTDtu"},"source":["for i in range(len(data_test)):\n","    #data_test[i].shape\n","    fr = decode_sequence(data_test[i][None,:], device)\n","    print(\"\\nEn: \", test.iloc[i][\"en\"])\n","    print(\"Fr: \", test.iloc[i][\"fr\"])\n","    print(\"Translated: \", fr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9waeKFrnTDtv"},"source":["### Seq2Seq model with attention"]},{"cell_type":"markdown","metadata":{"id":"tT_9FizTTDtv"},"source":["Now, let’s talk about attention mechanism. What is it and why do we need it?\n","\n","- Difficult to remember and process long complicated context\n","- Struggle with difference in syntax structures used by languages\n","\n","For detailed explanation, check: https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/\n","\n","\n","There are different ways to implement such an attention mechanism. You can find the Torch implemetation at https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lb3_rzbPTDtw"},"source":["## Take aways"]},{"cell_type":"markdown","metadata":{"id":"4YJAvafWTDtw"},"source":["* Neural Machine Translation contains an encoder and a decoder which both are LSTM layers\n","* Attention mechanism can help align encoder and decoder outputs\n","* You can try the following steps to enhance the translation models:\n","    - Use bidirectional LSTM\n","    - Try other more advanced attention mechanisms\n","    - Also, you may need to work on masking when allocating attention scores."]}]}