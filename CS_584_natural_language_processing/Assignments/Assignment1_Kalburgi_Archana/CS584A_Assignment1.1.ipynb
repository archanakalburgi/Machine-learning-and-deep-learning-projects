{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9c3b23",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 1 -- Text Classification (Machine Learning and NLP Basics)\n",
    "\n",
    "#### Name: Archana Kalburgi\n",
    "#### Stevens ID: 10469491"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8619d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part A: Binary Classification (80 Points)\n",
    "\n",
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement the data loading, preprocessing, tokenization, and TF-IDF feature extraction.\n",
    "3. Implement Logistic Regression model, evaluation metrics, SGD, and Mini-batch GD.\n",
    "4. Implement Cross-validation to choose the best lambda.\n",
    "5. Analysis the results in the Conlusion part.\n",
    "\n",
    "**Before you start**\n",
    "- Please read the code very carefully.\n",
    "- Install these packages (jupyterlab, nltk, numpy, scikit-learn, and matplotlib) using the following command.\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
    "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb307f36",
   "metadata": {},
   "source": [
    "## 1. Data Processing (20 points)\n",
    "\n",
    "* Download the dataset from Canvas\n",
    "* Load data to text and labels\n",
    "* Preprocessing\n",
    "* Tokenization\n",
    "* Split data\n",
    "* Feature extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006ba0d",
   "metadata": {},
   "source": [
    "#### Download NLTK stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "bf023a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to a1-data/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "nltk_path = os.path.join('a1-data', 'nltk')\n",
    "nltk.download('stopwords', download_dir=nltk_path)\n",
    "nltk.data.path.append(nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "aa147656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def print_line(*args):\n",
    "    \"\"\" Inline print and go to the begining of line\n",
    "    \"\"\"\n",
    "    args1 = [str(arg) for arg in args]\n",
    "    str_ = ' '.join(args1)\n",
    "    sys.stdout.write(str_ + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "49bca414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d3d668",
   "metadata": {},
   "source": [
    "### 1.1 Load data (5 Points)\n",
    "\n",
    "- Load sentences and labels\n",
    "- Transform string labels into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "c45103ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_label(data_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\" Load sentences and labels from the specified path\n",
    "    Args:\n",
    "        data_path: data_path: path to the data file, e.g., 'a1-data/SMSSpamCollection'\n",
    "        sentences: the raw text list of all sentences\n",
    "    Returns:\n",
    "        labels: the label list of all sentences\n",
    "    \"\"\"\n",
    "    sentences, labels = [], []\n",
    "    # Start your code here (load text and label from files)\n",
    "    \n",
    "    with open(\"data/SMSSpamCollection\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    mapper = map(lambda x: x.split(\"\\t\"), lines)\n",
    "    \n",
    "    for item in mapper:\n",
    "        labels.append(item[0])\n",
    "        sentences.append(item[1])\n",
    "\n",
    "    # End\n",
    "    return sentences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "b8c6acef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {'ham': 0, 'spam': 1}\n",
      "Number of sentences and labels: 5574 5574\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('a1-data', 'SMSSpamCollection')\n",
    "sentences, labels = load_sentence_label(data_path)\n",
    "\n",
    "label_map = {}\n",
    "for label in sorted(list(set(labels))):\n",
    "    label_map[label] = len(label_map)\n",
    "labels = np.array([label_map[label] for label in labels], dtype=int)\n",
    "sentences = np.array(sentences, dtype=object)\n",
    "\n",
    "print('Label map:', label_map)\n",
    "print('Number of sentences and labels:', len(sentences), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370f03e6",
   "metadata": {},
   "source": [
    "#### Split the data into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "100229ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(sentences: np.ndarray,\n",
    "                     labels: np.ndarray,\n",
    "                     test_ratio: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Split the sentences and labels into training and test data by shuffling\n",
    "    Args:\n",
    "        sentences: A numpy array containing all sentences\n",
    "        labels: A number array containing label ids\n",
    "        test_ratio: A float number to calculate the number of test data\n",
    "\n",
    "    Returns:\n",
    "        train_sentences: A numpy array containing all training sentences\n",
    "        train_labels: A number array containing all training label ids\n",
    "        test_sentences: A numpy array containing all test sentences\n",
    "        test_labels: A number array containing all test label ids\n",
    "    \"\"\"\n",
    "    assert 0 < test_ratio < 1\n",
    "    assert len(sentences) == len(labels)\n",
    "\n",
    "    train_index, test_index = [], []\n",
    "    # Start your code here (split the index for training and test)\n",
    "    end = math.ceil((1-test_ratio)*len(labels))\n",
    "    train_index = [i for i in range(end)]\n",
    "    test_index = [i for i in range(len(labels)-end)]\n",
    "    np.random.shuffle(train_index)\n",
    "    np.random.shuffle(train_index)\n",
    "    # End\n",
    "\n",
    "    train_sentences, train_labels = sentences[train_index], labels[train_index]\n",
    "    test_sentences, test_labels = sentences[test_index], labels[test_index]\n",
    "    return train_sentences, train_labels, test_sentences, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "a73f9b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 4014\n",
      "Validation data length: 446\n",
      "Test data length: 1114\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.1\n",
    "(train_sentences, train_labels,\n",
    "    test_sentences, test_labels) = train_test_split(sentences, labels, test_ratio)\n",
    "(train_sentences, train_labels,\n",
    "    valid_sentences, valid_labels) = train_test_split(train_sentences, train_labels, valid_ratio)\n",
    "\n",
    "print('Training data length:', len(train_sentences))\n",
    "print('Validation data length:', len(valid_sentences))\n",
    "print('Test data length:', len(test_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "b7af1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_label(labels: np.ndarray, label_map: dict[str, int]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: The labels of a dataset \n",
    "        label_map: The mapping from label to label id\n",
    "    Returns:\n",
    "        label_count: The mapping from label to its count\n",
    "    \"\"\"\n",
    "    label_count = {key: 0 for key in label_map.keys()}\n",
    "    # Start your code here (count the number of each label)\n",
    "    ham_count = len(list(filter(lambda x : x==0, labels)))\n",
    "    spam_count = len(list(filter(lambda x : x==1, labels)))\n",
    "    label_count['ham']=ham_count\n",
    "    label_count['spam']=spam_count\n",
    "    # End\n",
    "    return label_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "35defd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: {'ham': 3473, 'spam': 541}\n",
      "Validation: {'ham': 385, 'spam': 61}\n",
      "Test: {'ham': 946, 'spam': 168}\n"
     ]
    }
   ],
   "source": [
    "print('Training:', count_label(train_labels, label_map))\n",
    "print('Validation:', count_label(valid_labels, label_map))\n",
    "print('Test:', count_label(test_labels, label_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721d097",
   "metadata": {},
   "source": [
    "#### Dataset statistics\n",
    "Fill this table with the statistics you just printed (double click this cell to edit)\n",
    "\n",
    "|                | ham    | spam   | Total  |\n",
    "|:--------------:|--------|--------|--------|\n",
    "|  **Training**  |  3473  |  541   | 4,014  |\n",
    "| **Validation** |   385  |   61   |  446   |\n",
    "|    **Test**    |   946  |  168   | 1,114  |\n",
    "|    **Total**   | 4,804  |  770   | 5,574  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dca930",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess (Fill the code: 5 points)\n",
    "In this section, you need to remove all the unrelated characters, including punctuation, urls, and numbers. Please fill up the functions and test them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "9d695e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "\n",
    "    def apply(self, sentence: str) -> str:\n",
    "        \"\"\" Apply the preprocessing rules to the sentence\n",
    "        Args:\n",
    "            sentence: raw sentence\n",
    "        Returns:\n",
    "            sentence: clean sentence\n",
    "        \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        if self.url:\n",
    "            sentence = Preprocessor.remove_url(sentence)\n",
    "        if self.punctuation:\n",
    "            sentence = Preprocessor.remove_punctuation(sentence)\n",
    "        if self.number:\n",
    "            sentence = Preprocessor.remove_number(sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(sentence: str) -> str:\n",
    "        \"\"\" Remove punctuations in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible punctuations\n",
    "        Returns:\n",
    "            sentence: sentence without punctuations\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        sentence = sentence.translate(str.maketrans('','', string.punctuation))   \n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(sentence: str) -> str:\n",
    "        \"\"\" Remove urls in text with re\n",
    "        Args:\n",
    "            sentence: sentence with possible urls\n",
    "        Returns:\n",
    "            sentence: sentence without urls\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        pattern = re.compile(\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\")\n",
    "        sentence = pattern.sub('', sentence)\n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_number(sentence: str) -> str:\n",
    "        \"\"\" Remove numbers in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible numbers\n",
    "        Returns:\n",
    "            sentence: sentence without numbers\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        sentence = sentence.translate(str.maketrans('','',string.digits))\n",
    "        # End\n",
    "        return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991ded7",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "e29f1568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"interest rates are trimmed to by the south african central bank but the lack of warning hits the rand and surprises markets\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{clean_sentence}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a398179",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization (Fill the code: 5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "cdf04eb9-f0a4-41bd-91ac-ce28672a288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "ff2ad8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"won't\", 'had', 'against', 'such', 'hadn', 'hers', 'over', 'between', 'myself', \"hasn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "print(list(stopwords_set)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "37ff9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str) -> List[str]:\n",
    "    \"\"\" Tokenize a sentence into tokens (words)\n",
    "    Args:\n",
    "        sentence: clean sentence\n",
    "    Returns:\n",
    "        tokens\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    # Start your code here\n",
    "    #     Step 1. Split sentence into words\n",
    "    #     Step 2. Extract word stem using the defined stemmer (PorterStemmer) by calling stemmer.stem(word)\n",
    "    #     Step 3. Remove stop words using the defined stopwords_set\n",
    "    \n",
    "    tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # End\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97704226",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "75ca92ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"['interest', 'rate', 'trimmed', 'south', 'african', 'central', 'bank', 'lack', 'warning', 'hit', 'rand', 'surprise', 'market']\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "tokens = tokenize(clean_sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{tokens}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44120cb",
   "metadata": {},
   "source": [
    "### 1.5 Feature Extraction (Fill the code: 5 points)\n",
    "\n",
    "TF-IDF:\n",
    "$$\\text{TF-IDF}(t, d) = \\frac{f_{t, d}}{\\sum_{t'}{f_{t', d}}} \\times \\log{\\frac{N}{n_t}}$$\n",
    "\n",
    "- $t$: A term\n",
    "- $d$: A document. Here, we regard a sentence as a document\n",
    "- $f_{t, d}$: Number of term $t$ in $d$\n",
    "- $N$: Number of document\n",
    "- $n_t$: Number of document containing $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "9251e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class TfIdfEncoder:\n",
    "    def __init__(self):\n",
    "        self.vocab = defaultdict(int)\n",
    "        self.token2index = {}\n",
    "        self.df = defaultdict(int)\n",
    "        self.num_doc = 0\n",
    "        self.processor = Preprocessor()\n",
    "\n",
    "    def fit(self, sentences: Union[List[str], np.ndarray]) -> int:\n",
    "        \"\"\" Using the given texts to store key information in TF-IDF calculation\n",
    "            In this function, you are required to implement the fitting process.\n",
    "                1. Construct the vocabulary and store the frequency of tokens (self.vocab).\n",
    "                2. Construct the document frequency map to tokens (self.df).\n",
    "                3. Construct the token to index map based on the frequency.\n",
    "                   The token with a higher frequency has the smaller index\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            token_num\n",
    "        \"\"\"\n",
    "        self.num_doc = len(sentences)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == len(sentences) - 1:\n",
    "                print_line('Fitting TF-IDF encoder:', (i + 1), '/', len(sentences))\n",
    "            \n",
    "            # Start your code here (step 1 & 2)\n",
    "            clean_sentences = self.processor.apply(sentence)\n",
    "            tokens = tokenize(clean_sentences)\n",
    "            \n",
    "            token_counter = Counter(tokens)\n",
    "            for i,j in token_counter.items():\n",
    "                self.vocab[i] += j\n",
    "                self.df[i] += 1\n",
    "            \n",
    "            # End\n",
    "            \n",
    "        print_line('\\n')\n",
    "        # Start your code here (Step 3)\n",
    "        self.vocab = {i:j for i,j in sorted(self.vocab.items(), key=lambda x:x[1], reverse=True)}\n",
    "        self.vocab = {key: self.vocab[key] for key in list(self.vocab.keys())}\n",
    "        self.token2index = {key: idx for idx, key in enumerate(self.vocab.keys())}\n",
    "        # End\n",
    "        token_num = len(self.token2index) \n",
    "        print('The number of distinct tokens:', token_num)\n",
    "        return token_num\n",
    "\n",
    "    def encode(self, sentences: Union[List[str], np.ndarray]) -> np.ndarray:\n",
    "        \"\"\" Encode the sentences into TF-IDF feature vector\n",
    "            Note: if a token in a sentence does not exist in the fit encoder, we just ignore it.\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            features: A (n x token_num) matrix, where n is the number of sentences\n",
    "        \"\"\"\n",
    "        n = len(sentences)\n",
    "        features = np.zeros((n, len(self.token2index)))\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == n - 1:\n",
    "                print_line('Encoding with TF-IDF encoder:', (i + 1), '/', n)\n",
    "            # Start your code (calculate TF-IDF)\n",
    "            clean_sentence = self.processor.apply(sentence)\n",
    "            tokens = tokenize(clean_sentence)\n",
    "            \n",
    "            tokens_counter = Counter(tokens)\n",
    "            \n",
    "            for token, counter in tokens_counter.items():\n",
    "                if token in self.vocab:\n",
    "                    features[i][self.token2index[token]] = counter/min(len(tokens),len(self.vocab))\n",
    "\n",
    "        idf_values = np.zeros(len(self.vocab))\n",
    "\n",
    "        for i,j in self.df.items():\n",
    "            if i in self.vocab:\n",
    "                idf_values[self.token2index[i]] = np.log((self.num_doc + 1)/(j + 1))+1\n",
    "                \n",
    "        features = features*idf_values[None:,]\n",
    "            # End\n",
    "            \n",
    "        print_line('\\n')\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6f250",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "4fb72620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 100 / 100\n",
      "The number of distinct tokens: 592\n",
      "Encoding with TF-IDF encoder: 10 / 10\n",
      "[[0.49495773 0.23462855 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.19116805 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.76254279 0.         ... 0.         0.         0.        ]\n",
      " [0.17873474 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "encoder = TfIdfEncoder()\n",
    "encoder.fit(train_sentences[:100])\n",
    "features = encoder.encode(train_sentences[:10])\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "ea4cefe0-8e12-4642-b2af-f9755d46f427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape\n",
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c6390",
   "metadata": {},
   "source": [
    "#### Encode training, validation, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "1d403214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 4014 / 4014\n",
      "The number of distinct tokens: 6668\n",
      "Encoding with TF-IDF encoder: 4014 / 4014\n",
      "Encoding with TF-IDF encoder: 446 / 446\n",
      "Encoding with TF-IDF encoder: 1114 / 1114\n",
      "The size of training set: (4014, 6668) (4014, 1)\n",
      "The size of validation set: (446, 6668) (446, 1)\n",
      "The size of test set: (1114, 6668) (1114, 1)\n"
     ]
    }
   ],
   "source": [
    "num_class = 2\n",
    "\n",
    "encoder = TfIdfEncoder()\n",
    "vocab_size = encoder.fit(train_sentences)\n",
    "\n",
    "x_train = encoder.encode(train_sentences)\n",
    "x_valid = encoder.encode(valid_sentences)\n",
    "x_test = encoder.encode(test_sentences)\n",
    "\n",
    "y_train = train_labels.reshape(-1, 1)\n",
    "y_valid = valid_labels.reshape(-1, 1)\n",
    "y_test = test_labels.reshape(-1, 1)\n",
    "\n",
    "print('The size of training set:', x_train.shape, y_train.shape)\n",
    "print('The size of validation set:', x_valid.shape, y_valid.shape)\n",
    "print('The size of test set:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add940bf",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression (50 points)\n",
    "In this section, you are required to implement a Logistic Regression (LR) model with $L_2$ regularization from scratch. \n",
    "\n",
    "The objective function of LR for binary classification:\n",
    "\n",
    "$$J = \\sum_{i=1}^n L(\\mathbf{x}_i, \\mathbf{y}_i \\mid \\mathbf{w}, \\mathbf{b}) = \\underbrace{-\\frac{1}{n}\\sum_{i=1}^n \\left(\\mathbf{y} \\log\\hat{\\mathbf{y}} + \\left(1 - \\mathbf{y}\\right)\\log\\left(1 - \\hat{\\mathbf{y}}\\right)\\right)}_{\\mbox{binary cross entropy loss}} + \\underbrace{\\lambda \\sum_{j=1}^{d}\\mathbf{w}_{j}^2}_{\\mbox{regularization}}$$\n",
    "\n",
    "- $\\hat{\\mathbf{y}} = \\sigma(\\mathbf{x}\\mathbf{w} + \\mathbf{b})$\n",
    "- $n$: Number of samples\n",
    "- $d$: Dimension of $\\mathbf{w}$\n",
    "\n",
    "**Deliverable 1 (10 Points)**: Given the objective function, please show the steps to derive the graident of J with respect to $\\mathbf{w}$ and $\\mathbf{b}$. You can either list the steps in the notebook or submit a pdf with all the steps in the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db783ac-a6e7-475f-9d98-012bf98d0d40",
   "metadata": {},
   "source": [
    "**Markdown cell (double click to confirm)**\n",
    "\n",
    "$$\n",
    "\\mathbf{h}=\\mathbf{w}^T \\mathbf{X}\n",
    "$$\n",
    "Logistic regression: $\\mathbf{z}=\\sigma(\\mathbf{h})=\\frac{1}{1+e^{-\\mathbf{h}}}$\\\n",
    "Cross-entropy loss: $J(\\mathbf{w})=-(\\mathbf{y} \\log (\\mathbf{z})+(1-\\mathbf{y}) \\log (1-\\mathbf{z}))$\\\n",
    "Use chain rule: $\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}}=\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{z}} \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{h}} \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{w}}$\\\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{z}}=-\\left(\\frac{\\mathbf{y}}{\\mathbf{z}}-\\frac{1-\\mathbf{y}}{1-\\mathbf{z}}\\right)=\\frac{\\mathbf{z}-\\mathbf{y}}{\\mathbf{z}(1-\\mathbf{z})}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{h}}=\\mathbf{z}(1-\\mathbf{z})\n",
    "$$\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{w}}=\\mathbf{X} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}}=\\mathbf{X}^T(\\mathbf{z}-\\mathbf{y})\n",
    "\\end{gathered}\n",
    "$$\n",
    "Gradient descent: $\\mathbf{w}=\\mathbf{w}-\\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}}$\n",
    "\n",
    "Gradient descent: $\\mathbf{b}=1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db567d6",
   "metadata": {},
   "source": [
    "### 2.1 Logistic Regression Model (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "4871cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" The sigmoid activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "    Returns:\n",
    "        output: Sigmoid value of each entry in x\n",
    "    \"\"\"\n",
    "    # Start your code here\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "    # x=np.exp(-np.logaddexp(0, -x))\n",
    "    # End\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "id": "d5a0fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, feature_dim: int, lambda_: float):\n",
    "        \"\"\" Logistic Regression Model\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        # Start your code here (initialize weight and bias)\n",
    "        self.w = np.random.rand(feature_dim)\n",
    "        self.b = np.random.rand(1)\n",
    "        # End \n",
    "        \n",
    "        self.lambda_ = lambda_\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Forward process of logistic regression\n",
    "            Calculate y_hat using x\n",
    "        Args:\n",
    "            x: Input data\n",
    "        Returns:\n",
    "            y_hat: Output\n",
    "        \"\"\"\n",
    "        y_hat = 0\n",
    "        \n",
    "        # Start your code here (calculate y_hat of logistic regression using x)\n",
    "        mat = np.matmul(x, self.w.T) + self.b # (446, 6668) * (6668 * 1) = (446*1)\n",
    "        y_hat = sigmoid(mat)\n",
    "        # End\n",
    "        return y_hat\n",
    "\n",
    "    def backward(self,\n",
    "                 x: np.ndarray,\n",
    "                 y_hat: np.ndarray,\n",
    "                 y: np.ndarray) -> Tuple[np.ndarray, Union[float, np.ndarray]]:\n",
    "        \"\"\" Backward process of logistic regression\n",
    "            Calculate the gradient of w and b\n",
    "        Args:\n",
    "            x: Input data\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "        Returns:\n",
    "            w_grad: Gradient of w\n",
    "            b_grad: Gradient of b\n",
    "        \"\"\"\n",
    "        w_grad, b_grad = 0.0, 0.0\n",
    "        n = len(x)\n",
    "        \n",
    "        # Start your code here (calculate the gradient of w and b)\n",
    "\n",
    "        w_grad = np.matmul(x.T, (y_hat-y)) # 4,6668\n",
    "        b_grad = 1\n",
    "\n",
    "        # End\n",
    "        \n",
    "        return w_grad, b_grad\n",
    "\n",
    "    def binary_cross_entropy_loss(self,\n",
    "                                  y_hat: np.ndarray,\n",
    "                                  y: np.ndarray) -> Union[float, np.ndarray]:\n",
    "        \"\"\" Calculate the binary cross-entropy loss\n",
    "        Args:\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "        Returns:\n",
    "            loss: BCE loss\n",
    "        \"\"\"\n",
    "        y_hat = np.clip(y_hat, a_min=self.eps, a_max=1 - self.eps)\n",
    "        loss = 0\n",
    "        \n",
    "        # Start your code here (Calculate the binary cross-entropy)\n",
    "        # print(y_hat, y) # y -> 4 * 1 yh -> 4*1\n",
    "        loss = (-1/n) * ( np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) + self.lambda_*np.sum(self.w**2) )\n",
    "        # End\n",
    "                       \n",
    "        return loss\n",
    "\n",
    "    def gradient_descent(self, w_grad: np.ndarray, b_grad: Union[np.ndarray, float], lr: float):\n",
    "        \n",
    "        # print(lr, w_grad.shape)\n",
    "        # print((lr*w_grad).shape)\n",
    "        # print(self.w.shape) # 6668.1\n",
    "        self.w -= lr * w_grad\n",
    "        \n",
    "        self.b -= lr * b_grad\n",
    "\n",
    "    def predict(self, y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict the label using the output y_hat\n",
    "        Args:\n",
    "            y_hat: Model output\n",
    "        Returns:\n",
    "            pred: Prediction\n",
    "        \"\"\"\n",
    "        pred = np.round(y_hat)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cabf69",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Evaluation Metrics (5 Points)\n",
    "\n",
    "Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "1cb286f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "\n",
    "def get_metrics(y_pred: np.ndarray, y_true: np.ndarray) -> Tuple[float, float, float, float]:\n",
    "    \"\"\" Calculate the accuracy, precision, recall, and f1 score.\n",
    "        You are only allowed to use numpy here.\n",
    "    Args:\n",
    "        y_pred: Prediction\n",
    "        y_true: Ground-truth\n",
    "    Returns:\n",
    "        accuracy, precision, recall, f1\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y_true.shape\n",
    "    accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "    # Start your code here\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    # End\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6704d08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 Stochastic Gradient Descent (SGD) (Fill the code, 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "16d56bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def train_sgd(model: 'LogisticRegression',\n",
    "              x_train: np.ndarray,\n",
    "              y_train: np.ndarray,\n",
    "              x_valid: np.ndarray,\n",
    "              y_valid: np.ndarray,\n",
    "              lr: float,\n",
    "              num_epoch: int,\n",
    "              print_every: int = 10) -> Tuple[dict[str, List], dict[str, List]]:\n",
    "    \"\"\" Training with Stochastic Gradient Descent\n",
    "    Args:\n",
    "        model: The logistic regression model\n",
    "        x_train: Training feature, (n x d) matrix\n",
    "        y_train: Training label, (n, ) vector\n",
    "        x_valid: Validation feature, (n x d) matrix\n",
    "        y_valid: Validation label, (n, ) vector\n",
    "        lr: Learning rate\n",
    "        num_epoch: Number of training epochs\n",
    "        print_every: Print log every {print_every} epochs\n",
    "    Returns:\n",
    "        train_history: Log of training information. The format of training history is\n",
    "                       { 'loss': [] }\n",
    "                       It records the average loss of each epoch.\n",
    "        valid_history: Log of validation information. The format of validation history is\n",
    "                       {\n",
    "                           'loss': [],\n",
    "                           'accuracy': [],\n",
    "                           'precision': [],\n",
    "                           'recall': [],\n",
    "                           'f1': []\n",
    "                       }\n",
    "    \"\"\"\n",
    "    train_history = OrderedDict({'loss': []})\n",
    "    valid_history = OrderedDict({\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    })\n",
    "\n",
    "    def format_output(epoch, num_epoch, train_history, valid_history):\n",
    "        epoch_log = f'Epoch {epoch + 1} / {num_epoch}'\n",
    "        train_log = ' - '.join([f'train_{key}: {val[-1]:.4f}' for key, val in train_history.items()])\n",
    "        valid_log = ' - '.join([f'valid_{key}: {val[-1]:.4f}' for key, val in valid_history.items()])\n",
    "        log = f'{epoch_log}: {train_log} - {valid_log}'\n",
    "        return log\n",
    "\n",
    "    train_num_samples = len(x_train)\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Start your code here (training)\n",
    "        #     Step 1. Model forward\n",
    "        #     Step 2. Calculate loss\n",
    "        #     Step 3. Model backward\n",
    "        #     Step 4. Optimization with gradient descent\n",
    "        \n",
    "        for i in range(train_num_samples):\n",
    "            y_hat = model.forward(x_train[i].reshape(1, -1))\n",
    "            epoch_loss += model.binary_cross_entropy_loss(y_hat, y_train[i])\n",
    "            w_grad, b_grad = model.backward(x_train[i].reshape(1,-1), y_hat, y_train[i])\n",
    "            model.gradient_descent(w_grad, b_grad, lr)\n",
    "        # End\n",
    "\n",
    "        valid_loss = 0.\n",
    "        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        # Start your code here (validation)\n",
    "        #     Step 1. Predict\n",
    "        #     Step 2. Calculate loss\n",
    "        #     Step 3. Calculate metrics\n",
    "        \n",
    "        y_hat_valid = model.forward(x_valid) #\n",
    "        y_hat_val = np.argmax(y_hat_valid.reshape(1,-1), axis=0)\n",
    "        accuracy, precision, recall, f1 = get_metrics(y_hat_val.reshape(1,-1).T, y_valid)\n",
    "        valid_loss += model.binary_cross_entropy_loss(y_hat_valid, y_valid)\n",
    "        valid_loss = valid_loss/len(y_valid)\n",
    "        # End\n",
    "\n",
    "        train_history['loss'].append(epoch_loss / train_num_samples)\n",
    "        for vals, val in zip(valid_history.values(), [valid_loss, accuracy, precision, recall, f1]):\n",
    "            vals.append(val)\n",
    "        log = format_output(epoch, num_epoch, train_history, valid_history)\n",
    "        if epoch % print_every == 0 or epoch == num_epoch - 1:\n",
    "            print(log)\n",
    "        else:\n",
    "            print_line(log)\n",
    "\n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24319d",
   "metadata": {},
   "source": [
    "Run SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "085f1ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100: train_loss: 0.0005 - valid_loss: 0.0969 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 11 / 100: train_loss: 0.0003 - valid_loss: 0.1228 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 21 / 100: train_loss: 0.0003 - valid_loss: 0.1381 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 31 / 100: train_loss: 0.0004 - valid_loss: 0.1482 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 41 / 100: train_loss: 0.0004 - valid_loss: 0.1558 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 51 / 100: train_loss: 0.0004 - valid_loss: 0.1621 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 61 / 100: train_loss: 0.0005 - valid_loss: 0.1674 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 71 / 100: train_loss: 0.0005 - valid_loss: 0.1720 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 81 / 100: train_loss: 0.0005 - valid_loss: 0.1760 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 91 / 100: train_loss: 0.0005 - valid_loss: 0.1796 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 100 / 100: train_loss: 0.0005 - valid_loss: 0.1825 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "num_epoch = 100\n",
    "lr = 1e-1\n",
    "lambda_ = 1e-7\n",
    "print_every = 10\n",
    "\n",
    "model_sgd = LogisticRegression(feature_dim=vocab_size, lambda_=lambda_)\n",
    "sgd_train_history, sgd_valid_history = train_sgd(model_sgd, x_train, y_train, x_valid, y_valid, lr, num_epoch, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f510c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4 Mini-batch Gradient Descent (Fill the code: 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "id": "b89aa7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mbgd(model: 'LogisticRegression',\n",
    "               x_train: np.ndarray,\n",
    "               y_train: np.ndarray,\n",
    "               x_valid: np.ndarray,\n",
    "               y_valid: np.ndarray,\n",
    "               lr: float,\n",
    "               num_epoch: int,\n",
    "               batch_size: int,\n",
    "               print_every: int = 10) -> Tuple[dict[str, List], dict[str, List]]:\n",
    "    \"\"\" Training with Stochastic Gradient Descent\n",
    "    Args:\n",
    "        model: The logistic regression model\n",
    "        x_train: Training feature, (n x d) matrix\n",
    "        y_train: Training label, (n, ) vector\n",
    "        x_valid: Validation feature, (n x d) matrix\n",
    "        y_valid: Validation label, (n, ) vector\n",
    "        lr: Learning rate\n",
    "        num_epoch: Number of training epochs\n",
    "        batch_size: Number of training samples in a batch\n",
    "        print_every: Print log every {print_every} epochs\n",
    "    Returns:\n",
    "        train_history: Log of training information. The format of training history is\n",
    "                       { 'loss': [] }\n",
    "                       It records the average loss of each epoch.\n",
    "        valid_history: Log of validation information. The format of training and validation history is\n",
    "                       {\n",
    "                           'loss': [],\n",
    "                           'accuracy': [],\n",
    "                           'precision': [],\n",
    "                           'recall': [],\n",
    "                           'f1': []\n",
    "                       }\n",
    "    \"\"\"\n",
    "    train_history = OrderedDict({'loss': []})\n",
    "    valid_history = OrderedDict({\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    })\n",
    "\n",
    "    def format_output(epoch, num_epoch, train_history, valid_history):\n",
    "        epoch_log = f'Epoch {epoch + 1} / {num_epoch}'\n",
    "        train_log = ' - '.join([f'train_{key}: {val[-1]:.4f}' for key, val in train_history.items()])\n",
    "        valid_log = ' - '.join([f'valid_{key}: {val[-1]:.4f}' for key, val in valid_history.items()])\n",
    "        log = f'{epoch_log}: {train_log} - {valid_log}'\n",
    "        return log\n",
    "\n",
    "    train_num_samples = len(x_train)\n",
    "    n_batch = train_num_samples // batch_size\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        # Start your code here (training)\n",
    "        #     Step 1. Model forward\n",
    "        #     Step 2. Calculate loss\n",
    "        #     Step 3. Model backward\n",
    "        #     Step 4. Optimization with gradient descent\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            y_hat = None\n",
    "            kx_train = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            ky_train = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            y_hat = model.forward(kx_train)\n",
    "            epoch_loss += model.binary_cross_entropy_loss(y_hat, ky_train)\n",
    "            w_grad, b_grad = model.backward(kx_train, y_hat.reshape(1,-1).T, ky_train)\n",
    "            model.gradient_descent(w_grad.flatten(), b_grad, lr)\n",
    "        \n",
    "        # End\n",
    "        # print(epoch_loss)\n",
    "        valid_loss = 0.\n",
    "        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        # Start your code here (validation)\n",
    "        #     Step 1. Predict\n",
    "        #     Step 2. Calculate loss\n",
    "        #     Step 3. Calculate metrics\n",
    "        y_hat_valid = model.forward(x_valid) #\n",
    "        y_hat_val = np.argmax(y_hat_valid.reshape(1,-1), axis=0)\n",
    "        accuracy, precision, recall, f1 = get_metrics(y_hat_val.reshape(1,-1).T, y_valid)\n",
    "        valid_loss += model.binary_cross_entropy_loss(y_hat_valid, y_valid)\n",
    "        valid_loss = valid_loss/len(y_valid)\n",
    "        # End\n",
    "\n",
    "        train_history['loss'].append(epoch_loss / train_num_samples)\n",
    "        for vals, val in zip(valid_history.values(), [valid_loss, accuracy, precision, recall, f1]):\n",
    "            vals.append(val)\n",
    "        log = format_output(epoch, num_epoch, train_history, valid_history)\n",
    "        if epoch % print_every == 0 or epoch == num_epoch - 1:\n",
    "            print(log)\n",
    "        else:\n",
    "            print_line(log)\n",
    "\n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74314ba7",
   "metadata": {},
   "source": [
    "Run Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "id": "368748a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100: train_loss: 0.0025 - valid_loss: 0.3149 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 11 / 100: train_loss: 0.0028 - valid_loss: 0.3149 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 21 / 100: train_loss: 0.0028 - valid_loss: 0.3149 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 31 / 100: train_loss: 0.0028 - valid_loss: 0.3149 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 41 / 100: train_loss: 0.0028 - valid_loss: 0.3149 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 51 / 100: train_loss: 0.0028 - valid_loss: 0.3149 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 61 / 100: train_loss: 0.0028 - valid_loss: 0.3149 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 71 / 100: train_loss: 0.0028 - valid_loss: 0.3149 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 81 / 100: train_loss: 0.0028 - valid_loss: 0.3149 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 91 / 100: train_loss: 0.0028 - valid_loss: 0.3149 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
      "Epoch 100 / 100: train_loss: 0.0028 - valid_loss: 0.3149 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "num_epoch = 100\n",
    "lr = 1e-1\n",
    "batch_size = 4\n",
    "lambda_ = 1e-7\n",
    "print_every = 10\n",
    "\n",
    "model_mbgd = LogisticRegression(feature_dim=vocab_size, lambda_=lambda_)\n",
    "mbgd_train_history, mbgd_valid_history = train_mbgd(model_mbgd, x_train, y_train, x_valid, y_valid, lr, num_epoch, batch_size, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a3006",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.5 Evaluation (5 Points)\n",
    "You are required to report the loss, accuracy, precision, recall, and f1 on test set and plot the the curve of them for both SGD and Mini-batch GD on train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "id": "c78da609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAez0lEQVR4nO3dfZSVZb3/8fenAUF5SAVMY1SgUMKAATdooghmBeoRI/sJxyMi/jRMM7VMTk9SHtfqrDge4ydmZD517EytCiNDLUhDs5JBCUWhEDEnfEBMwQAB+/7+2DfTdrgG9gxzM8zM57XWXrPv676u+74uZtiffT8rIjAzM6vvXS3dATMz2zc5IMzMLMkBYWZmSQ4IMzNLckCYmVmSA8LMzJIcENYuSLpF0leao66kNZJObb7eNbieGZL+J+/1mDXEAWGtWvZhvVVSz3rlSyWFpD4AETEtIq4rZ5mNqdtYkh6S9H9zWrYkXSZpmaRNkl7K1jex3vq3SNooaYOkJZKmS+qUR5+sdXNAWFvwHDBpx4SkQcD+LdedFjMLuAL4HNAD6A18GRhbr95lEdENOCyrOxGYL0l7r6vWGjggrC34PjC5ZPp84K7SCpLukPQf2fvRkmolfU7SK5JelHRBqu4uDJf0tKS/SbpdUues7UGS7pW0Lpt3r6TKbN71wEnATZLelHRTVn6MpF9Jek3Sy5K+WLKe/STdlX3jXy6pkOqMpKOATwMTI+JXEbE5It6OiEciYkqqTUT8PSIeAs4EPgScvpsxWzvjgLC24PdAd0kfkFQBnAPsbt/9ocC7KX7LvhCYLemgRqzzXOBjwPuAoyh+U4fi/6nbgSOBI4DNwE0AEfEl4GGK3+C7RsRlkroBC4D7gfcC7wcWlqznTKAaOBCYt2NZCacAL0RETSPGQNavvwA1FMPLrI4DwtqKHVsRHwFWAH/dTf1twNcjYltEzAfeBI5uxPpuiogXIuI14HqyXVwRsT4ifhIRmyJiYzbv5F0s5wzgpYj4r4jYEhEbI+IPJfMfiYj5EfF2NsYhDSynJ/BSaUG2lfR6dszhyN2MZy1w8G7qWDvjgLC24vvAvwJTqLd7qQHrI2J7yfQmoGv9SpLuy3YHvSnp3JJZL5S8f57it38kHSDpO5Kel7QBWAQcmG3ZpBwOPLuLfpZ+6G8COkvqkBoPxWMKdSKikmJwdAJ2d3yhN/DabupYO+OAsDYhIp6neLD6NOCnzbjccdnuoK4RcXfJrMNL3h9B8Rs4FA/6Hg0cFxHdgVFZ+Y4P6Pq3T36B4m6qPfVroLKhYxS7Iulw4FiKu7/M6jggrC25EDglIv6+F9Z1qaRKSQcDXwR+mJV3o3jc4fVs3rX12r0M9CuZvhc4VNIVkjpJ6ibpuMZ2JiJWAt8BqiV9RNL+2VbLCQ21ybZ2TgZ+BjwGzG/seq1tc0BYmxERzzblIG0T/QD4JbA6e+046+lGiqfYvkrx4Pn99dp9Czg7O8NpVnac4iPAv1DcnfRnYEwT+3QpxVNdb6C4u6gWuI7iQfu/lNS7SdJGimF1I/ATYGxE/KOJ67U2Sn5gkJmZpXgLwszMkhwQZmaW5IAwM7MkB4SZmSWlLrhptXr27Bl9+vRp6W6YmbUaS5YseTUieqXmtamA6NOnDzU1e+ssRzOz1k/S8w3N8y4mMzNLckCYmVmSA8LMzJIcEGZmluSAMDOzJAeEmZklOSDMzCypTV0H0WT3TYeXnmzpXpiZNc2hg2DcN5p9sd6CMDOzJG9BQC7Ja2bW2nkLwszMkhwQZmaW5IAwM7MkB4SZmSU5IMzMLCnXgJA0VtJKSaskTU/MHy9pmaSlkmoknVhuWzMzy1duASGpApgNjAMGApMkDaxXbSEwJCKqgKnArY1oa2ZmOcpzC2IEsCoiVkfEVqAaGF9aISLejIjIJrsAUW5bMzPLV54B0Rt4oWS6Nit7B0kfl7QC+AXFrYiy22btL852T9WsW7euWTpuZmb5BoQSZbFTQcTciBgAnAVc15i2Wfs5EVGIiEKvXsnnbpuZWRPkGRC1wOEl05XA2oYqR8Qi4H2Seja2rZmZNb88A2Ix0F9SX0n7AROBeaUVJL1fkrL3w4D9gPXltDUzs3zldrO+iNgu6TLgAaACuC0ilkuals2/BfgEMFnSNmAzcE520DrZNq++mpnZzvTPk4hav0KhEDU1NS3dDTOzVkPSkogopOb5SmozM0tyQJiZWZIDwszMkhwQZmaW5IAwM7MkB4SZmSU5IMzMLMkBYWZmSQ4IMzNLckCYmVmSA8LMzJIcEGZmluSAMDOzJAeEmZklOSDMzCzJAWFmZkkOCDMzS3JAmJlZkgPCzMySHBBmZpbkgDAzsyQHhJmZJeUaEJLGSlopaZWk6Yn550palr0elTSkZN4aSU9KWiqpJs9+mpnZzjrktWBJFcBs4CNALbBY0ryIeLqk2nPAyRHxN0njgDnAcSXzx0TEq3n10czMGpbnFsQIYFVErI6IrUA1ML60QkQ8GhF/yyZ/D1Tm2B8zM2uEPAOiN/BCyXRtVtaQC4H7SqYD+KWkJZIubqiRpIsl1UiqWbdu3R512MzM/im3XUyAEmWRrCiNoRgQJ5YUj4yItZIOAX4laUVELNppgRFzKO6aolAoJJdvZmaNl+cWRC1weMl0JbC2fiVJg4FbgfERsX5HeUSszX6+AsyluMvKzMz2kjwDYjHQX1JfSfsBE4F5pRUkHQH8FDgvIv5UUt5FUrcd74GPAk/l2FczM6snt11MEbFd0mXAA0AFcFtELJc0LZt/C/BVoAdwsySA7RFRAN4DzM3KOgA/iIj78+qrmZntTBFtZ7d9oVCImhpfMmFmVi5JS7Iv5jvxldRmZpbkgDAzsyQHhJmZJTkgzMwsyQFhZmZJDggzM0tyQJiZWZIDwszMkhwQZmaW5IAwM7MkB4SZmSU5IMzMLMkBYWZmSQ4IMzNLckCYmVmSA8LMzJIcEGZmluSAMDOzJAeEmZklOSDMzCzJAWFmZkkOCDMzS3JAmJlZUq4BIWmspJWSVkmanph/rqRl2etRSUPKbWtmZvnKLSAkVQCzgXHAQGCSpIH1qj0HnBwRg4HrgDmNaGtmZjnKcwtiBLAqIlZHxFagGhhfWiEiHo2Iv2WTvwcqy21rZmb5yjMgegMvlEzXZmUNuRC4r7FtJV0sqUZSzbp16/agu2ZmVirPgFCiLJIVpTEUA+KaxraNiDkRUYiIQq9evZrUUTMz21mHHJddCxxeMl0JrK1fSdJg4FZgXESsb0xbMzPLT55bEIuB/pL6StoPmAjMK60g6Qjgp8B5EfGnxrQ1M7N85bYFERHbJV0GPABUALdFxHJJ07L5twBfBXoAN0sC2J7tLkq2zauvZma2M0Ukd+23SoVCIWpqalq6G2ZmrYakJRFRSM3zldRmZpbkgDAzsyQHhJmZJTkgzMwsKc/rIMysjdu2bRu1tbVs2bKlpbtiu9G5c2cqKyvp2LFj2W0cEGbWZLW1tXTr1o0+ffqQnapu+6CIYP369dTW1tK3b9+y23kXk5k12ZYtW+jRo4fDYR8niR49ejR6S88BYWZ7xOHQOjTl9+SAMLNWa/369VRVVVFVVcWhhx5K796966a3bt26y7Y1NTVcfvnlu13HCSec0Cx9feihhzjjjDOaZVl7i49BmFmr1aNHD5YuXQrAjBkz6Nq1K5///Ofr5m/fvp0OHdIfc4VCgUIheQHxOzz66KPN0tfWyFsQZtamTJkyhauuuooxY8ZwzTXX8Nhjj3HCCScwdOhQTjjhBFauXAm88xv9jBkzmDp1KqNHj6Zfv37MmjWrbnldu3atqz969GjOPvtsBgwYwLnnnsuOWxXNnz+fAQMGcOKJJ3L55Zfvdkvhtdde46yzzmLw4MEcf/zxLFu2DIDf/OY3dVtAQ4cOZePGjbz44ouMGjWKqqoqPvjBD/Lwww83+79ZQ7wFYWbN4ms/X87Tazc06zIHvrc71/7LMY1u96c//YkFCxZQUVHBhg0bWLRoER06dGDBggV88Ytf5Cc/+clObVasWMGDDz7Ixo0bOfroo7nkkkt2OiX0iSeeYPny5bz3ve9l5MiR/Pa3v6VQKPCpT32KRYsW0bdvXyZNmrTb/l177bUMHTqUe+65h1//+tdMnjyZpUuXMnPmTGbPns3IkSN588036dy5M3PmzOFjH/sYX/rSl3j77bfZtGlTo/89mqqsgJDUBdgcEf+QdBQwALgvIrbl2jszsyb45Cc/SUVFBQBvvPEG559/Pn/+85+RxLZt6Y+t008/nU6dOtGpUycOOeQQXn75ZSorK99RZ8SIEXVlVVVVrFmzhq5du9KvX7+600cnTZrEnDlzdtm/Rx55pC6kTjnlFNavX88bb7zByJEjueqqqzj33HOZMGEClZWVDB8+nKlTp7Jt2zbOOussqqqq9uSfplHK3YJYBJwk6SBgIVADnAOcm1fHzKx1aco3/bx06dKl7v1XvvIVxowZw9y5c1mzZg2jR49OtunUqVPd+4qKCrZv315WnabcETvVRhLTp0/n9NNPZ/78+Rx//PEsWLCAUaNGsWjRIn7xi19w3nnncfXVVzN58uRGr7Mpyj0GoYjYBEwA/l9EfBwYmF+3zMyaxxtvvEHv3sVH2t9xxx3NvvwBAwawevVq1qxZA8APf/jD3bYZNWoUd999N1A8ttGzZ0+6d+/Os88+y6BBg7jmmmsoFAqsWLGC559/nkMOOYSLLrqICy+8kMcff7zZx9CQcrcgJOlDFLcYLmxkWzOzFvOFL3yB888/nxtuuIFTTjml2Ze///77c/PNNzN27Fh69uzJiBEjdttmxowZXHDBBQwePJgDDjiAO++8E4Abb7yRBx98kIqKCgYOHMi4ceOorq7mm9/8Jh07dqRr167cddddzT6GhpT1wCBJJwOfA34bEf8pqR9wRUTs/iTivcgPDDLbu5555hk+8IEPtHQ3Wtybb75J165diQguvfRS+vfvz5VXXtnS3dpJ6ve1qwcGlbUVEBG/AX6TLexdwKv7WjiYmbWU7373u9x5551s3bqVoUOH8qlPfaqlu9Qsyj2L6QfANOBtYAnwbkk3RMQ38+ycmVlrcOWVV+6TWwx7qtyD1AMjYgNwFjAfOAI4L69OmZlZyys3IDpK6kgxIH6WXf/Q+HO7zMys1Sg3IL4DrAG6AIskHQk07yWTZma2TykrICJiVkT0jojTouh5YMzu2kkaK2mlpFWSpifmD5D0O0lvSfp8vXlrJD0paakkn5pkZraXlRUQkt4t6QZJNdnrvyhuTeyqTQUwGxhH8aK6SZLqX1z3GnA5MLOBxYyJiKqGTsEys/Zt9OjRPPDAA+8ou/HGG/n0pz+9yzY7Toc/7bTTeP3113eqM2PGDGbObOhjqeiee+7h6aefrpv+6le/yoIFCxrR+7R96bbg5e5iug3YCPyf7LUBuH03bUYAqyJidURsBaqB8aUVIuKViFgM+J5OZtZokyZNorq6+h1l1dXVZd0wD4p3YT3wwAObtO76AfH1r3+dU089tUnL2leVGxDvi4hrsw/71RHxNaDfbtr0Bl4oma7NysoVwC8lLZF0cSPamVk7cfbZZ3Pvvffy1ltvAbBmzRrWrl3LiSeeyCWXXEKhUOCYY47h2muvTbbv06cPr776KgDXX389Rx99NKeeemrdLcGheI3D8OHDGTJkCJ/4xCfYtGkTjz76KPPmzePqq6+mqqqKZ599lilTpvDjH/8YgIULFzJ06FAGDRrE1KlT6/rXp08frr32WoYNG8agQYNYsWLFLsfX0rcFL/d2GZslnRgRjwBIGgls3k2b1PPtGnPm08iIWCvpEOBXklZExKKdVlIMj4sBjjjiiEYs3sya1X3T4aUnm3eZhw6Ccd9ocHaPHj0YMWIE999/P+PHj6e6uppzzjkHSVx//fUcfPDBvP3223z4wx9m2bJlDB48OLmcJUuWUF1dzRNPPMH27dsZNmwYxx57LAATJkzgoosuAuDLX/4y3/ve9/jMZz7DmWeeyRlnnMHZZ5/9jmVt2bKFKVOmsHDhQo466igmT57Mt7/9ba644goAevbsyeOPP87NN9/MzJkzufXWWxscX0vfFrzcLYhpwOzswPEa4CZgd5cK1gKHl0xXAmvL7VhErM1+vgLMpbjLKlVvTkQUIqLQq1evchdvZm1E6W6m0t1LP/rRjxg2bBhDhw5l+fLl79gdVN/DDz/Mxz/+cQ444AC6d+/OmWeeWTfvqaee4qSTTmLQoEHcfffdLF++fJf9WblyJX379uWoo44C4Pzzz2fRon9+t50wYQIAxx57bN0N/hryyCOPcN55xUvOUrcFnzVrFq+//jodOnRg+PDh3H777cyYMYMnn3ySbt267XLZ5Sj3Vht/BIZI6p5Nb5B0BbBsF80WA/0l9QX+CkwE/rWc9WXPn3hXRGzM3n8U+Ho5bc2shezim36ezjrrLK666ioef/xxNm/ezLBhw3juueeYOXMmixcv5qCDDmLKlCls2bJll8uRUjs9ik+ou+eeexgyZAh33HEHDz300C6Xs7v72+24ZXhDtxTf3bL25m3BG/XI0YjYkF1RDXDVbupuBy4DHgCeAX4UEcslTZM0DUDSoZJqs2V9WVJtFkLvAR6R9EfgMeAXEXF/o0ZmZu1C165dGT16NFOnTq3betiwYQNdunTh3e9+Ny+//DL33XffLpcxatQo5s6dy+bNm9m4cSM///nP6+Zt3LiRww47jG3bttXdohugW7dubNy4cadlDRgwgDVr1rBq1SoAvv/973PyySc3aWwtfVvwPblldzpuS0TEfIq35igtu6Xk/UsUdz3VtwEYsgd9M7N2ZNKkSUyYMKFuV9OQIUMYOnQoxxxzDP369WPkyJG7bD9s2DDOOeccqqqqOPLIIznppJPq5l133XUcd9xxHHnkkQwaNKguFCZOnMhFF13ErFmz6g5OA3Tu3Jnbb7+dT37yk2zfvp3hw4czbdq0Jo2rpW8LXtbtvpMNpb9ExD51VNi3+zbbu3y779alWW/3LWkj6TOPBOzf1E6amdm+b5cBERF7fhjczMxapUYdpDYzs/bDAWFme6SpxzFt72rK78kBYWZN1rlzZ9avX++Q2MdFBOvXr6dz586Narcnp7maWTtXWVlJbW0t69ata+mu2G507tyZysrUVQUNc0CYWZN17NiRvn37tnQ3LCfexWRmZkkOCDMzS3JAmJlZkgPCzMySHBBmZpbkgDAzsyQHhJmZJTkgzMwsyQFhZmZJDggzM0tyQJiZWZIDwszMkhwQZmaW5IAwM7MkB4SZmSXlGhCSxkpaKWmVpOmJ+QMk/U7SW5I+35i2ZmaWr9wCQlIFMBsYBwwEJkkaWK/aa8DlwMwmtDUzsxzluQUxAlgVEasjYitQDYwvrRARr0TEYmBbY9uamVm+8gyI3sALJdO1WVmztpV0saQaSTV+Lq6ZWfPJMyCUKIvmbhsRcyKiEBGFXr16ld05MzPbtTwDohY4vGS6Eli7F9qamVkzyDMgFgP9JfWVtB8wEZi3F9qamVkz6JDXgiNiu6TLgAeACuC2iFguaVo2/xZJhwI1QHfgH5KuAAZGxIZU27z6amZmO1NEuYcF9n2FQiFqampauhtmZq2GpCURUUjN85XUZmaW5IAwM7MkB4SZmSU5IMzMLMkBYWZmSQ4IMzNLckCYmVmSA8LMzJIcEGZmluSAMDOzJAeEmZklOSDMzCzJAWFmZkkOCDMzS3JAmJlZkgPCzMySHBBmZpbkgDAzsyQHhJmZJTkgzMwsyQFhZmZJDggzM0tyQJiZWVKuASFprKSVklZJmp6YL0mzsvnLJA0rmbdG0pOSlkqqybOfZma2sw55LVhSBTAb+AhQCyyWNC8ini6pNg7on72OA76d/dxhTES8mlcfzcysYXluQYwAVkXE6ojYClQD4+vVGQ/cFUW/Bw6UdFiOfTIzszLlGRC9gRdKpmuzsnLrBPBLSUskXdzQSiRdLKlGUs26deuaodtmZgb5BoQSZdGIOiMjYhjF3VCXShqVWklEzImIQkQUevXq1fTempnZO+QZELXA4SXTlcDacutExI6frwBzKe6yMjOzvSTPgFgM9JfUV9J+wERgXr0684DJ2dlMxwNvRMSLkrpI6gYgqQvwUeCpHPtqZmb15HYWU0Rsl3QZ8ABQAdwWEcslTcvm3wLMB04DVgGbgAuy5u8B5kra0ccfRMT9efXVzMx2poj6hwVar0KhEDU1vmTCzKxckpZERCE1z1dSm5lZkgPCzMySHBBmZpbkgDAzsyQHhJmZJTkgzMwsyQFhZmZJDggzM0tyQJiZWZIDwszMkhwQZmaW5IAwM7MkB4SZmSU5IMzMLMkBYWZmSQ4IMzNLckCYmVmSA8LMzJIcEGZmluSAMDOzJAeEmZklOSDMzCypQ54LlzQW+BZQAdwaEd+oN1/Z/NOATcCUiHi8nLZ5iQj+EXtjTW2LWroD1mRq5788tfd/gF3ILSAkVQCzgY8AtcBiSfMi4umSauOA/tnrOODbwHFltm02p896mNc3bWPDlm28+dZ2wgFhZq1Iz66dqPnyqc2+3Dy3IEYAqyJiNYCkamA8UPohPx64KyIC+L2kAyUdBvQpo22zOeo93XiXRLfOHejWuQMdK7znrTEcqK1X0L5/eW3lb7dLp4pclptnQPQGXiiZrqW4lbC7Or3LbNts/vucqrwWbWbWauX5VTm1Y69+XjdUp5y2xQVIF0uqkVSzbt26RnbRzMwakmdA1AKHl0xXAmvLrFNOWwAiYk5EFCKi0KtXrz3utJmZFeUZEIuB/pL6StoPmAjMq1dnHjBZRccDb0TEi2W2NTOzHOV2DCIitku6DHiA4qmqt0XEcknTsvm3APMpnuK6iuJprhfsqm1efTUzs50p2sphfKBQKERNTU1Ld8PMrNWQtCQiCql5Pp/TzMySHBBmZpbkgDAzs6Q2dQxC0jrg+SY27wm82ozdaQ3a45ihfY67PY4Z2ue4GzvmIyMieY1AmwqIPSGppqEDNW1VexwztM9xt8cxQ/scd3OO2buYzMwsyQFhZmZJDoh/mtPSHWgB7XHM0D7H3R7HDO1z3M02Zh+DMDOzJG9BmJlZkgPCzMyS2n1ASBoraaWkVZKmt3R/8iLpcEkPSnpG0nJJn83KD5b0K0l/zn4e1NJ9bW6SKiQ9IenebLo9jPlAST+WtCL7nX+orY9b0pXZ3/ZTkv5XUue2OGZJt0l6RdJTJWUNjlPSv2efbyslfawx62rXAVHy7OtxwEBgkqSBLdur3GwHPhcRHwCOBy7NxjodWBgR/YGF2XRb81ngmZLp9jDmbwH3R8QAYAjF8bfZcUvqDVwOFCLigxTvAj2RtjnmO4Cx9cqS48z+j08Ejsna3Jx97pWlXQcEJc/NjoitwI5nX7c5EfFiRDyevd9I8QOjN8Xx3plVuxM4q0U6mBNJlcDpwK0lxW19zN2BUcD3ACJia0S8ThsfN8XHF+wvqQNwAMWHjLW5MUfEIuC1esUNjXM8UB0Rb0XEcxQfrTCi3HW194Bo6JnYbZqkPsBQ4A/Ae7KHNJH9PKQFu5aHG4EvAP8oKWvrY+4HrANuz3at3SqpC2143BHxV2Am8BfgRYoPH/slbXjM9TQ0zj36jGvvAVH2s6/bCkldgZ8AV0TEhpbuT54knQG8EhFLWrove1kHYBjw7YgYCvydtrFrpUHZPvfxQF/gvUAXSf/Wsr3aJ+zRZ1x7D4iyn33dFkjqSDEc7o6In2bFL0s6LJt/GPBKS/UvByOBMyWtobj78BRJ/0PbHjMU/65rI+IP2fSPKQZGWx73qcBzEbEuIrYBPwVOoG2PuVRD49yjz7j2HhDt5tnXkkRxn/QzEXFDyax5wPnZ+/OBn+3tvuUlIv49Iiojog/F3+2vI+LfaMNjBoiIl4AXJB2dFX0YeJq2Pe6/AMdLOiD7W/8wxeNsbXnMpRoa5zxgoqROkvoC/YHHyl5qRLTrF8VnYv8JeBb4Ukv3J8dxnkhx03IZsDR7nQb0oHjWw5+znwe3dF9zGv9o4N7sfZsfM1AF1GS/73uAg9r6uIGvASuAp4DvA53a4piB/6V4nGUbxS2EC3c1TuBL2efbSmBcY9blW22YmVlSe9/FZGZmDXBAmJlZkgPCzMySHBBmZpbkgDAzsyQHhFkjSHpb0tKSV7NdoSypT+kdOs1aWoeW7oBZK7M5IqpauhNme4O3IMyagaQ1kv5T0mPZ6/1Z+ZGSFkpalv08Iit/j6S5kv6YvU7IFlUh6bvZcw1+KWn/FhuUtXsOCLPG2b/eLqZzSuZtiIgRwE0U7yJL9v6uiBgM3A3MyspnAb+JiCEU75O0PCvvD8yOiGOA14FP5Doas13wldRmjSDpzYjomihfA5wSEauzmyK+FBE9JL0KHBYR27LyFyOip6R1QGVEvFWyjD7Ar6L40BckXQN0jIj/2AtDM9uJtyDMmk808L6hOilvlbx/Gx8ntBbkgDBrPueU/Pxd9v5RineSBTgXeCR7vxC4BOqemd19b3XSrFz+dmLWOPtLWloyfX9E7DjVtZOkP1D84jUpK7scuE3S1RSf8nZBVv5ZYI6kCyluKVxC8Q6dZvsMH4MwawbZMYhCRLza0n0xay7exWRmZknegjAzsyRvQZiZWZIDwszMkhwQZmaW5IAwM7MkB4SZmSX9fyYCXtkGT2fuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start your code here\n",
    "# Calculate the metrics for test set and fill in the table below\n",
    "\n",
    "# SGD\n",
    "y_hat_sgd = model_sgd.predict(y_test)\n",
    "accuracy_sgd, precision_sgd, recall_sgd, f1_sgd=get_metrics(y_hat_sgd, y_test)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# sgd_train_history, sgd_valid_history\n",
    "plt.plot(range(num_epoch), sgd_train_history['loss'], sgd_valid_history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "plt.title('SGD')\n",
    "plt.show()\n",
    "\n",
    "# Mini-batch GD\n",
    "y_hat_mbgd = model_mbgd.predict(y_test)\n",
    "accuracy_mbgd, precision_mbgd, recall_mbgd, f1_mbgd=get_metrics(y_hat_mbgd, y_test)\n",
    "\n",
    "plt.plot(range(num_epoch), mbgd_train_history['loss'], mbgd_valid_history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend([\"Training loss\", \"Validation loss\"])\n",
    "plt.title('Mini-batch GD')\n",
    "plt.show()\n",
    "\n",
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7f6f4",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics on Test set\n",
    "Fill this table with the result you just printed (double click this cell to edit)\n",
    "\n",
    "valid_loss: 0.1825 - valid_accuracy: 0.8632 - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
    "\n",
    "valid_loss: 0.3149 - valid_accuracy0.8632: 0. - valid_precision: 0.8632 - valid_recall: 0.8632 - valid_f1: 0.8632\n",
    "\n",
    "\n",
    "\n",
    "|     Optimizer     | Accuracy    | Precision   | Recall      | F1 Score    |\n",
    "|:-----------------:|-------------|-------------|-------------|-------------|\n",
    "|      **SGD**      |    0.8632   |   0.8632    |   0.8632    |    0.8632   |\n",
    "| **Mini-batch GD** |    0.8632   |   0.8632    |   0.8632    |    0.8632   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9416159",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the training loss curve for SGD and Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "id": "ecab27a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.plot(sgd_train_history['loss'], label='SGD')\n",
    "plt.plot(mbgd_train_history['loss'], label='Mini-batch GD')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71ab20",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the validation metrics curve for SGD and Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "id": "38c67a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(20, 3))\n",
    "for i, key in enumerate(sgd_valid_history.keys()):\n",
    "    sgd_vals, mbgd_vals = sgd_valid_history[key], mbgd_valid_history[key]\n",
    "    ax = axes[i]\n",
    "    ax.plot(sgd_vals, label='SGD')\n",
    "    ax.plot(mbgd_vals, label='Mini-batch GD')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_title('Validation ' + key.capitalize())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc09fc66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Cross-Validation (5 points)\n",
    "\n",
    "You are required to use cross-validation to choose the best $\\lambda$ with Mini-batch GD.\n",
    "\n",
    "The best $\\lambda$ should be chosen by monitoring validation $F_1$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "id": "e69f73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "0240a346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1\n",
      "Epoch 1 / 100: train_loss: -0.1457 - valid_loss: -0.1024 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 100 / 100: train_loss: -194.9452 - valid_loss: -131.2454 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 1 / 100: train_loss: -198.8692 - valid_loss: -133.8795 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -781.0542 - valid_loss: -523.2812 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -788.9007 - valid_loss: -528.5252 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -1759.9059 - valid_loss: -1177.1418 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -1771.6672 - valid_loss: -1184.9956 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -3129.9542 - valid_loss: -2091.7991 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -3144.8234 - valid_loss: -2522.6966 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "Epoch 100 / 100: train_loss: -4887.9588 - valid_loss: -3919.0868 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "\n",
      "\n",
      "0.1\n",
      "Epoch 1 / 100: train_loss: -0.0123 - valid_loss: -0.0009 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 100 / 100: train_loss: -19.4920 - valid_loss: -13.1152 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 1 / 100: train_loss: -19.8844 - valid_loss: -13.3833 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -78.1029 - valid_loss: -52.3235 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -78.8876 - valid_loss: -52.8479 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -175.9881 - valid_loss: -117.7095 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -177.1642 - valid_loss: -118.4949 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -312.9929 - valid_loss: -209.1753 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -314.4798 - valid_loss: -252.2604 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "Epoch 100 / 100: train_loss: -488.7934 - valid_loss: -391.8994 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "\n",
      "\n",
      "0.01\n",
      "Epoch 1 / 100: train_loss: 0.0010 - valid_loss: 0.0092 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 100 / 100: train_loss: -1.9467 - valid_loss: -1.3022 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 1 / 100: train_loss: -1.9859 - valid_loss: -1.3337 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -7.8078 - valid_loss: -5.2277 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -7.8863 - valid_loss: -5.2801 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -17.5963 - valid_loss: -11.7663 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -17.7139 - valid_loss: -11.8448 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -31.2968 - valid_loss: -20.9129 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -31.4455 - valid_loss: -25.2167 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "Epoch 100 / 100: train_loss: -48.8768 - valid_loss: -39.1806 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "\n",
      "\n",
      "0.001\n",
      "Epoch 1 / 100: train_loss: 0.0024 - valid_loss: 0.0102 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 100 / 100: train_loss: -0.1922 - valid_loss: -0.1209 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 1 / 100: train_loss: -0.1961 - valid_loss: -0.1287 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -0.7783 - valid_loss: -0.5181 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -0.7861 - valid_loss: -0.5234 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -1.7571 - valid_loss: -1.1720 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -1.7689 - valid_loss: -1.1798 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -3.1272 - valid_loss: -2.0866 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -3.1420 - valid_loss: -2.5124 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "Epoch 100 / 100: train_loss: -4.8852 - valid_loss: -3.9088 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "\n",
      "\n",
      "0.0001\n",
      "Epoch 1 / 100: train_loss: 0.0025 - valid_loss: 0.0103 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 100 / 100: train_loss: -0.0167 - valid_loss: -0.0028 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 1 / 100: train_loss: -0.0171 - valid_loss: -0.0082 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -0.0753 - valid_loss: -0.0472 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -0.0761 - valid_loss: -0.0477 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -0.1732 - valid_loss: -0.1126 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -0.1744 - valid_loss: -0.1133 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -0.3102 - valid_loss: -0.2040 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -0.3117 - valid_loss: -0.2419 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "Epoch 100 / 100: train_loss: -0.4860 - valid_loss: -0.3816 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "\n",
      "\n",
      "1e-05\n",
      "Epoch 1 / 100: train_loss: 0.0025 - valid_loss: 0.0103 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 100 / 100: train_loss: 0.0008 - valid_loss: 0.0090 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 1 / 100: train_loss: 0.0008 - valid_loss: 0.0038 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -0.0050 - valid_loss: -0.0001 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -0.0051 - valid_loss: -0.0001 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -0.0148 - valid_loss: -0.0066 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -0.0149 - valid_loss: -0.0067 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -0.0285 - valid_loss: -0.0158 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -0.0287 - valid_loss: -0.0149 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "Epoch 100 / 100: train_loss: -0.0461 - valid_loss: -0.0289 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "\n",
      "\n",
      "1e-06\n",
      "Epoch 1 / 100: train_loss: 0.0025 - valid_loss: 0.0103 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 100 / 100: train_loss: 0.0026 - valid_loss: 0.0102 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 1 / 100: train_loss: 0.0026 - valid_loss: 0.0050 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: 0.0020 - valid_loss: 0.0046 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: 0.0020 - valid_loss: 0.0046 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: 0.0010 - valid_loss: 0.0040 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: 0.0010 - valid_loss: 0.0040 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: -0.0003 - valid_loss: 0.0031 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: -0.0004 - valid_loss: 0.0078 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "Epoch 100 / 100: train_loss: -0.0021 - valid_loss: 0.0064 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "\n",
      "\n",
      "1e-07\n",
      "Epoch 1 / 100: train_loss: 0.0025 - valid_loss: 0.0103 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 100 / 100: train_loss: 0.0028 - valid_loss: 0.0103 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 1 / 100: train_loss: 0.0028 - valid_loss: 0.0051 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: 0.0027 - valid_loss: 0.0051 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: 0.0027 - valid_loss: 0.0051 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: 0.0026 - valid_loss: 0.0050 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: 0.0026 - valid_loss: 0.0050 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: 0.0025 - valid_loss: 0.0050 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: 0.0025 - valid_loss: 0.0101 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "Epoch 100 / 100: train_loss: 0.0023 - valid_loss: 0.0099 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "\n",
      "\n",
      "0.0\n",
      "Epoch 1 / 100: train_loss: 0.0025 - valid_loss: 0.0103 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 100 / 100: train_loss: 0.0028 - valid_loss: 0.0103 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 1 / 100: train_loss: 0.0028 - valid_loss: 0.0052 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: 0.0028 - valid_loss: 0.0052 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: 0.0028 - valid_loss: 0.0052 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: 0.0028 - valid_loss: 0.0052 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: 0.0028 - valid_loss: 0.0052 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 100 / 100: train_loss: 0.0028 - valid_loss: 0.0052 - valid_accuracy: 0.8333 - valid_precision: 0.8333 - valid_recall: 0.8333 - valid_f1: 0.8333\n",
      "Epoch 1 / 100: train_loss: 0.0028 - valid_loss: 0.0103 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "Epoch 100 / 100: train_loss: 0.0028 - valid_loss: 0.0103 - valid_accuracy: 0.6000 - valid_precision: 0.6000 - valid_recall: 0.6000 - valid_f1: 0.6000\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "best_lambda = 0.0\n",
    "best_f1 = 0.0\n",
    "\n",
    "num_epoch = 100\n",
    "lr = 1e-1\n",
    "batch_size = 4\n",
    "print_every = 100\n",
    "# Start your code here\n",
    "#     Step 1. Iterate all lambdas\n",
    "#     Step 2. set the random seed and run train_mbgd for every lambda\n",
    "#     Step 3. Choose the best lambda\n",
    "\n",
    "k = 5\n",
    "\n",
    "mini_batch_losses = []\n",
    "lambda_loss_map = dict()\n",
    "\n",
    "x_train_batches = np.array_split(x_train, x_train.shape[0] // k)\n",
    "y_train_batches = np.array_split(y_train, x_train.shape[0] // k)\n",
    "\n",
    "for lam in lambdas:\n",
    "    np.random.seed(6666)\n",
    "    \n",
    "    model_mbgd_lr = LogisticRegression(feature_dim=vocab_size, lambda_=lam)\n",
    "    mini_batch_loss = 0\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(lam)\n",
    "    \n",
    "    for i in range(k):\n",
    "        xk_val = x_train_batches[i]\n",
    "        yk_val = y_train_batches[i]\n",
    "        \n",
    "        xk_train = np.vstack((x_train_batches[:i]+x_train_batches[i+1:]))\n",
    "        yk_train = np.vstack((y_train_batches[:i]+y_train_batches[i+1:]))\n",
    "        \n",
    "        mbgd_train_history, mbgd_valid_history=\\\n",
    "        train_mbgd(model_mbgd_lr, xk_train, yk_train, xk_val, yk_val, lr, num_epoch, batch_size, print_every)\n",
    "\n",
    "        mini_batch_loss += mbgd_valid_history['loss'][-1]\n",
    "    \n",
    "    mini_batch_loss /= k\n",
    "    \n",
    "    mini_batch_losses.append(mini_batch_loss)\n",
    "    \n",
    "    lambda_loss_map[lam] = mini_batch_loss\n",
    "    \n",
    "best_lambda=min(lambda_loss_map, key=lambda_loss_map.get)\n",
    "\n",
    "# End\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb1425",
   "metadata": {},
   "source": [
    "#### Report the best $\\lambda$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42d818",
   "metadata": {},
   "source": [
    "Answer: **1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e49df",
   "metadata": {},
   "source": [
    "# 4. Conclusion (5 Points)\n",
    "\n",
    "Provide an analysis for all the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531ca04",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "We can see that stochastic gradient descent algorithm outperforms the mini-batch gradient descent algorithm with respect to percentage accuracy and takes longer time to converge. stochastic gradient descent algorithmstochastic gradient descent algorithm is computationally less expensive than Batch GD. \n",
    "\n",
    "Compared to batch gradient descent (or \"standard\"), stochastic gradient descent (SGD) tends to reach convergence much faster.\n",
    "\n",
    "As opposed to batch gradient descent, in which the gradient is computed using the entire dataset, SGD, also called incremental gradient descent, searches for the minimum or maximum by iteration from one randomly selected example, which typically causes a higher level of error than gradient descent.\n",
    "\n",
    "The stochastic gradient descent, however, can also avoid shallow local minima more easily.\n",
    "\n",
    "With stochastic gradient descent, the data sample must be in a random order.Compared to batch gradient descent (or \"standard\"), stochastic gradient descent (SGD) tends to reach convergence much faster.\n",
    "\n",
    "As opposed to batch gradient descent, in which the gradient is computed using the entire dataset, SGD, also called incremental gradient descent, searches for the minimum or maximum by iteration from one randomly selected example, which typically causes a higher level of error than gradient descent.\n",
    "\n",
    "The stochastic gradient descent, however, can also avoid shallow local minima more easily.\n",
    "\n",
    "With stochastic gradient descent, the data sample must be in a random order.\n",
    "\n",
    "We can notice that the best lambda value is **1**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "318b20e5e552330ac64402a63c1a3d538eebc51f858fc08a88a1f9c9a530966c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
