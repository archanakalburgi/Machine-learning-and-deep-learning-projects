{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "91a0e79c",
      "metadata": {
        "id": "91a0e79c"
      },
      "source": [
        "# CS 584 Assignment 3 -- Language Model\n",
        "\n",
        "#### Name: Archana Kalburgi\n",
        "#### Stevens ID: 10469491"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "316c650d",
      "metadata": {
        "id": "316c650d"
      },
      "source": [
        "## In this assignment, you are required to follow the steps below:\n",
        "1. Review the lecture slides.\n",
        "2. Implement N-gram language modeling.\n",
        "3. Implement RNN language modeling.\n",
        "\n",
        "**Before you start**\n",
        "- Please read the code very carefully.\n",
        "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons) using the following command.\n",
        "```console\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
        "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
        "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0f8b4e",
      "metadata": {
        "id": "1c0f8b4e"
      },
      "source": [
        "## Part A: 1. N-Gram (60 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "21f28cd3",
      "metadata": {
        "id": "21f28cd3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "def print_line(*args):\n",
        "    \"\"\" Inline print and go to the begining of line\n",
        "    \"\"\"\n",
        "    args1 = [str(arg) for arg in args]\n",
        "    str_ = ' '.join(args1)\n",
        "    print('\\r' + str_, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "58abd5c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58abd5c8",
        "outputId": "2332afe4-7771-4a7d-f589-c380ad52bd49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# If you are going to use GPU, make sure the GPU is in the output\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "e9627745",
      "metadata": {
        "id": "e9627745"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Union, Dict\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "_6oWTYHf20ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6oWTYHf20ea",
        "outputId": "058cb750-1ef7-47d8-bbc5-cb96e383b5e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3550e5b8",
      "metadata": {
        "id": "3550e5b8",
        "tags": []
      },
      "source": [
        "### 1.1 Load Data & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db60cd7",
      "metadata": {
        "id": "2db60cd7"
      },
      "source": [
        "You will not need to implement the data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "1bf05fcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bf05fcb",
        "outputId": "fd222051-2cdd-436b-adbc-21a26d731a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of train sentences: 42068\n",
            "number of valid sentences: 3370\n",
            "number of test sentences: 3165\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "# data_path = 'a3-data'\n",
        "data_path = '/content/drive/MyDrive/nlp_assignments/a3-data'\n",
        "\n",
        "train_sentences = open(os.path.join(data_path, 'train.txt')).readlines()\n",
        "valid_sentences = open(os.path.join(data_path, 'valid.txt')).readlines()\n",
        "test_sentences = open(os.path.join(data_path, 'input.txt')).readlines()\n",
        "print('number of train sentences:', len(train_sentences))\n",
        "print('number of valid sentences:', len(valid_sentences))\n",
        "print('number of test sentences:', len(test_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "f3d1b612",
      "metadata": {
        "id": "f3d1b612"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, punctuation=True, url=True, number=True):\n",
        "        self.punctuation = punctuation\n",
        "        self.url = url\n",
        "        self.number = number\n",
        "\n",
        "    def apply(self, sentence: str) -> str:\n",
        "        \"\"\" Apply the preprocessing rules to the sentence\n",
        "        Args:\n",
        "            sentence: raw sentence\n",
        "        Returns:\n",
        "            sentence: clean sentence\n",
        "        \"\"\"\n",
        "        sentence = sentence.lower()\n",
        "        sentence = sentence.replace('<unk>', '')\n",
        "        if self.url:\n",
        "            sentence = Preprocessor.remove_url(sentence)\n",
        "        if self.punctuation:\n",
        "            sentence = Preprocessor.remove_punctuation(sentence)\n",
        "        if self.number:\n",
        "            sentence = Preprocessor.remove_number(sentence)\n",
        "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_punctuation(sentence: str) -> str:\n",
        "        \"\"\" Remove punctuations in sentence with re\n",
        "        Args:\n",
        "            sentence: sentence with possible punctuations\n",
        "        Returns:\n",
        "            sentence: sentence without punctuations\n",
        "        \"\"\"\n",
        "        sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_url(sentence: str) -> str:\n",
        "        \"\"\" Remove urls in text with re\n",
        "        Args:\n",
        "            sentence: sentence with possible urls\n",
        "        Returns:\n",
        "            sentence: sentence without urls\n",
        "        \"\"\"\n",
        "        sentence = re.sub(r'(https|http)?://(\\w|\\.|/|\\?|=|&|%)*\\b', ' ', sentence)\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_number(sentence: str) -> str:\n",
        "        \"\"\" Remove numbers in sentence with re\n",
        "        Args:\n",
        "            sentence: sentence with possible numbers\n",
        "        Returns:\n",
        "            sentence: sentence without numbers\n",
        "        \"\"\"\n",
        "        sentence = re.sub(r'\\d+', ' ', sentence)\n",
        "        return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "c8ccfff1",
      "metadata": {
        "id": "c8ccfff1"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, sos_token='<s>', eos_token='</s>', pad_token='<pad>', unk_token='<unk>', mask_token='<mask>'):\n",
        "        # Special tokens.\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.mask_token = mask_token\n",
        "        \n",
        "        self.vocab = { sos_token: 0, eos_token: 1, pad_token: 2, unk_token: 3, mask_token: 4 }  # token -> id\n",
        "        self.inverse_vocab = { 0: sos_token, 1: eos_token, 2: pad_token, 3: unk_token, 4: mask_token }  # id -> token\n",
        "        self.token_occurrence = { sos_token: 0, eos_token: 0, pad_token: 0, unk_token: 0, mask_token: 0 }  # token -> occurrence\n",
        "        \n",
        "        self.preprocessor = Preprocessor()\n",
        "\n",
        "    @property\n",
        "    def sos_token_id(self):\n",
        "        \"\"\" Create a property method.\n",
        "            You can use self.sos_token_id or tokenizer.sos_token_id to get the id of the sos_token.\n",
        "        \"\"\"\n",
        "        return self.vocab[self.sos_token]\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self):\n",
        "        return self.vocab[self.eos_token]\n",
        "\n",
        "    @property\n",
        "    def pad_token_id(self):\n",
        "        return self.vocab[self.pad_token]\n",
        "\n",
        "    @property\n",
        "    def unk_token_id(self):\n",
        "        return self.vocab[self.unk_token]\n",
        "\n",
        "    @property\n",
        "    def mask_token_id(self):\n",
        "        return self.vocab[self.mask_token]\n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\" A magic method that enable program to know the number of tokens by calling:\n",
        "            ```python\n",
        "            tokenizer = Tokenizer()\n",
        "            num_tokens = len(tokenizer)\n",
        "            ```\n",
        "        \"\"\"\n",
        "        return len(self.vocab)\n",
        "        \n",
        "    def fit(self, sentences: List[str]):\n",
        "        \"\"\" Fit the tokenizer using all sentences.\n",
        "        1. Tokenize the sentence by splitting with spaces.\n",
        "        2. Record the occurrence of all tokens\n",
        "        3. Construct the token to index (self.vocab) map and the inversed map (self.inverse_vocab) based on the occurrence. The token with a higher occurrence has the smaller index\n",
        "\n",
        "        Args:\n",
        "            sentences: All sentences in the dataset.\n",
        "        \"\"\"\n",
        "        n = len(sentences)\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0 or i == n - 1:\n",
        "                print_line('Fitting Tokenizer:', (i + 1), '/', n)\n",
        "            tokens = self.preprocessor.apply(sentence.strip()).split()\n",
        "            if len(tokens) <= 1:\n",
        "                continue\n",
        "            for token in tokens:\n",
        "                if token == '<unk>':\n",
        "                    continue\n",
        "                self.token_occurrence[token] = self.token_occurrence.get(token, 0) + 1\n",
        "        print_line('\\n')\n",
        "\n",
        "        token_occurrence = sorted(self.token_occurrence.items(), key=lambda e: e[1], reverse=True)\n",
        "        for token, occurrence in token_occurrence[:-5]:\n",
        "            token_id = len(self.vocab)\n",
        "            self.vocab[token] = token_id\n",
        "            self.inverse_vocab[token_id] = token\n",
        "\n",
        "        print('The number of distinct tokens:', len(self.vocab))\n",
        "        \n",
        "    def encode(self, sentences: List[str]) -> List[List[int]]:\n",
        "        \"\"\" Encode the sentences into token ids\n",
        "            Note: 1. if a token in a sentence does not exist in the fit encoder, we ignore it.\n",
        "                  2. If the number of tokens in a sentence is less than two, we ignore this sentence.\n",
        "                  3. Note that, for every sentence, we will add an sos_token, i.e., the id of <s> at the start of the sentence,\n",
        "                     and add an eos_token, i.e., the id of </s> at the end of the sentence.\n",
        "        Args:\n",
        "            sentences: Raw sentences\n",
        "        Returns:\n",
        "            sent_token_ids: A list of id list\n",
        "        \"\"\"\n",
        "        n = len(sentences)\n",
        "        sent_token_ids = []\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0 or i == n - 1:\n",
        "                print_line('Encoding with Tokenizer:', (i + 1), '/', n)\n",
        "            token_ids = []\n",
        "            tokens = self.preprocessor.apply(sentence.strip()).split()\n",
        "            for token in tokens:\n",
        "                if token == '<unk>':\n",
        "                    continue\n",
        "                if token in self.vocab:\n",
        "                    token_ids.append(self.vocab[token])\n",
        "            if len(token_ids) <= 1:\n",
        "                continue\n",
        "            token_ids = [self.sos_token_id] + token_ids + [self.eos_token_id]\n",
        "            sent_token_ids.append(token_ids)\n",
        "        print_line('\\n')\n",
        "        return sent_token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "5ae8a449",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ae8a449",
        "outputId": "0ca8dc7e-00a5-4162-e901-d8621375f4b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rFitting Tokenizer: 1 / 2\rFitting Tokenizer: 2 / 2\r\n",
            "The number of distinct tokens: 44\n",
            "\n",
            "n : 2\n",
            "aer : 1\n",
            "banknote : 1\n",
            "berlitz : 1\n",
            "calloway : 1\n",
            "centrust : 1\n",
            "cluett : 1\n",
            "fromstein : 1\n",
            "gitano : 1\n",
            "guterman : 1\n",
            "\n",
            "\rEncoding with Tokenizer: 1 / 2\rEncoding with Tokenizer: 2 / 2\r\n",
            "\n",
            " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
            " ['<s>', 'aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro', 'quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack', 'food', 'ssangyong', 'swapo', 'wachter', '</s>'] \n",
            "\n",
            " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
            " ['<s>', 'pierre', 'n', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov', 'n', '</s>'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit(train_sentences[:2])\n",
        "print()\n",
        "\n",
        "token_occurrence = sorted(tokenizer.token_occurrence.items(), key=lambda e: e[1], reverse=True)\n",
        "for token, occurrence in token_occurrence[:10]:\n",
        "    print(token, ':', occurrence)\n",
        "print()\n",
        "sent_token_ids = tokenizer.encode(train_sentences[:2])\n",
        "print()\n",
        "for original_sentence, token_ids in zip(train_sentences[:2], sent_token_ids):\n",
        "    sentence = [tokenizer.inverse_vocab[token] for token in token_ids]\n",
        "    print(original_sentence, sentence, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "c1820989",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1820989",
        "outputId": "cbcc1751-c1bd-4d6a-9b58-a4118565e9d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting Tokenizer: 42068 / 42068\n",
            "The number of distinct tokens: 9614\n",
            "Encoding with Tokenizer: 42068 / 42068\n",
            "Encoding with Tokenizer: 3370 / 3370\n",
            "Encoding with Tokenizer: 3165 / 3165\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit(train_sentences)\n",
        "train_token_ids = tokenizer.encode(train_sentences)\n",
        "valid_token_ids = tokenizer.encode(valid_sentences)\n",
        "test_token_ids = tokenizer.encode(test_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead405c0",
      "metadata": {
        "id": "ead405c0",
        "tags": []
      },
      "source": [
        "### 1.2 Calculate unigram and bigram count (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "70e51bbd",
      "metadata": {
        "id": "70e51bbd"
      },
      "outputs": [],
      "source": [
        "def get_unigram_count(train_token_ids: List[List[int]]) -> Dict:\n",
        "    \"\"\" Calculate the occurrence of each token in the dataset.\n",
        "    \n",
        "    Args:\n",
        "        train_token_ids: each element is a list of token ids\n",
        "    Return:\n",
        "        unigram_count: A map from token_id to occurrence\n",
        "    \"\"\"\n",
        "    unigram_count = {}\n",
        "    # Start your code here\n",
        "    \n",
        "    for ids in train_token_ids:\n",
        "        for i in ids:\n",
        "            if i in unigram_count:\n",
        "                unigram_count[i]+=1\n",
        "            else:\n",
        "                unigram_count[i]=1 \n",
        "\n",
        "    # End\n",
        "    return unigram_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "af2b0a9f",
      "metadata": {
        "id": "af2b0a9f"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "def get_bigram_count(train_token_ids: List[List[int]]) -> Dict[int, Dict]:\n",
        "    \"\"\" Calculate the occurrence of bigrams in the dataset.\n",
        "    \n",
        "    Args:\n",
        "        train_token_ids: each element is a list of token ids\n",
        "    Return:\n",
        "        bigram_count: A map from token_id to next token occurrence. Key: token_id, value: Dict[token_id -> occurrence]\n",
        "                      For example, {\n",
        "                          5: { 10: 5, 20: 4 }\n",
        "                      } means (5, 10) occurs 10 times, (5, 20) occurs 4 times.\n",
        "    \"\"\"\n",
        "    bigram_count = {}\n",
        "    \n",
        "    # Start your code here\n",
        "    bigram_freq = Counter()\n",
        "    \n",
        "    for ids in train_token_ids:\n",
        "        for i in range(len(ids)-1):\n",
        "            bigram_freq[ids[i], ids[i+1]] += 1\n",
        "\n",
        "    for (t1,t2),v in bigram_freq.items():\n",
        "        if t1 not in bigram_count:\n",
        "            bigram_count[t1] = {t2:v}\n",
        "        else:\n",
        "            bigram_count[t1].update({t2:v})\n",
        "\n",
        "    # End\n",
        "    return bigram_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "5800f820",
      "metadata": {
        "id": "5800f820",
        "tags": []
      },
      "outputs": [],
      "source": [
        "unigram_count = get_unigram_count(train_token_ids)\n",
        "bigram_count = get_bigram_count(train_token_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72feba53",
      "metadata": {
        "id": "72feba53",
        "tags": []
      },
      "source": [
        "### 1.3 BiGram (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "d686d1c6",
      "metadata": {
        "id": "d686d1c6"
      },
      "outputs": [],
      "source": [
        "class BiGram:\n",
        "    def __init__(self, unigram_count, bigram_count):\n",
        "        self.unigram_count = unigram_count\n",
        "        self.bigram_count = bigram_count\n",
        "        \n",
        "    def calc_prob(self, w1: int, w2: int) -> float:\n",
        "        \"\"\" Calculate the probability of p(w2 | w1) using the BiGram model.\n",
        "        \n",
        "        Args:\n",
        "            w1, w2: current token and next token\n",
        "        Note:\n",
        "            if prob you calculated is 0, you should return 1e-5.\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "\n",
        "        prob = float(self.bigram_count[w1].get(w2,0)/self.unigram_count[w1])\n",
        "    \n",
        "        return 1E-5 if prob == 0 else prob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fb70f7c",
      "metadata": {
        "id": "3fb70f7c",
        "tags": []
      },
      "source": [
        "### 1.4 Good Turing (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "7229229e",
      "metadata": {
        "id": "7229229e"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import curve_fit\n",
        "from collections import defaultdict\n",
        "\n",
        "def power_law(x, a, b):\n",
        "    \"\"\" Power law to fit the number of occurrence\n",
        "    \"\"\"\n",
        "    return a * np.power(x, b)\n",
        "\n",
        "\n",
        "class GoodTuring(BiGram):\n",
        "    def __init__(self, unigram_count, bigram_count, threshold=100):\n",
        "        super().__init__(unigram_count, bigram_count)\n",
        "        self.threshold = threshold\n",
        "        \n",
        "        self.bigram_Nc = self.calc_Nc()\n",
        "        self.bi_c_star, self.bi_N = self.smoothing(self.bigram_Nc)\n",
        "\n",
        "    def calc_Nc(self) -> Dict[int, Union[float, int]]:\n",
        "        \"\"\" You need to calculate Nc of bigram\n",
        "        \n",
        "        Return:\n",
        "            bigram_Nc: A map from count to the occurrence (count of count)\n",
        "                       For example {\n",
        "                           10: 78\n",
        "                       } means there are 78 bigrams occurs 10 times in the dataset.\n",
        "                       Also, 10 is a small c, for large c, it's occurrence will be replaced with the power law.\n",
        "        \"\"\"\n",
        "        bigram_Nc = {}\n",
        "        # Start your code here\n",
        "        # Count the occurrence of count in self.bigram_count.\n",
        "        counter = Counter()\n",
        "        for value in self.bigram_count.values():\n",
        "            counter += Counter(value.values())\n",
        "            \n",
        "        bigram_Nc = defaultdict(lambda: 0, counter)\n",
        "        # End\n",
        "\n",
        "        self.replace_large_c(bigram_Nc)\n",
        "        return bigram_Nc\n",
        "\n",
        "    def replace_large_c(self, Nc):\n",
        "        \"\"\" Fit with power law\n",
        "        \"\"\"\n",
        "        x, y = zip(*sorted(Nc.items(), reverse=True))\n",
        "        popt, pcov = curve_fit(power_law, x, y, bounds=([0, -np.inf], [np.inf, 0]))\n",
        "        a, b = popt\n",
        "\n",
        "        max_count = max(Nc.keys())\n",
        "        for c in range(self.threshold + 1, max_count + 2):\n",
        "            Nc[c] = power_law(c, a, b)\n",
        "\n",
        "    def smoothing(self, Nc: Dict[int, Union[float, int]]) -> Tuple[Dict[int, float], float]:\n",
        "        \"\"\" Calculate the c_star and N\n",
        "        \n",
        "        Args:\n",
        "            self.bigram_Nc\n",
        "        Returns:\n",
        "            c_star: The mapping from bigram count to smoothed count\n",
        "            N: The sum of c multiplied by Nc\n",
        "        \"\"\"\n",
        "        c_star = defaultdict(lambda: 0, Nc)\n",
        "        N = 0\n",
        "        max_count = max(Nc.keys())\n",
        "        # Start your code here\n",
        "        # 1. loop the c from 1 to max_count\n",
        "        # 2. calculate the smoothed count for c and store it to c_star\n",
        "        # 3. update N\n",
        "        for c in range(1, max_count):\n",
        "            c_star[c] = (c+1) * (Nc[c+1])/Nc[c]\n",
        "            N += c * Nc[c]\n",
        "        # End\n",
        "        return c_star, N\n",
        "\n",
        "    def calc_prob(self, w1, w2):\n",
        "        \"\"\" Calculate the probability of p(w2 | w1) using the Good Turing model.\n",
        "        \n",
        "        Args:\n",
        "            w1, w2: current token and next token\n",
        "        Note:\n",
        "            1. The numerator is the smoothed bigram count of (w1, w2)\n",
        "            2. The denominator is the unigram count of w1\n",
        "            3. You should be careful to distinguish when (w1, w2) does not exists in the training data.\n",
        "        \"\"\"\n",
        "        prob = 0 \n",
        "        c_bi = bigram_count[w1].get(w2,None)\n",
        "        prob = (self.bigram_Nc[1]/self.bi_N)/unigram_count[w1] if c_bi is None else self.bi_c_star[c_bi]/unigram_count[w1]\n",
        "\n",
        "        return prob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e727355",
      "metadata": {
        "id": "2e727355",
        "tags": []
      },
      "source": [
        "### 1.5 Kneser-Ney (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "dc6a82cf",
      "metadata": {
        "id": "dc6a82cf"
      },
      "outputs": [],
      "source": [
        "class KneserNey(BiGram):\n",
        "    def __init__(self, unigram_count, bigram_count, d=0.75):\n",
        "        super().__init__(unigram_count, bigram_count)\n",
        "        self.d = d\n",
        "        \n",
        "        self.lambda_ = self.calc_lambda()\n",
        "        self.p_continuation = self.calc_p_continuation()\n",
        "        \n",
        "    def calc_lambda(self):\n",
        "        \"\"\" Calculate the λ(w)\n",
        "        \n",
        "        Return:\n",
        "            lambda_: A dict from token_id (w) to λ(w).\n",
        "        \"\"\"\n",
        "        lambda_ = {}\n",
        "        # Start your code here\n",
        "        \n",
        "        w1_list = list(bigram_count.keys())\n",
        "        for w1 in w1_list:\n",
        "            uni_cnt = unigram_count.get(w1,0)\n",
        "            normalised_d = self.d/uni_cnt\n",
        "            count = len(bigram_count[w1])\n",
        "            lambda_[w1] = normalised_d*count\n",
        "            \n",
        "        # End\n",
        "        return lambda_\n",
        "    \n",
        "    \n",
        "    \n",
        "    def calc_p_continuation(self):\n",
        "        \"\"\" Calculate the p_continuation(w)\n",
        "        \n",
        "        Return:\n",
        "            lambda_: A dict from token_id (w) to λ(w).\n",
        "        \"\"\"\n",
        "        numerator = {}  # token -> type of previous token\n",
        "        denominator = len(self.bigram_count)  # type of all previous tokens\n",
        "        # Start your code here\n",
        "        # 1. loop token, next_token_occurrence in self.bigram_count\n",
        "        # 2. loop next_token in next_token_occurrence and update its previous token type       \n",
        "        for token, next_token in self.bigram_count.items():\n",
        "            for nt in next_token:\n",
        "                count = numerator.get(nt, None) \n",
        "                if count == None:\n",
        "                    numerator[nt]=1\n",
        "                else:\n",
        "                    numerator[nt]+=1\n",
        "        #End\n",
        "        p_continuation = { 0: 0, 2: 0, 3: 0, 4: 0 }\n",
        "        for w, count in numerator.items():\n",
        "            p_continuation[w] = count / denominator\n",
        "        return p_continuation\n",
        "    \n",
        "    def calc_prob(self, w1, w2):\n",
        "        \"\"\" Calculate the probability of p(w2 | w1) using the Kneser-Ney model.\n",
        "        \n",
        "        Args:\n",
        "            w1, w2: current token and next token\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        \n",
        "        c_w1_w2 = self.bigram_count[w1][w2] if w1 in self.bigram_count and w2 in self.bigram_count[w1] else 0\n",
        "        prob = max(c_w1_w2 - self.d, 0) / self.unigram_count[w1] + self.lambda_[w1] * self.p_continuation[w2]\n",
        "        \n",
        "        # End\n",
        "        return prob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a5d248",
      "metadata": {
        "id": "64a5d248"
      },
      "source": [
        "$$\\text{Perplexity} = \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P{(w_i | w_{i - 1}})}} = \\left(\\prod_{i=1}^{N}\\frac{1}{P{(w_i | w_{i - 1}})}\\right)^{\\frac{1}{n}} $$\n",
        "$$\\log_2{\\text{Perplexity}} = -\\frac{1}{n}\\sum_{i = 1}^{N}{\\log_2{P{(w_i | w_{i - 1}})}}$$\n",
        "$$\\text{Perplexity} = 2 ^ {-\\frac{1}{n} \\times \\text{sum of log probs}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449517dc",
      "metadata": {
        "id": "449517dc",
        "tags": []
      },
      "source": [
        "### 1.6 Perplexity (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "55566267",
      "metadata": {
        "id": "55566267"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def perplexity(model, token_ids):\n",
        "    \"\"\" Calculate the perplexity score.\n",
        "\n",
        "    Args:\n",
        "        model: the model you want to evaluate (BiGram, GoodTuring, or KneserNey)\n",
        "        token_ids: a list of validation token_ids\n",
        "    Return:\n",
        "        perplexity: the perplexity of the model on texts\n",
        "    Note:\n",
        "        \n",
        "    \"\"\"\n",
        "    log_probs = 0\n",
        "    n = len(token_ids)\n",
        "    n_words = 0\n",
        "    for i, tokens in enumerate(token_ids):\n",
        "        if i % 100 == 0 or i == n - 1:\n",
        "            print_line('Calculating perplexity:', (i + 1), '/', n)\n",
        "        # Start your code here\n",
        "        # 1. loop the bigram (w1, w2) in this sentence\n",
        "        # 2. get the probability of p(w2 | w2) from model.calc_prob\n",
        "        # 3. update the probability with log2 of prob\n",
        "        # 4. update number of words predicted (n_words, i.e., N in our slides)\n",
        "        \n",
        "        for i in range(len(tokens)-1):     \n",
        "            prob = model.calc_prob(tokens[i], tokens[i+1])   \n",
        "            log_probs += math.log2(prob)\n",
        "            n_words+=1\n",
        "                \n",
        "        # End\n",
        "\n",
        "    perp = 0\n",
        "    # Start your code here\n",
        "    # Calculate the final perplexity\n",
        "    perp= 2 ** ((-1 / n_words) * log_probs)\n",
        "    # End\n",
        "    print('\\n')\n",
        "    \n",
        "    return perp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a6a832e",
      "metadata": {
        "id": "8a6a832e"
      },
      "source": [
        "If you implement correctly, the perplexity of bigram will be around 320"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "be205a78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be205a78",
        "outputId": "b051afda-85a7-4ea7-f6bf-723f1ad0fa69",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rCalculating perplexity: 1 / 3352\rCalculating perplexity: 101 / 3352\rCalculating perplexity: 201 / 3352\rCalculating perplexity: 301 / 3352\rCalculating perplexity: 401 / 3352\rCalculating perplexity: 501 / 3352\rCalculating perplexity: 601 / 3352\rCalculating perplexity: 701 / 3352\rCalculating perplexity: 801 / 3352\rCalculating perplexity: 901 / 3352\rCalculating perplexity: 1001 / 3352\rCalculating perplexity: 1101 / 3352\rCalculating perplexity: 1201 / 3352\rCalculating perplexity: 1301 / 3352\rCalculating perplexity: 1401 / 3352\rCalculating perplexity: 1501 / 3352\rCalculating perplexity: 1601 / 3352\rCalculating perplexity: 1701 / 3352\rCalculating perplexity: 1801 / 3352\rCalculating perplexity: 1901 / 3352\rCalculating perplexity: 2001 / 3352\rCalculating perplexity: 2101 / 3352\rCalculating perplexity: 2201 / 3352\rCalculating perplexity: 2301 / 3352\rCalculating perplexity: 2401 / 3352\rCalculating perplexity: 2501 / 3352\rCalculating perplexity: 2601 / 3352\rCalculating perplexity: 2701 / 3352\rCalculating perplexity: 2801 / 3352\rCalculating perplexity: 2901 / 3352\rCalculating perplexity: 3001 / 3352\rCalculating perplexity: 3101 / 3352\rCalculating perplexity: 3201 / 3352\rCalculating perplexity: 3301 / 3352\rCalculating perplexity: 3352 / 3352\n",
            "\n",
            "The perplexity of Bigram is: 325.8354\n"
          ]
        }
      ],
      "source": [
        "bigram = BiGram(unigram_count, bigram_count)\n",
        "\n",
        "# Perplexity\n",
        "bigram_perplexity = perplexity(bigram, valid_token_ids)\n",
        "print(f'The perplexity of Bigram is: {bigram_perplexity:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8587dd",
      "metadata": {
        "id": "5e8587dd"
      },
      "source": [
        "If you implement correctly, the perplexity of good turing will be around 130"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "93dc1542-7dba-402c-990d-78d65286140b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93dc1542-7dba-402c-990d-78d65286140b",
        "outputId": "819e647c-f94c-4917-895b-da30e96b2e80",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rCalculating perplexity: 1 / 3352\rCalculating perplexity: 101 / 3352\rCalculating perplexity: 201 / 3352\rCalculating perplexity: 301 / 3352\rCalculating perplexity: 401 / 3352\rCalculating perplexity: 501 / 3352\rCalculating perplexity: 601 / 3352\rCalculating perplexity: 701 / 3352\rCalculating perplexity: 801 / 3352\rCalculating perplexity: 901 / 3352\rCalculating perplexity: 1001 / 3352\rCalculating perplexity: 1101 / 3352\rCalculating perplexity: 1201 / 3352\rCalculating perplexity: 1301 / 3352\rCalculating perplexity: 1401 / 3352\rCalculating perplexity: 1501 / 3352\rCalculating perplexity: 1601 / 3352\rCalculating perplexity: 1701 / 3352\rCalculating perplexity: 1801 / 3352\rCalculating perplexity: 1901 / 3352\rCalculating perplexity: 2001 / 3352\rCalculating perplexity: 2101 / 3352\rCalculating perplexity: 2201 / 3352\rCalculating perplexity: 2301 / 3352\rCalculating perplexity: 2401 / 3352\rCalculating perplexity: 2501 / 3352\rCalculating perplexity: 2601 / 3352\rCalculating perplexity: 2701 / 3352\rCalculating perplexity: 2801 / 3352\rCalculating perplexity: 2901 / 3352\rCalculating perplexity: 3001 / 3352\rCalculating perplexity: 3101 / 3352\rCalculating perplexity: 3201 / 3352\rCalculating perplexity: 3301 / 3352\rCalculating perplexity: 3352 / 3352\n",
            "\n",
            "The perplexity of Good Turing is: 130.5334\n"
          ]
        }
      ],
      "source": [
        "gt = GoodTuring(unigram_count, bigram_count, threshold=100)\n",
        "\n",
        "# Perplexity\n",
        "gt_perplexity = perplexity(gt, valid_token_ids)\n",
        "print(f'The perplexity of Good Turing is: {gt_perplexity:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aafe3b20",
      "metadata": {
        "id": "aafe3b20"
      },
      "source": [
        "If you implement correctly, the perplexity of good turing will be around 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "3b22699c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b22699c",
        "outputId": "2ce9b1c1-98ae-4729-feee-ced20ba60afb",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating perplexity: 3352 / 3352\n",
            "\n",
            "The perplexity of Kneser-Ney is: 62.5908\n"
          ]
        }
      ],
      "source": [
        "kn = KneserNey(unigram_count, bigram_count, d=0.75)\n",
        "\n",
        "# Perplexity\n",
        "kn_perplexity = perplexity(kn, valid_token_ids)\n",
        "print(f'The perplexity of Kneser-Ney is: {kn_perplexity:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a705d51c",
      "metadata": {
        "id": "a705d51c",
        "tags": []
      },
      "source": [
        "### 1.7 Predict the next word given a previous word (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "504a4d35",
      "metadata": {
        "id": "504a4d35"
      },
      "outputs": [],
      "source": [
        "def predict(model: 'BiGram', w1: int, vocab_size: int):\n",
        "    \"\"\" Predict the w2 with the hightest probability given w1\n",
        "    \n",
        "    Args:\n",
        "        model: A BiGram, GoodTuring, or KneserNey model that has the calc_prob function\n",
        "        w1: current word\n",
        "        vocab_size: the number of tokens in the vocabulary\n",
        "    \"\"\"\n",
        "    result = None\n",
        "    highest_prob = 0\n",
        "    for w2 in range(1, vocab_size):\n",
        "        # Start your code here\n",
        "        prob = model.calc_prob(w1,w2)\n",
        "        if prob > highest_prob:\n",
        "            highest_prob=prob\n",
        "            result = w2\n",
        "        #End\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "2bcc0c3a-4d08-4df1-a17f-eaaeada6a5c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bcc0c3a-4d08-4df1-a17f-eaaeada6a5c7",
        "outputId": "b9c0a0ff-ac0e-47ff-a3d2-8957e56242a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sharply falling stock prices do reduce consumer wealth damage business ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "but robert an official of the association said no ____\n",
            "predicted last token: longer\n",
            "---------------------------------------------\n",
            "it also has interests in military electronics and marine ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "first chicago since n has reduced its loans to such ____\n",
            "predicted last token: as\n",
            "---------------------------------------------\n",
            "david m jones vice president at g ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "the n stock specialist firms on the big board floor ____\n",
            "predicted last token: traders\n",
            "---------------------------------------------\n",
            "at the same time the business was hurt by ____\n",
            "predicted last token: the\n",
            "---------------------------------------------\n",
            "salomon will cover the warrants by buying sufficient shares or ____\n",
            "predicted last token: n\n",
            "---------------------------------------------\n",
            "in july southmark corp the dallas based real estate and financial ____\n",
            "predicted last token: services\n",
            "---------------------------------------------\n",
            "he concluded his remarks by and at some ____\n",
            "predicted last token: of\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "indexes = np.random.choice(len(test_token_ids), 10, replace=False)\n",
        "for i in indexes:\n",
        "    token_ids = test_token_ids[i][1:-1]\n",
        "    print(' '.join([tokenizer.inverse_vocab[token_id] for token_id in token_ids]) + ' ____')\n",
        "    pred = predict(bigram, token_ids[-1], vocab_size)\n",
        "    print(f'predicted last token: {tokenizer.inverse_vocab[pred]}')\n",
        "    print('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "629a551e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "629a551e",
        "outputId": "cff36d39-9e40-47c9-fb07-1841c9dfb5af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sharply falling stock prices do reduce consumer wealth damage business ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "but robert an official of the association said no ____\n",
            "predicted last token: longer\n",
            "---------------------------------------------\n",
            "it also has interests in military electronics and marine ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "first chicago since n has reduced its loans to such ____\n",
            "predicted last token: as\n",
            "---------------------------------------------\n",
            "david m jones vice president at g ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "the n stock specialist firms on the big board floor ____\n",
            "predicted last token: traders\n",
            "---------------------------------------------\n",
            "at the same time the business was hurt by ____\n",
            "predicted last token: the\n",
            "---------------------------------------------\n",
            "salomon will cover the warrants by buying sufficient shares or ____\n",
            "predicted last token: n\n",
            "---------------------------------------------\n",
            "in july southmark corp the dallas based real estate and financial ____\n",
            "predicted last token: officer\n",
            "---------------------------------------------\n",
            "he concluded his remarks by and at some ____\n",
            "predicted last token: of\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "indexes = np.random.choice(len(test_token_ids), 10, replace=False)\n",
        "for i in indexes:\n",
        "    token_ids = test_token_ids[i][1:-1]\n",
        "    print(' '.join([tokenizer.inverse_vocab[token_id] for token_id in token_ids]) + ' ____')\n",
        "    pred = predict(gt, token_ids[-1], vocab_size)\n",
        "    print(f'predicted last token: {tokenizer.inverse_vocab[pred]}')\n",
        "    print('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "c5f08173-30b8-40ac-95c7-5a4f0f722cc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5f08173-30b8-40ac-95c7-5a4f0f722cc9",
        "outputId": "44f3d448-9aaa-4748-b38e-12577834a531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sharply falling stock prices do reduce consumer wealth damage business ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "but robert an official of the association said no ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "it also has interests in military electronics and marine ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "first chicago since n has reduced its loans to such ____\n",
            "predicted last token: as\n",
            "---------------------------------------------\n",
            "david m jones vice president at g ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "the n stock specialist firms on the big board floor ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "at the same time the business was hurt by ____\n",
            "predicted last token: the\n",
            "---------------------------------------------\n",
            "salomon will cover the warrants by buying sufficient shares or ____\n",
            "predicted last token: n\n",
            "---------------------------------------------\n",
            "in july southmark corp the dallas based real estate and financial ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "he concluded his remarks by and at some ____\n",
            "predicted last token: of\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "indexes = np.random.choice(len(test_token_ids), 10, replace=False)\n",
        "for i in indexes:\n",
        "    token_ids = test_token_ids[i][1:-1]\n",
        "    print(' '.join([tokenizer.inverse_vocab[token_id] for token_id in token_ids]) + ' ____')\n",
        "    pred = predict(kn, token_ids[-1], vocab_size)\n",
        "    print(f'predicted last token: {tokenizer.inverse_vocab[pred]}')\n",
        "    print('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24d0a446",
      "metadata": {
        "id": "24d0a446"
      },
      "source": [
        "## Part B: 2. RNN (35 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f57f57",
      "metadata": {
        "id": "47f57f57"
      },
      "source": [
        "### 2.1 Split feature and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "5b0ab65b",
      "metadata": {
        "id": "5b0ab65b"
      },
      "outputs": [],
      "source": [
        "def get_feature_label(token_ids: List[List[int]], window_size: int=-1):\n",
        "    \"\"\" Split features and labels for the training, validation, and test datasets.\n",
        "    \n",
        "    Note:\n",
        "        If window size is -1, for a sentence with n tokens,\n",
        "            it selects the tokens rangeing from [0, n - 1) as the feature,\n",
        "            and selects tokens ranging from [1, n) as the label.\n",
        "        Otherwise, it divides a sentence with multiple windows and do the previous split.\n",
        "    \"\"\"\n",
        "    x = []\n",
        "    y = []\n",
        "    seq_lens = []\n",
        "    for sent_token_ids in token_ids:\n",
        "        if window_size == -1:\n",
        "            x.append(sent_token_ids[:-1])\n",
        "            y.append(sent_token_ids[1:])\n",
        "            seq_lens.append(len(sent_token_ids) - 1)\n",
        "        else:\n",
        "            if len(sent_token_ids) > window_size:\n",
        "                sub_sent_size = window_size + 1\n",
        "                n_window = len(sent_token_ids) // (sub_sent_size)\n",
        "                for i in range(n_window):\n",
        "                    start = i * sub_sent_size\n",
        "                    sub_sent = sent_token_ids[start:(start + sub_sent_size)]\n",
        "                    x.append(sub_sent[:-1])\n",
        "                    y.append(sub_sent[1:])\n",
        "                    seq_lens.append(len(sub_sent) - 1)\n",
        "                if len(sent_token_ids) % sub_sent_size > 0:\n",
        "                    sub_sent = sent_token_ids[-sub_sent_size:]\n",
        "                    x.append(sub_sent[:-1])\n",
        "                    y.append(sub_sent[1:])\n",
        "                    seq_lens.append(len(sub_sent) - 1)\n",
        "            else:\n",
        "                x.append(sent_token_ids[:-1])\n",
        "                y.append(sent_token_ids[1:])\n",
        "                seq_lens.append(len(sent_token_ids) - 1)\n",
        "    return x, y, seq_lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "a7a3c8bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7a3c8bb",
        "outputId": "19eeca4b-3eaa-4bbe-feaa-57fdda7d32cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40 68 68\n"
          ]
        }
      ],
      "source": [
        "window_size = 40\n",
        "x_train, y_train, train_seq_lens = get_feature_label(train_token_ids, window_size)\n",
        "x_valid, y_valid, valid_seq_lens = get_feature_label(valid_token_ids)\n",
        "x_test, y_test, test_seq_lens = get_feature_label(valid_token_ids)\n",
        "print(max(train_seq_lens), max(valid_seq_lens), max(test_seq_lens))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b147bfee",
      "metadata": {
        "id": "b147bfee"
      },
      "source": [
        "### 2.2 Pad sentences in a batch to equal length (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "21049fb3",
      "metadata": {
        "id": "21049fb3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def pad_batch(x_batch: List[List[int]], y_batch: List[List[int]], seq_lens_batch: List[int], pad_val: int):\n",
        "    \"\"\" Pad the sentences in a batch with pad_val based on the longest sentence.\n",
        "    \n",
        "    Args:\n",
        "        x_batch, y_batch, seq_lens_batch: the input data\n",
        "        pad_val: the padding value you need to fill to pad the sentences to the longest sentence.\n",
        "        \n",
        "    Return:\n",
        "        x_batch: Tensor, (batch_size x max_seq_len)\n",
        "        y_batch: Tensor, (batch_size x max_seq_len)\n",
        "        seq_lens_batch: Tensor, (batch_size, )\n",
        "    \"\"\"\n",
        "    max_len = max(seq_lens_batch)\n",
        "    # Start your code here\n",
        "    # Padding\n",
        "\n",
        "    x_batch = pad_sequences(x_batch, maxlen = max_len, value=pad_val)\n",
        "    y_batch = pad_sequences(y_batch, maxlen = max_len, value=pad_val) \n",
        "    \n",
        "    # End\n",
        "    x_batch, y_batch = tf.convert_to_tensor(x_batch, dtype=tf.int64), \\\n",
        "    tf.convert_to_tensor(y_batch, dtype=tf.int64)\n",
        "    seq_lens_batch = tf.convert_to_tensor(seq_lens_batch, dtype=tf.int64)\n",
        "    return x_batch, y_batch, seq_lens_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd8ad897",
      "metadata": {
        "id": "dd8ad897",
        "tags": []
      },
      "source": [
        "### 2.3 RNN language model (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "9f7fcff9",
      "metadata": {
        "id": "9f7fcff9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model\n",
        "\n",
        "\n",
        "class RNN(Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        \"\"\" Init of the RNN model\n",
        "        \n",
        "        Args:\n",
        "            vocab_size, embedding_dim: used for initialze the embedding layer.\n",
        "            hidden_units: number of hidden units of the RNN layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Start your code here\n",
        "        # self.embeddings =  # embedding layer of vocab_size x embedding_dim\n",
        "        # self.rnn =  # see tf.keras.layers.SimpleRNN, carefully read the parameters of it. Remember you need to predict the next word for every time step.\n",
        "        # self.classifier =  # a linear layer that maps the output of each rnn cell to the vocabulary space\n",
        "        \n",
        "        self.embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "        self.rnn = tf.keras.layers.SimpleRNN(units=hidden_units, return_sequences=True, stateful=False)\n",
        "        self.classifier = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # End\n",
        "        \n",
        "    def call(self, x):\n",
        "        \"\"\" Forward of the RNN model\n",
        "        \n",
        "        Args:\n",
        "            x: Tensor, (batch_size x max_seq_len). Input tokens. Here, max_seq_len is the longest length of sentences in this batch becasue we did pad_batch.\n",
        "        Return:\n",
        "            outputs: Tensor, (batch_size x max_seq_len x vocab_size). Logits for every time step. !!!NO SOFTMAX HERE!!!\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        x = self.embeddings(x)\n",
        "        x = self.rnn(x)\n",
        "        outputs = self.classifier(x)\n",
        "\n",
        "        # End\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "306c1ecb",
      "metadata": {
        "id": "306c1ecb",
        "tags": []
      },
      "source": [
        "### 2.4 Seq2seq loss (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "r_VLyQRg0cpa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_VLyQRg0cpa",
        "outputId": "6936bc9f-fc55-4a6e-c419-1412cfcf37af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "a4a493fb",
      "metadata": {
        "id": "a4a493fb"
      },
      "outputs": [],
      "source": [
        "from tensorflow_addons.seq2seq import sequence_loss\n",
        "\n",
        "\n",
        "def seq2seq_loss(logits, target, seq_lens):\n",
        "    \"\"\" Calculate the sequence to sequence loss using the sequence_loss from tensorflow\n",
        "    \n",
        "    Args:\n",
        "        logits: Tensor (batch_size x max_seq_len x vocab_size). The output of the RNN model.\n",
        "        target: Tensor (batch_size x max_seq_len). The groud-truth of words.\n",
        "        seq_lens: Tensor (batch_size, ). The real sequence length before padding.\n",
        "    \"\"\"\n",
        "    loss = 0\n",
        "    # Start your code here\n",
        "    # 1. make a sequence mask (batch_size x max_seq_len) using tf.sequence_mask. This is to build a mask with 1 and 0.\n",
        "    #    Entry with 1 is the valid time step without padding. Entry with 0 is the time step with padding. We need to exclude this time step.\n",
        "    # 2. calculate the loss with sequence_loss. Carefully read the documentation of each parameter\n",
        "    # print(seq_lens)\n",
        "    weights = tf.sequence_mask(seq_lens, dtype=tf.dtypes.float32)\n",
        "    loss = sequence_loss(logits, target, weights,average_across_timesteps=True)\n",
        "\n",
        "    # End\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "8d3ed654",
      "metadata": {
        "id": "8d3ed654"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer)\n",
        "hidden_units = 128\n",
        "embedding_dim = 64\n",
        "num_epoch = 30\n",
        "batch_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "2827abe7",
      "metadata": {
        "id": "2827abe7"
      },
      "outputs": [],
      "source": [
        "model = RNN(vocab_size, embedding_dim, hidden_units)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f23a792",
      "metadata": {
        "id": "7f23a792",
        "tags": []
      },
      "source": [
        "### 2.5 Train RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d714e6a7",
      "metadata": {
        "id": "d714e6a7"
      },
      "source": [
        "If you implement everything correctly, the finall loss will be around 5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "d08344fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d08344fb",
        "outputId": "3c26a08f-ddfc-4404-9454-a412885a4d18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 / 30 - Step 170 / 170 - train loss: 4.4607 - valid loss: 1.6819\n",
            "Epoch 2 / 30 - Step 170 / 170 - train loss: 3.5625 - valid loss: 1.1880\n",
            "Epoch 3 / 30 - Step 170 / 170 - train loss: 3.0027 - valid loss: 1.0571\n",
            "Epoch 4 / 30 - Step 170 / 170 - train loss: 2.9237 - valid loss: 1.0323\n",
            "Epoch 5 / 30 - Step 170 / 170 - train loss: 2.8637 - valid loss: 1.0155\n",
            "Epoch 6 / 30 - Step 170 / 170 - train loss: 2.8065 - valid loss: 0.9971\n",
            "Epoch 7 / 30 - Step 170 / 170 - train loss: 2.7382 - valid loss: 0.9782\n",
            "Epoch 8 / 30 - Step 170 / 170 - train loss: 2.6807 - valid loss: 0.9629\n",
            "Epoch 9 / 30 - Step 170 / 170 - train loss: 2.6315 - valid loss: 0.9485\n",
            "Epoch 10 / 30 - Step 170 / 170 - train loss: 2.5844 - valid loss: 0.9359\n",
            "Epoch 11 / 30 - Step 170 / 170 - train loss: 2.5429 - valid loss: 0.9259\n",
            "Epoch 12 / 30 - Step 170 / 170 - train loss: 2.5060 - valid loss: 0.9177\n",
            "Epoch 13 / 30 - Step 170 / 170 - train loss: 2.4724 - valid loss: 0.9108\n",
            "Epoch 14 / 30 - Step 170 / 170 - train loss: 2.4412 - valid loss: 0.9048\n",
            "Epoch 15 / 30 - Step 170 / 170 - train loss: 2.4119 - valid loss: 0.8995\n",
            "Epoch 16 / 30 - Step 170 / 170 - train loss: 2.3841 - valid loss: 0.8947\n",
            "Epoch 17 / 30 - Step 170 / 170 - train loss: 2.3576 - valid loss: 0.8901\n",
            "Epoch 18 / 30 - Step 170 / 170 - train loss: 2.3324 - valid loss: 0.8867\n",
            "Epoch 19 / 30 - Step 170 / 170 - train loss: 2.3059 - valid loss: 0.8824\n",
            "Epoch 20 / 30 - Step 170 / 170 - train loss: 2.2799 - valid loss: 0.8787\n",
            "Epoch 21 / 30 - Step 170 / 170 - train loss: 2.2549 - valid loss: 0.8757\n",
            "Epoch 22 / 30 - Step 170 / 170 - train loss: 2.2307 - valid loss: 0.8733\n",
            "Epoch 23 / 30 - Step 170 / 170 - train loss: 2.2074 - valid loss: 0.8713\n",
            "Epoch 24 / 30 - Step 170 / 170 - train loss: 2.1849 - valid loss: 0.8696\n",
            "Epoch 25 / 30 - Step 170 / 170 - train loss: 2.1631 - valid loss: 0.8684\n",
            "Epoch 26 / 30 - Step 170 / 170 - train loss: 2.1416 - valid loss: 0.8676\n",
            "Epoch 27 / 30 - Step 170 / 170 - train loss: 2.1216 - valid loss: 0.8675\n",
            "Epoch 28 / 30 - Step 170 / 170 - train loss: 2.1043 - valid loss: 0.8681\n",
            "Epoch 29 / 30 - Step 170 / 170 - train loss: 2.0868 - valid loss: 0.8681\n",
            "Epoch 30 / 30 - Step 170 / 170 - train loss: 2.0688 - valid loss: 0.8681\n"
          ]
        }
      ],
      "source": [
        "num_samples = len(x_train)\n",
        "n_batch = int(np.ceil(num_samples / batch_size))\n",
        "n_valid_batch = int(np.ceil(len(x_valid) / batch_size))\n",
        "for epoch in range(num_epoch):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_idx in range(n_batch):\n",
        "        start = batch_idx * batch_size\n",
        "        end = start + batch_size\n",
        "        x_batch, y_batch, seq_lens_batch = x_train[start:end], y_train[start:end], train_seq_lens[start:end]\n",
        "        real_batch_size = len(x_batch)\n",
        "        x_batch, y_batch, seq_lens_batch = pad_batch(x_batch, y_batch, seq_lens_batch, pad_val=tokenizer.pad_token_id)\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(x_batch)\n",
        "            loss = seq2seq_loss(output, y_batch, seq_lens_batch)\n",
        "\n",
        "        if batch_idx % 1 == 0 or batch_idx == num_samples - 1:\n",
        "            print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {n_batch} - loss: {loss:.4f}')\n",
        "            \n",
        "        trainable_vars = model.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        epoch_loss += loss * real_batch_size\n",
        "    \n",
        "    valid_loss = 0.0\n",
        "    for batch_idx in range(n_valid_batch):\n",
        "        start = batch_idx * batch_size\n",
        "        end = start + batch_size\n",
        "        x_batch, y_batch, seq_lens_batch = x_valid[start:end], y_valid[start:end], valid_seq_lens[start:end]\n",
        "        real_batch_size = len(x_batch)\n",
        "        x_batch, y_batch, seq_lens_batch = pad_batch(x_batch, y_batch, seq_lens_batch, pad_val=tokenizer.pad_token_id)\n",
        "        output = model(x_batch)\n",
        "        loss = seq2seq_loss(output, y_batch, seq_lens_batch)\n",
        "\n",
        "        if batch_idx % 1 == 0 or batch_idx == len(x_valid) - 1:\n",
        "            print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {n_valid_batch} - loss: {loss:.4f}')\n",
        "\n",
        "        valid_loss += loss * real_batch_size\n",
        "    print(f'\\rEpoch {epoch + 1} / {num_epoch} - Step {n_batch} / {n_batch} - train loss: {epoch_loss / num_samples:.4f} - valid loss: {valid_loss / len(x_valid):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8de2350",
      "metadata": {
        "id": "a8de2350",
        "tags": []
      },
      "source": [
        "### 2.6 Perplexity of RNN (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20763d8f",
      "metadata": {
        "id": "20763d8f"
      },
      "source": [
        "Here,\n",
        "1. you need to calculate the perplexity based on its definition.\n",
        "2. Besides, you need to record the loss for every word prediction and calculate the sum of loss\n",
        "3. Finaly, you will need to compare the perplexity by definition and the perplexity by the loss: `np.exp(total_loss / n_words)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "d1c38a82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1c38a82",
        "outputId": "d7aee63c-b15f-4f51-cef9-b89e2f4cd846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating perplexity: 3352 / 33521\n",
            "\n",
            "\n",
            "Perplexity by definition: 309.6566, Perplexity by loss: 309.6566\n"
          ]
        }
      ],
      "source": [
        "n = len(x_valid)\n",
        "log_probs = 0\n",
        "n_words = 0  # number of words to predict in the entire dataset\n",
        "total_loss = 0  # total loss of each word's loss\n",
        "for i in range(n):\n",
        "    if i % 1 == 0 or i == n - 1:\n",
        "        print_line('Calculating perplexity:', (i + 1), '/', n)\n",
        "    x_line, y_line, line_seq_lens = x_valid[i:i + 1], y_valid[i: i + 1], valid_seq_lens[i:i + 1]\n",
        "    x_line, y_line, line_seq_lens = pad_batch(x_line, y_line, line_seq_lens, tokenizer.pad_token_id)\n",
        "    output = model(x_line)\n",
        "    pred_probs = tf.nn.softmax(output, axis=-1)\n",
        "\n",
        "    for real_token, probs in zip(y_line[0], pred_probs[0]):\n",
        "        # Start your code here\n",
        "        # 1. retrieve the probability of the real token from probs\n",
        "        # 2. update the log probabiltiy\n",
        "        # 3. update number of words predicted (n_words, i.e., N in our slides)\n",
        "\n",
        "        probability = tf.gather(probs, real_token)\n",
        "        log_probs += math.log2(probability)\n",
        "        n_words+=1\n",
        "        # End\n",
        "\n",
        "    loss = 0\n",
        "    # Start your code here\n",
        "    # 1. Calculate the seq2seq loss for this sentence using output, y_line, and line_seq_lens. This is the average loss over time step\n",
        "    # 2. Calculate the sum loss for all time steps.\n",
        "    # 3. Update total loss, do not forget to use loss.numpy() to convert the loss tensor to numpy scalar value.\n",
        "    \n",
        "    line_seq_lens_mask = tf.sequence_mask(line_seq_lens, dtype=tf.dtypes.float32)\n",
        "    loss = sequence_loss(output, y_line, line_seq_lens_mask, average_across_timesteps= True)\n",
        "    loss *= output.shape[1]\n",
        "    total_loss += loss.numpy()\n",
        "\n",
        "    # End\n",
        "print(len(pred_probs))\n",
        "print('\\n')\n",
        "perplexity = 2 ** ((-1 / n_words) * log_probs)\n",
        "print(f'Perplexity by definition: {perplexity:.4f}, Perplexity by loss: {np.exp(total_loss / n_words):.4f}')\n",
        "\n",
        "# If you implement correctly, the two perplexity will be almost the same."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a43f930",
      "metadata": {
        "id": "6a43f930"
      },
      "source": [
        "### 2.7 Predict the next word given a previous sentence (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "0d70de99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d70de99",
        "outputId": "a07be4ca-d8f7-454a-fca6-b89a7b4b312b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sharply falling stock prices do reduce consumer wealth damage business ____\n",
            "predicted last token: to\n",
            "---------------------------------------------\n",
            "but robert an official of the association said no ____\n",
            "predicted last token: n\n",
            "---------------------------------------------\n",
            "it also has interests in military electronics and marine ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "first chicago since n has reduced its loans to such ____\n",
            "predicted last token: a\n",
            "---------------------------------------------\n",
            "david m jones vice president at g ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "the n stock specialist firms on the big board floor ____\n",
            "predicted last token: is\n",
            "---------------------------------------------\n",
            "at the same time the business was hurt by ____\n",
            "predicted last token: the\n",
            "---------------------------------------------\n",
            "salomon will cover the warrants by buying sufficient shares or ____\n",
            "predicted last token: n\n",
            "---------------------------------------------\n",
            "in july southmark corp the dallas based real estate and financial ____\n",
            "predicted last token: services\n",
            "---------------------------------------------\n",
            "he concluded his remarks by and at some ____\n",
            "predicted last token: of\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "indexes = np.random.choice(len(test_token_ids), 10, replace=False)\n",
        "for i in indexes:\n",
        "    token_ids = test_token_ids[i][1:-1]\n",
        "    print(' '.join([tokenizer.inverse_vocab[token_id] for token_id in token_ids]) + ' ____')   \n",
        "    x = tf.convert_to_tensor(token_ids, dtype=tf.int64)  # now x is a tensor of (seq_len, )\n",
        "    # Start your code here\n",
        "    # 1. reshape x as a batch with size 1\n",
        "    # 2. calculate the output logits\n",
        "    # 3. get the prediction with the largest logits\n",
        "    # 4. select the last time step as the final prediction\n",
        "    x = tf.reshape(x, [1,x.shape[0]])\n",
        "    output = model(x)\n",
        "    pred = tf.math.argmax(output[0], axis=1)\n",
        "    pred = pred[-1].numpy()\n",
        "    # End\n",
        "    \n",
        "    print(f'predicted last token: {tokenizer.inverse_vocab[pred]}')\n",
        "    print('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "632e0d85",
      "metadata": {
        "id": "632e0d85"
      },
      "source": [
        "## 3. Conclusion (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18809ead",
      "metadata": {
        "id": "18809ead"
      },
      "source": [
        "**Briefly analyze the result of N-Gram and RNN**\n",
        "\n",
        "- The choice of N-Gram model is a bigram model that considers 2 words for making predictions\n",
        "- Performance of both bigram and RNN model are comparable\n",
        "- Both models are evaluated based on the preplexity of the model, an intrinsic evaluation metric.\n",
        "- Additionally we are ananlysing the loss for RNN model\n",
        "- We can see that the preplexity scores for bigram model without smooting and preplexity for RNN model are comparable.\n",
        "- Preplexity score for bigram model without any smoothing technique, somethinf the deals with the unseen data in the training set, is 325.8354 and that of RNN is 309.6566\n",
        "- Another noteworthy point here is that the preplexity score for RNN model based on both probabailty and loss is the same (309.6566)\n",
        "- Bigram models with Kneser-Ney performs better with perplexity score 62.5908 and can be ranked before bigram model with good smoothing with preplexity score 130.5334\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "account_details",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, May 24 2022, 21:28:31) \n[Clang 13.1.6 (clang-1316.0.21.2)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "318b20e5e552330ac64402a63c1a3d538eebc51f858fc08a88a1f9c9a530966c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
