{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a7d7050b-a87a-4954-adf9-afb9cf6d058a",
      "metadata": {
        "id": "a7d7050b-a87a-4954-adf9-afb9cf6d058a"
      },
      "source": [
        "# CS 584 Assignment 2 -- MLP and Word Vectors\n",
        "\n",
        "#### Name: Archana Kalburgi\n",
        "#### Stevens ID: 10469491"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe7ac70-1da0-4432-839d-6d65ce4edf35",
      "metadata": {
        "id": "bfe7ac70-1da0-4432-839d-6d65ce4edf35",
        "tags": []
      },
      "source": [
        "## Part A: Multi-Layer Perceptron (MLP) (50 Points)\n",
        "\n",
        "## In this assignment, you are required to follow the steps below:\n",
        "1. Review the lecture slides.\n",
        "2. Implement the data loading, preprocessing, tokenization, and TF-IDF feature extraction.\n",
        "3. Implement MLP model, evaluation metrics, and Mini-batch GD with AdaGrad.\n",
        "4. Implement the MLP with Tensorflow and compare to your implementation.\n",
        "5. Analysis the results in the Conlusion part.\n",
        "\n",
        "**Before you start**\n",
        "- Please read the code very carefully.\n",
        "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons, pandas) using the following command.\n",
        "```console\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
        "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
        "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b95a75c8-30c8-4ca6-ac9d-42c90d32b04d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "b95a75c8-30c8-4ca6-ac9d-42c90d32b04d",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "5abad0de-c153-49e0-b19e-ec9b6be49ee6",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jupyterlab\n",
            "  Downloading jupyterlab-3.4.8-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 4)) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (2.8.2+zzzcolab20220929150707)\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 49.2 MB/s \n",
            "\u001b[?25hCollecting nbclassic\n",
            "  Downloading nbclassic-0.4.5-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2.0.1)\n",
            "Collecting jupyter-server~=1.16\n",
            "  Downloading jupyter_server-1.19.1-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 71.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (21.3)\n",
            "Collecting jupyterlab-server~=2.10\n",
            "  Downloading jupyterlab_server-2.15.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: notebook<7 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (5.3.1)\n",
            "Collecting tornado>=6.1.0\n",
            "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
            "\u001b[K     |████████████████████████████████| 423 kB 67.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2.11.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (7.9.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (4.11.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.1->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2.0.1)\n",
            "Collecting nbconvert>=6.4.4\n",
            "  Downloading nbconvert-7.2.1-py3-none-any.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 74.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (1.8.0)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.4.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (6.1.12)\n",
            "Collecting prometheus-client\n",
            "  Downloading prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (5.6.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (5.1.1)\n",
            "Collecting anyio<4,>=3.1.0\n",
            "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (0.13.3)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (23.2.1)\n",
            "Collecting argon2-cffi\n",
            "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2.10)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (5.0.0)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2.10.3)\n",
            "Collecting json5\n",
            "  Downloading json5-0.9.10-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (4.3.3)\n",
            "Collecting jinja2>=2.1\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 76.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (5.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (1.5.0)\n",
            "Collecting jupyterlab-pygments\n",
            "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
            "Collecting nbclient>=0.5.0\n",
            "  Downloading nbclient-0.7.0-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 257 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2.6.1)\n",
            "Collecting tinycss2\n",
            "  Downloading tinycss2-1.1.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (5.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (4.6.3)\n",
            "Collecting mistune<3,>=2.0.3\n",
            "  Downloading mistune-2.0.4-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (0.7.1)\n",
            "Collecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting traitlets>=5.1\n",
            "  Downloading traitlets-5.4.0-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 72.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.2.0->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2.16.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook<7->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (5.3.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook<7->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.3->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 2)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 3)) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 3)) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 5)) (1.7.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (0.27.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (1.49.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (22.9.24)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (14.0.6)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (2.7.1)\n",
            "Collecting argon2-cffi-bindings\n",
            "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2.21)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->jupyterlab-server~=2.10->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2022.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (2.0.10)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (4.4.2)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1)) (0.2.5)\n",
            "Collecting notebook-shim>=0.1.0\n",
            "  Downloading notebook_shim-0.1.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: traitlets, tornado, nest-asyncio, tinycss2, sniffio, nbclient, mistune, jupyterlab-pygments, jinja2, argon2-cffi-bindings, websocket-client, prometheus-client, nbconvert, jedi, argon2-cffi, anyio, jupyter-server, notebook-shim, json5, nbclassic, jupyterlab-server, tensorflow-addons, jupyterlab\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: mistune\n",
            "    Found existing installation: mistune 0.8.4\n",
            "    Uninstalling mistune-0.8.4:\n",
            "      Successfully uninstalled mistune-0.8.4\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 5.6.1\n",
            "    Uninstalling nbconvert-5.6.1:\n",
            "      Successfully uninstalled nbconvert-5.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.2 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\n",
            "Successfully installed anyio-3.6.1 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 jedi-0.18.1 jinja2-3.1.2 json5-0.9.10 jupyter-server-1.19.1 jupyterlab-3.4.8 jupyterlab-pygments-0.2.2 jupyterlab-server-2.15.2 mistune-2.0.4 nbclassic-0.4.5 nbclient-0.7.0 nbconvert-7.2.1 nest-asyncio-1.5.6 notebook-shim-0.1.0 prometheus-client-0.14.1 sniffio-1.3.0 tensorflow-addons-0.18.0 tinycss2-1.1.1 tornado-6.2 traitlets-5.4.0 websocket-client-1.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# you may not run this cell after the first installation\n",
        "!pip install -r \"/content/drive/MyDrive/Colab Notebooks/requirements.txt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDIGvi60k8xB",
        "outputId": "2a4cf17d-47c8-4d0d-fd7c-d42830975944"
      },
      "id": "UDIGvi60k8xB",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "91d76782-187b-4da7-baf2-d92655c79d63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91d76782-187b-4da7-baf2-d92655c79d63",
        "outputId": "45fe42f2-3bf2-4706-ef7f-263234f17e4f",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63231e67-b83e-4628-9c49-31b6d096b4f7",
      "metadata": {
        "id": "63231e67-b83e-4628-9c49-31b6d096b4f7",
        "tags": []
      },
      "source": [
        "## 1. Data Processing (5 points)\n",
        "\n",
        "* Download the dataset from Canvas\n",
        "* Load data to text and labels\n",
        "* Preprocessing\n",
        "* Tokenization\n",
        "* Split data\n",
        "* Feature extraction (TF-IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d184b3d1-7b4b-4ee0-85b9-12a205f58687",
      "metadata": {
        "id": "d184b3d1-7b4b-4ee0-85b9-12a205f58687"
      },
      "source": [
        "#### Download NLTK stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "636b2ba9-1e79-4479-b1d8-28e139590cf2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "636b2ba9-1e79-4479-b1d8-28e139590cf2",
        "outputId": "7ea06079-d3ac-4e6c-e0ba-3b015133a566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to a2-data/nltk...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk_path = os.path.join('a2-data', 'nltk')\n",
        "nltk.download('stopwords', download_dir=nltk_path)\n",
        "nltk.data.path.append(nltk_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a98cc310-65bb-49dc-bd54-476f4e941f7d",
      "metadata": {
        "id": "a98cc310-65bb-49dc-bd54-476f4e941f7d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "def print_line(*args):\n",
        "    \"\"\" Inline print and go to the begining of line\n",
        "    \"\"\"\n",
        "    args1 = [str(arg) for arg in args]\n",
        "    str_ = ' '.join(args1)\n",
        "    sys.stdout.write(str_ + '\\r')\n",
        "    sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "653997a8-637a-4725-8ee5-f240913e0644",
      "metadata": {
        "id": "653997a8-637a-4725-8ee5-f240913e0644"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Union\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9eb08e4-91fa-4d1e-bc92-0445397567ca",
      "metadata": {
        "id": "b9eb08e4-91fa-4d1e-bc92-0445397567ca"
      },
      "source": [
        "### 1.1 Load data\n",
        "\n",
        "- Load sentences and labels\n",
        "- Transform string labels into integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9fc26198-cfbf-4c2f-89e6-460f5277c177",
      "metadata": {
        "id": "9fc26198-cfbf-4c2f-89e6-460f5277c177"
      },
      "outputs": [],
      "source": [
        "def load_sentence_label(data_path: str) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\" Load sentences and labels from the specified path\n",
        "    Args:\n",
        "        data_path: data_path: path to the data file, e.g., 'a1-data/SMSSpamCollection'\n",
        "        sentences: the raw text list of all sentences\n",
        "    Returns:\n",
        "        labels: the label list of all sentences\n",
        "    \"\"\"\n",
        "    sentences, labels = [], []\n",
        "    # Start your code here (load text and label from files)\n",
        "    with open(data_path) as f:\n",
        "        lines = f.readlines()\n",
        "        \n",
        "    mapper = map(lambda x: x.split(\"\\t\"), lines)\n",
        "    \n",
        "    for item in mapper:\n",
        "        labels.append(item[0])\n",
        "        sentences.append(item[1])\n",
        "    \n",
        "    # End\n",
        "    return sentences, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "103a0975-aaa9-48ac-9a2b-445757ef614b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "103a0975-aaa9-48ac-9a2b-445757ef614b",
        "outputId": "87c64cb6-4d7d-4bc1-bb5c-bfcf8854c3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label map: {'Arthur Conan Doyle': 0, 'Fyodor Dostoyevsky': 1, 'Jane Austen': 2}\n",
            "Number of sentences and labels: 19536 19536\n"
          ]
        }
      ],
      "source": [
        "data_path = os.path.join('/content/drive/MyDrive/Colab Notebooks/a2-data', 'books.txt')\n",
        "sentences, labels = load_sentence_label(data_path)\n",
        "\n",
        "label_map = {}\n",
        "for label in sorted(list(set(labels))):\n",
        "    label_map[label] = len(label_map)\n",
        "labels = np.array([label_map[label] for label in labels], dtype=int)\n",
        "sentences = np.array(sentences, dtype=object)\n",
        "\n",
        "print('Label map:', label_map)\n",
        "print('Number of sentences and labels:', len(sentences), len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20468e5-9fb9-48ec-ae47-92b463549853",
      "metadata": {
        "id": "b20468e5-9fb9-48ec-ae47-92b463549853"
      },
      "source": [
        "#### Split the data into training, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dba74813-eb7f-484c-bc9a-0f391ae48a09",
      "metadata": {
        "id": "dba74813-eb7f-484c-bc9a-0f391ae48a09"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def train_test_split(sentences: np.ndarray,\n",
        "                     labels: np.ndarray,\n",
        "                     test_ratio: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\" Split the sentences and labels into training and test data by shuffling\n",
        "    Args:\n",
        "        sentences: A numpy array containing all sentences\n",
        "        labels: A number array containing label ids\n",
        "        test_ratio: A float number to calculate the number of test data\n",
        "\n",
        "    Returns:\n",
        "        train_sentences: A numpy array containing all training sentences\n",
        "        train_labels: A number array containing all training label ids\n",
        "        test_sentences: A numpy array containing all test sentences\n",
        "        test_labels: A number array containing all test label ids\n",
        "    \"\"\"\n",
        "    assert 0 < test_ratio < 1\n",
        "    assert len(sentences) == len(labels)\n",
        "\n",
        "    train_index, test_index = [], []\n",
        "    \n",
        "    # Start your code here (split the index for training and test)\n",
        "    \n",
        "    end = math.ceil((1-test_ratio)*len(labels))\n",
        "    train_index = [i for i in range(end)]\n",
        "    test_index = [i for i in range(len(labels)-end)]\n",
        "    np.random.shuffle(train_index)\n",
        "    np.random.shuffle(train_index)\n",
        "\n",
        "    # End\n",
        "\n",
        "    train_sentences, train_labels = sentences[train_index], labels[train_index]\n",
        "    test_sentences, test_labels = sentences[test_index], labels[test_index]\n",
        "    return train_sentences, train_labels, test_sentences, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1757acef-5872-4683-86da-c4ae68af7f18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1757acef-5872-4683-86da-c4ae68af7f18",
        "outputId": "0442820a-7c44-4d5d-d78d-a1993342b2b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length: 14067\n",
            "Validation data length: 1562\n",
            "Test data length: 3907\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(6666)\n",
        "\n",
        "test_ratio = 0.2\n",
        "valid_ratio = 0.1\n",
        "(train_sentences, train_labels,\n",
        "    test_sentences, test_labels) = train_test_split(sentences, labels, test_ratio)\n",
        "(train_sentences, train_labels,\n",
        "    valid_sentences, valid_labels) = train_test_split(train_sentences, train_labels, valid_ratio)\n",
        "\n",
        "print('Training data length:', len(train_sentences))\n",
        "print('Validation data length:', len(valid_sentences))\n",
        "print('Test data length:', len(test_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f1202227-1197-4c6c-b1c4-525bcbb65ea9",
      "metadata": {
        "id": "f1202227-1197-4c6c-b1c4-525bcbb65ea9"
      },
      "outputs": [],
      "source": [
        "def count_label(labels: np.ndarray, label_map):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        labels: The labels of a dataset \n",
        "        label_map: The mapping from label to label id\n",
        "    Returns:\n",
        "        label_count: The mapping from label to its count\n",
        "    \"\"\"\n",
        "    label_count = {key: 0 for key in label_map.keys()}\n",
        "    # Start your code here (count the number of each label)\n",
        "    label_count = {key: 0 for key in label_map.keys()}\n",
        "    # Start your code here (count the number of each label)\n",
        "    arthur_conan_doyle = len(list(filter(lambda x : x==0, labels)))\n",
        "    fyodor_dostoyevsky = len(list(filter(lambda x : x==1, labels)))\n",
        "    jane_austen = len(list(filter(lambda x : x==2, labels)))\n",
        "    label_count['Arthur Conan Doyle']=arthur_conan_doyle\n",
        "    label_count['Fyodor Dostoyevsky']=fyodor_dostoyevsky\n",
        "    label_count['Jane Austen']=jane_austen\n",
        "    # End\n",
        "    return label_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e7744c12-8055-478a-b64b-67104fd81c92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7744c12-8055-478a-b64b-67104fd81c92",
        "outputId": "6faadae1-36c6-48c1-8f8c-ab6e3b1e1269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: {'Arthur Conan Doyle': 2281, 'Fyodor Dostoyevsky': 1846, 'Jane Austen': 9940}\n",
            "Validation: {'Arthur Conan Doyle': 262, 'Fyodor Dostoyevsky': 198, 'Jane Austen': 1102}\n",
            "Test: {'Arthur Conan Doyle': 0, 'Fyodor Dostoyevsky': 0, 'Jane Austen': 3907}\n"
          ]
        }
      ],
      "source": [
        "print('Training:', count_label(train_labels, label_map))\n",
        "print('Validation:', count_label(valid_labels, label_map))\n",
        "print('Test:', count_label(test_labels, label_map))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47476cac-f0da-4b92-aa09-8f57e61053e9",
      "metadata": {
        "id": "47476cac-f0da-4b92-aa09-8f57e61053e9"
      },
      "source": [
        "#### Dataset statistics\n",
        "Fill this table with the statistics you just printed (double click this cell to edit)\n",
        "\n",
        "|                | Arthur Conan Doyle | Fyodor Dostoyevsky | Jane Austen | Total |\n",
        "|:--------------:|--------------------|--------------------|-------------|-------|\n",
        "|  **Training**  |      2281          |      1846          |    9940     | 14067 |\n",
        "| **Validation** |      262           |      198           |    1102     | 1562  |\n",
        "|    **Test**    |      0             |       0            |    3907     | 3907  |\n",
        "|    **Total**   |      2543          |      2044          |    14949    | 19536 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8438e6-64a6-4355-b39f-88bdf3550593",
      "metadata": {
        "id": "ff8438e6-64a6-4355-b39f-88bdf3550593"
      },
      "source": [
        "### 1.2 Preprocess\n",
        "In this section, you need to remove all the unrelated characters, including punctuation, urls, and numbers. Please fill up the functions and test them by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1007c276-5692-4f3b-afd8-d2be06f6ecfd",
      "metadata": {
        "id": "1007c276-5692-4f3b-afd8-d2be06f6ecfd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, punctuation=True, url=True, number=True):\n",
        "        self.punctuation = punctuation\n",
        "        self.url = url\n",
        "        self.number = number\n",
        "\n",
        "    def apply(self, sentence: str) -> str:\n",
        "        \"\"\" Apply the preprocessing rules to the sentence\n",
        "        Args:\n",
        "            sentence: raw sentence\n",
        "        Returns:\n",
        "            sentence: clean sentence\n",
        "        \"\"\"\n",
        "        sentence = sentence.lower()\n",
        "        if self.url:\n",
        "            sentence = Preprocessor.remove_url(sentence)\n",
        "        if self.punctuation:\n",
        "            sentence = Preprocessor.remove_punctuation(sentence)\n",
        "        if self.number:\n",
        "            sentence = Preprocessor.remove_number(sentence)\n",
        "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_punctuation(sentence: str) -> str:\n",
        "        \"\"\" Remove punctuations in sentence with re\n",
        "        Args:\n",
        "            sentence: sentence with possible punctuations\n",
        "        Returns:\n",
        "            sentence: sentence without punctuations\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        sentence = sentence.translate(str.maketrans('','', string.punctuation))   \n",
        "\n",
        "        # End\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_url(sentence: str) -> str:\n",
        "        \"\"\" Remove urls in text with re\n",
        "        Args:\n",
        "            sentence: sentence with possible urls\n",
        "        Returns:\n",
        "            sentence: sentence without urls\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        pattern = re.compile(\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\")\n",
        "        sentence = pattern.sub('', sentence)\n",
        "        # End\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_number(sentence: str) -> str:\n",
        "        \"\"\" Remove numbers in sentence with re\n",
        "        Args:\n",
        "            sentence: sentence with possible numbers\n",
        "        Returns:\n",
        "            sentence: sentence without numbers\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        sentence = sentence.translate(str.maketrans('','',string.digits))\n",
        "        # End\n",
        "        return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c56c4e51-e434-4703-a23e-bb609c0d3b51",
      "metadata": {
        "id": "c56c4e51-e434-4703-a23e-bb609c0d3b51"
      },
      "source": [
        "##### Test your implementation by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "bbf21295-1e18-46ab-b9fe-07fba8d9e4c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbf21295-1e18-46ab-b9fe-07fba8d9e4c0",
        "outputId": "cee94f8a-af2c-4a00-ba04-c22132da9571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
            "===>\n",
            "\"interest rates are trimmed to by the south african central bank but the lack of warning hits the rand and surprises markets\"\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
        "\n",
        "processor = Preprocessor()\n",
        "clean_sentence = processor.apply(sentence)\n",
        "\n",
        "print(f'\"{sentence}\"') \n",
        "print('===>')\n",
        "print(f'\"{clean_sentence}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e173a8d-8f57-40e6-b278-052290ab9ffe",
      "metadata": {
        "id": "9e173a8d-8f57-40e6-b278-052290ab9ffe"
      },
      "source": [
        "### 1.3 Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "facf2964-b766-4a2a-8709-ac831c041b30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "facf2964-b766-4a2a-8709-ac831c041b30",
        "outputId": "cccc3ac5-e7f7-41c9-c919-b26737b95722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['other', 've', 'to', 'hers', 'hasn', 'weren', 'are', 'against', 'been', 'all']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "stopwords_set = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "print(list(stopwords_set)[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e3fc31ba-ee84-4268-9f7c-f15fe7169151",
      "metadata": {
        "id": "e3fc31ba-ee84-4268-9f7c-f15fe7169151"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence: str) -> List[str]:\n",
        "    \"\"\" Tokenize a sentence into tokens (words)\n",
        "    Args:\n",
        "        sentence: clean sentence\n",
        "    Returns:\n",
        "        tokens\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    # Start your code here\n",
        "    #     Step 1. Split sentence into words\n",
        "    #     Step 2. Extract word stem using the defined stemmer (PorterStemmer) by calling stemmer.stem(word)\n",
        "    #     Step 3. Remove stop words using the defined stopwords_set\n",
        "    \n",
        "    tokens = word_tokenize(sentence)\n",
        "    tokens = [token for token in tokens if token not in stopwords_set]  \n",
        "    words = [stemmer.stem(token) for token in tokens]\n",
        "    \n",
        "    # End\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bfc568d-4343-42c5-9c5b-c4e4627ebae6",
      "metadata": {
        "id": "9bfc568d-4343-42c5-9c5b-c4e4627ebae6"
      },
      "source": [
        "##### Test your implementation by running the following block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f1ea9a53-a8f5-4f46-b30e-5116714adcf0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1ea9a53-a8f5-4f46-b30e-5116714adcf0",
        "outputId": "f4dbc266-9def-47ad-b598-f703f34d4512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
            "===>\n",
            "\"['interest', 'rate', 'trim', 'south', 'african', 'central', 'bank', 'lack', 'warn', 'hit', 'rand', 'surpris', 'market']\"\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
        "\n",
        "processor = Preprocessor()\n",
        "clean_sentence = processor.apply(sentence)\n",
        "tokens = tokenize(clean_sentence)\n",
        "\n",
        "print(f'\"{sentence}\"') \n",
        "print('===>')\n",
        "print(f'\"{tokens}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ccea260-2e4c-45e6-8d07-953452fd8c4b",
      "metadata": {
        "id": "7ccea260-2e4c-45e6-8d07-953452fd8c4b"
      },
      "source": [
        "### 1.5 Feature Extraction\n",
        "\n",
        "TF-IDF:\n",
        "$$\\text{TF-IDF}(t, d) = \\frac{f_{t, d}}{\\sum_{t'}{f_{t', d}}} \\times \\log{\\frac{N}{n_t}}$$\n",
        "\n",
        "- $t$: A term\n",
        "- $d$: A document. Here, we regard a sentence as a document\n",
        "- $f_{t, d}$: Number of term $t$ in $d$\n",
        "- $N$: Number of document\n",
        "- $n_t$: Number of document containing $t$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fc82e419-05d8-4919-8b97-194b2933171a",
      "metadata": {
        "id": "fc82e419-05d8-4919-8b97-194b2933171a"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict,Counter\n",
        "\n",
        "\n",
        "class TfIdfEncoder:\n",
        "    def __init__(self):\n",
        "        self.vocab = defaultdict(int)\n",
        "        self.token2index = {}\n",
        "        self.df = defaultdict(int)\n",
        "        self.num_doc = 0\n",
        "        self.processor = Preprocessor()\n",
        "\n",
        "    def fit(self, sentences: Union[List[str], np.ndarray]) -> int:\n",
        "        \"\"\" Using the given texts to store key information in TF-IDF calculation\n",
        "            In this function, you are required to implement the fitting process.\n",
        "                1. Construct the vocabulary and store the frequency of tokens (self.vocab).\n",
        "                2. Construct the document frequency map to tokens (self.df).\n",
        "                3. Construct the token to index map based on the frequency.\n",
        "                   The token with a higher frequency has the smaller index\n",
        "        Args:\n",
        "            sentences: Raw sentences\n",
        "        Returns:\n",
        "            token_num\n",
        "        \"\"\"\n",
        "        self.num_doc = len(sentences)\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0 or i == len(sentences) - 1:\n",
        "                print_line('Fitting TF-IDF encoder:', (i + 1), '/', len(sentences))\n",
        "            # Start your code here (step 1 & 2)\n",
        "            clean_sentences = self.processor.apply(sentence)\n",
        "            tokens = tokenize(clean_sentences)\n",
        "\n",
        "            token_counter = Counter(tokens)\n",
        "            \n",
        "            for i,j in token_counter.items():\n",
        "                self.vocab[i] += j\n",
        "                self.df[i] += 1\n",
        "                       \n",
        "            # End\n",
        "        print_line('\\n')\n",
        "        # Start your code here (Step 3\n",
        "        \n",
        "        self.vocab = {i:j for i,j in sorted(self.vocab.items(), key=lambda x:x[1], reverse=True)}\n",
        "        self.vocab = {key: self.vocab[key] for key in list(self.vocab.keys())}\n",
        "        self.token2index = {key: idx for idx, key in enumerate(self.vocab.keys())}\n",
        "        \n",
        "        # End\n",
        "        token_num = len(self.token2index) \n",
        "        print('The number of distinct tokens:', token_num)\n",
        "        return token_num\n",
        "\n",
        "    def encode(self, sentences: Union[List[str], np.ndarray]) -> np.ndarray:\n",
        "        \"\"\" Encode the sentences into TF-IDF feature vector\n",
        "            Note: if a token in a sentence does not exist in the fit encoder, we just ignore it.\n",
        "        Args:\n",
        "            sentences: Raw sentences\n",
        "        Returns:\n",
        "            features: A (n x token_num) matrix, where n is the number of sentences\n",
        "        \"\"\"\n",
        "        n = len(sentences)\n",
        "        features = np.zeros((n, len(self.token2index)))\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0 or i == n - 1:\n",
        "                print_line('Encoding with TF-IDF encoder:', (i + 1), '/', n)\n",
        "            # Start your code (calculate TF-IDF)\n",
        "            clean_sentence = self.processor.apply(sentence)\n",
        "            tokens = tokenize(clean_sentence)\n",
        "            \n",
        "            tokens_counter = Counter(tokens)\n",
        "            \n",
        "            for token, counter in tokens_counter.items():\n",
        "                if token in self.vocab:\n",
        "                    features[i][self.token2index[token]] = counter/min(len(tokens),len(self.vocab))\n",
        "\n",
        "        idf_values = np.zeros(len(self.vocab))\n",
        "\n",
        "        for i,j in self.df.items():\n",
        "            if i in self.vocab:\n",
        "                idf_values[self.token2index[i]] = np.log((self.num_doc + 1)/(j + 1))+1\n",
        "                \n",
        "        features = features*idf_values[:,]\n",
        "\n",
        "            # End\n",
        "        print_line('\\n')\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61755eeb-5d1c-4923-b58a-0e9409f4b4cd",
      "metadata": {
        "id": "61755eeb-5d1c-4923-b58a-0e9409f4b4cd"
      },
      "source": [
        "##### Test your implementation by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f1c1bd2b-4d06-46f2-97da-9ce53fb714ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1c1bd2b-4d06-46f2-97da-9ce53fb714ec",
        "outputId": "4dd7a6af-0b9a-4ca5-b308-4391f3437c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The number of distinct tokens: 1464\n",
            "\n",
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.22542057 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.04526579 ... 0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "encoder = TfIdfEncoder()\n",
        "encoder.fit(train_sentences[:100])\n",
        "features = encoder.encode(train_sentences[:10])\n",
        "\n",
        "print(features[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f6e3946-8032-474c-9643-74ee3c5fcdb9",
      "metadata": {
        "id": "1f6e3946-8032-474c-9643-74ee3c5fcdb9"
      },
      "source": [
        "#### Encode training, validation, and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "85fd6854-727c-4cfd-bb66-e080f21dd909",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85fd6854-727c-4cfd-bb66-e080f21dd909",
        "outputId": "fbeb1bab-22b0-4c95-9092-e0aed13558f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The number of distinct tokens: 16760\n",
            "\n",
            "\n",
            "\n",
            "The size of training set: (14067, 16760) (14067, 3)\n",
            "The size of validation set: (1562, 16760) (1562, 3)\n",
            "The size of test set: (3907, 16760) (3907, 3)\n"
          ]
        }
      ],
      "source": [
        "num_class = 3\n",
        "\n",
        "encoder = TfIdfEncoder()\n",
        "vocab_size = encoder.fit(train_sentences)\n",
        "\n",
        "x_train = encoder.encode(train_sentences)\n",
        "x_valid = encoder.encode(valid_sentences)\n",
        "x_test = encoder.encode(test_sentences)\n",
        "\n",
        "y_train = np.zeros((len(train_labels), num_class))\n",
        "y_valid = np.zeros((len(valid_labels), num_class))\n",
        "y_test = np.zeros((len(test_labels), num_class))\n",
        "y_train[np.arange(len(train_labels)), train_labels] = 1\n",
        "y_valid[np.arange(len(valid_labels)), valid_labels] = 1\n",
        "y_test[np.arange(len(test_labels)), test_labels] = 1\n",
        "\n",
        "print('The size of training set:', x_train.shape, y_train.shape)\n",
        "print('The size of validation set:', x_valid.shape, y_valid.shape)\n",
        "print('The size of test set:', x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e71bd1-478c-45e6-b679-6def46ff4829",
      "metadata": {
        "id": "11e71bd1-478c-45e6-b679-6def46ff4829"
      },
      "source": [
        "## 2. MLP (20 Points)\n",
        "In this section, you are required to implement a two-layer MLP model (input -> hidden layer -> output layer) with $L_2$ regularization from scratch. \n",
        "\n",
        "The objective function of LR for multi-class classification:\n",
        "\n",
        "$$J = L(\\mathbf{x}, \\mathbf{y} \\mid \\mathbf{w}, \\mathbf{b}) = -\\frac{1}{n}\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{ik}log\\frac{e^{f_k}}{\\sum_{c=1}^{K}e^{f_c}} + \\lambda \\sum_{j=1}^{d}w_{kj}^2$$\n",
        "\n",
        "- $z_1 = w_1x$\n",
        "- $h_1 = activation(z_1)$\n",
        "- $z_2 = w_2 h_1$\n",
        "- $\\hat{y} = softmax(z_2)$\n",
        "\n",
        "- $n$: Number of samples\n",
        "- $d$: Dimension of $\\mathbf{w}$\n",
        "- Here, you can use `sigmoid` as the activation function for the hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "944a5d09-483d-4fed-b8f8-6b5da594c59a",
      "metadata": {
        "id": "944a5d09-483d-4fed-b8f8-6b5da594c59a"
      },
      "source": [
        "### 2.1 MLP Model (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "44537480-4186-488f-b4e7-89d211054717",
      "metadata": {
        "id": "44537480-4186-488f-b4e7-89d211054717"
      },
      "outputs": [],
      "source": [
        "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
        "    \"\"\" The softmax activation function\n",
        "    Args:\n",
        "        x: Input matrix or vector\n",
        "        axis: The dimension of x that needs to run softmax, default -1, i.e., the last dimension\n",
        "    Returns:\n",
        "        output: Softmax value of the specified dimension in x\n",
        "    \"\"\"\n",
        "    # Start your code here\n",
        "    x = np.exp(x)/np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "    # End\n",
        "    return x\n",
        "\n",
        "\n",
        "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\" The sigmoid activation function\n",
        "    Args:\n",
        "        x: Input matrix or vector\n",
        "    Returns:\n",
        "        output: Sigmoid value of each entry in x\n",
        "    \"\"\"\n",
        "    # Start your code here\n",
        "    x = 1 / (1 + np.exp(-x))\n",
        "    # End\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e8fefa08-6231-4fbf-869a-d07ebce350f8",
      "metadata": {
        "id": "e8fefa08-6231-4fbf-869a-d07ebce350f8"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
        "        \"\"\" MLP Model\n",
        "        Args:\n",
        "            feature_dim: feature dimension\n",
        "            hidden_dim: hidden units\n",
        "            num_class: number of class\n",
        "            lambda_: lambda in L2 regularizer\n",
        "        \"\"\"\n",
        "        # Start your code here (initialize weight and bias)\n",
        "        self.w1 = np.random.rand(hidden_dim, feature_dim)\n",
        "        self.b1 = np.random.rand(1)\n",
        "        self.w2 = np.random.rand(num_class, hidden_dim)\n",
        "        self.b2 = np.random.rand(1)\n",
        "\n",
        "        # End\n",
        "        self.lambda_ = lambda_\n",
        "        self.eps = 1e-9\n",
        "\n",
        "    def forward(self, x: np.ndarray, return_hiddens: bool = False) -> np.ndarray:\n",
        "        \"\"\" Forward process of logistic regression\n",
        "            Calculate y_hat using x\n",
        "        Args:\n",
        "            x: Input data\n",
        "            return_hiddens: If true the function will return h1 for gradient calculation\n",
        "        Returns:\n",
        "            y_hat: Output\n",
        "            h1: Hidden output, used for gradient calculation. Returned if return_hiddens is set to True\n",
        "        \"\"\"\n",
        "        y_hat = 0\n",
        "        h1 = 0, 0\n",
        "        w1, b1, w2, b2 = self.w1, self.b1, self.w2, self.b2\n",
        "        # Start your code here (calculate y_hat of MLP using x)\n",
        "        \n",
        "        z1 = np.matmul(x, self.w1.T)\n",
        "        h1 = sigmoid(z1)\n",
        "        \n",
        "        z2 = np.matmul(h1, self.w2.T)\n",
        "        y_hat = softmax(z2)\n",
        "        \n",
        "        # End\n",
        "        if return_hiddens:\n",
        "            return y_hat, h1\n",
        "        else:\n",
        "            return y_hat\n",
        "\n",
        "    def backward(self,\n",
        "                 x: np.ndarray,\n",
        "                 y_hat: np.ndarray,\n",
        "                 y: np.ndarray,\n",
        "                 h1: np.array) -> Tuple[np.ndarray, Union[float, np.ndarray], np.ndarray, Union[float, np.ndarray]]:\n",
        "        \"\"\" Backward process of logistic regression\n",
        "            Calculate the gradient of w and b\n",
        "        Args:\n",
        "            x: Input data\n",
        "            y_hat: Output of forward\n",
        "            y: Ground-truth\n",
        "            h1: Hidden output of the hidden layer\n",
        "        Returns:\n",
        "            dw1: Gradient of w1\n",
        "            db1: Gradient of b1\n",
        "            dw2: Gradient of w2\n",
        "            db2: Gradient of b2\n",
        "        \"\"\"\n",
        "        w1, w2 = self.w1, self.w2\n",
        "        dw1, db1, dw2, db2 = 0.0, 0.0, 0.0, 0.0\n",
        "        n = len(x)\n",
        "        # Start your code here (calculate the gradient of w and b)\n",
        "        dz2 = y_hat - y\n",
        "        dw2 = (1/n)*np.matmul(dz2.T, h1)\n",
        "        \n",
        "        db1 = np.mean((y_hat-y), axis=0)\n",
        "        \n",
        "        dz1 = np.multiply(np.matmul(dz2, self.w2), (h1>0).astype('int'))\n",
        "        dw1 = (1/n)*np.matmul(dz1.T, x)\n",
        "        \n",
        "        db2 = np.mean((y_hat-y), axis=0)\n",
        "        # End\n",
        "        return dw1, db1, dw2, db2\n",
        "\n",
        "    def categorical_cross_entropy_loss(self,\n",
        "                                       y_hat: np.ndarray,\n",
        "                                       y: np.ndarray) -> Union[float, np.ndarray]:\n",
        "        \"\"\" Calculate the binary cross-entropy loss\n",
        "        Args:\n",
        "            y_hat: Output of forward\n",
        "            y: Ground-truth\n",
        "        Returns:\n",
        "            loss: BCE loss\n",
        "        \"\"\"\n",
        "        y_hat = np.clip(y_hat, a_min=self.eps, a_max=1 - self.eps)\n",
        "        loss = 0\n",
        "        # Start your code here (Calculate the binary cross-entropy)\n",
        "        loss = -np.mean(np.sum(y * np.log(y_hat),axis=1))\n",
        "        # End\n",
        "        return loss\n",
        "\n",
        "    def gradient_descent(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float], lr: float):\n",
        "        self.w1 -= lr * dw1\n",
        "        self.b1 -= lr * db1\n",
        "        self.w2 -= lr * dw2\n",
        "        self.b2 -= lr * db2\n",
        "\n",
        "    def predict(self, y_hat: np.ndarray) -> np.ndarray:\n",
        "        \"\"\" Predict the label using the output y_hat\n",
        "        Args:\n",
        "            y_hat: Model output\n",
        "        Returns:\n",
        "            pred: Prediction\n",
        "        \"\"\"\n",
        "        pred = np.zeros_like(y_hat)\n",
        "        index = np.argmax(y_hat, axis=-1)\n",
        "        pred[np.arange(len(y_hat)), index] = 1\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0edf807c-3c04-42df-9ad6-3aa8bbc02e38",
      "metadata": {
        "id": "0edf807c-3c04-42df-9ad6-3aa8bbc02e38",
        "tags": []
      },
      "source": [
        "### 2.2 Evaluation Metrics\n",
        "\n",
        "Accuracy, Precision, Recall, F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "5f627c34-e233-4313-82b3-d5285556209b",
      "metadata": {
        "id": "5f627c34-e233-4313-82b3-d5285556209b"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "\n",
        "def get_metrics(y_pred: np.ndarray, y_true: np.ndarray) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\" Calculate the accuracy, precision, recall, and f1 score.\n",
        "        You are allowed to use precision_recall_fscore_support from scikit-learn. Please set average to 'micro'\n",
        "    Args:\n",
        "        y_pred: Prediction\n",
        "        y_true: Ground-truth\n",
        "    Returns:\n",
        "        accuracy: float number. The accuracy for the whole dataset\n",
        "        precision, recall, f1: np.ndarray (num_class, ). The precision, recall, f1 for each class\n",
        "    \"\"\"\n",
        "    assert y_pred.shape == y_true.shape\n",
        "    accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
        "    # Start your code here\n",
        "    y_pred=np.argmax(y_pred, axis=1)\n",
        "    y_true=np.argmax(y_true, axis=1)\n",
        "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
        "    # End\n",
        "    return accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1794868a-0c46-4219-8d82-979c79e6fedb",
      "metadata": {
        "id": "1794868a-0c46-4219-8d82-979c79e6fedb"
      },
      "source": [
        "### 2.3 AdaGrad (5 points)\n",
        "\n",
        "$$ \\mathbf{G}^{(t + 1)} \\leftarrow \\mathbf{G}^{(t)} + \\boldsymbol{g}^{(t + 1)} \\cdot \\boldsymbol{g}^{(t + 1)} $$\n",
        "$$ \\mathbf{w}^{(t + 1)} \\leftarrow \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}}\\boldsymbol{g}^{(t + 1)} = \\mathbf{w}^{(t)} - \\eta\\frac{\\boldsymbol{g}^{(t + 1)}}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4a817e4d-7f0f-429a-84a6-03c1bdd50680",
      "metadata": {
        "id": "4a817e4d-7f0f-429a-84a6-03c1bdd50680"
      },
      "outputs": [],
      "source": [
        "class AdaGrad:\n",
        "    def __init__(self, init_lr, model):\n",
        "        self.init_lr = init_lr\n",
        "        self.model = model\n",
        "        \n",
        "        self.accumulative_dw1 = 0\n",
        "        self.accumulative_db1 = 0\n",
        "        self.accumulative_dw2 = 0\n",
        "        self.accumulative_db2 = 0\n",
        "        self.eps = 1e-9\n",
        "        \n",
        "    def update(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float]):\n",
        "        \"\"\" 1. Use the gradient in the current step to update the accumulative gradient of each parameter.\n",
        "            2. Calculate the new gradient with the accumulative gradient\n",
        "            3. Use the init learning rate the new gradient to update the parameter with model.gradient_descent()\n",
        "        \n",
        "        Do not return anything\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        # Step 1\n",
        "        self.model.gradient_descent(dw1,db1,dw2,db2,self.init_lr)\n",
        "        \n",
        "        # Step 2\n",
        "        new_dw1,new_db1,new_dw2,new_db2 = 0.0,0.0,0.0,0.0\n",
        "        new_dw1=new_dw1 + dw1**2\n",
        "        new_db1=new_db1 + db1**2\n",
        "        new_dw2=new_dw2 + dw2**2\n",
        "        new_db2=new_db2 + db2**2\n",
        "        \n",
        "        # Step 3\n",
        "        self.accumulative_dw1 += self.accumulative_dw1-(0.1/np.sqrt(new_dw1+self.eps))**dw1\n",
        "        self.accumulative_db1 += self.accumulative_db1-(0.1/np.sqrt(new_db1+self.eps))**db1\n",
        "        self.accumulative_dw2 += self.accumulative_dw2-(0.1/np.sqrt(new_dw2+self.eps))**dw2\n",
        "        self.accumulative_db2 += self.accumulative_db2-(0.1/np.sqrt(new_db2+self.eps))**db2\n",
        "        \n",
        "        # End\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e976ba-17d9-45ac-818e-fbfdf304b190",
      "metadata": {
        "id": "31e976ba-17d9-45ac-818e-fbfdf304b190",
        "tags": []
      },
      "source": [
        "### 2.4 Mini-batch Gradient Descent (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "958ee97d-7c88-4a23-8da5-c43fa36038e6",
      "metadata": {
        "id": "958ee97d-7c88-4a23-8da5-c43fa36038e6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "def train_mbgd(model: 'MLP',\n",
        "               x_train: np.ndarray,\n",
        "               y_train: np.ndarray,\n",
        "               x_valid: np.ndarray,\n",
        "               y_valid: np.ndarray,\n",
        "               lr: float,\n",
        "               num_epoch: int,\n",
        "               batch_size: int,\n",
        "               print_every: int = 10):\n",
        "    \"\"\" Training with Gradient Descent\n",
        "    Args:\n",
        "        model: The logistic regression model\n",
        "        x_train: Training feature, (n x d) matrix\n",
        "        y_train: Training label, (n, ) vector\n",
        "        x_valid: Validation feature, (n x d) matrix\n",
        "        y_valid: Validation label, (n, ) vector\n",
        "        lr: Learning rate\n",
        "        num_epoch: Number of training epochs\n",
        "        batch_size: Number of training samples in a batch\n",
        "        print_every: Print log every {print_every} epochs\n",
        "    Returns:\n",
        "        train_history: Log of training information. The format of training history is\n",
        "                       { 'loss': [] }\n",
        "                       It records the average loss of each epoch.\n",
        "        valid_history: Log of validation information. The format of training and validation history is\n",
        "                       {\n",
        "                           'loss': [],\n",
        "                           'accuracy': [],\n",
        "                           'precision': [],\n",
        "                           'recall': [],\n",
        "                           'f1': []\n",
        "                       }\n",
        "    \"\"\"\n",
        "    train_history = OrderedDict({'loss': []})\n",
        "    valid_history = OrderedDict({\n",
        "        'loss': [],\n",
        "        'accuracy': [],\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'f1': []\n",
        "    })\n",
        "\n",
        "    def format_output(epoch, num_epoch, train_history, valid_history):\n",
        "        epoch_log = f'Epoch {epoch + 1} / {num_epoch}'\n",
        "        train_log = ' - '.join([f'train_{key}: {val[-1]:.4f}' for key, val in train_history.items()])\n",
        "        valid_log = ' - '.join([f'valid_{key}: {val[-1]:.4f}' for key, val in valid_history.items()])\n",
        "        log = f'{epoch_log}: {train_log} - {valid_log}'\n",
        "        return log\n",
        "\n",
        "    # IMPORTANT: YOU SHOULD USE THIS OPTIMIZER TO UPDATE THE MODEL\n",
        "    optimizer = AdaGrad(init_lr=lr, model=model)\n",
        "\n",
        "    train_num_samples = len(x_train)\n",
        "    n_batch = train_num_samples // batch_size\n",
        "    \n",
        "    for epoch in range(num_epoch):\n",
        "        epoch_loss = 0.0\n",
        "        # Start your code here (training)\n",
        "        #     Step 1. Model forward\n",
        "        #     Step 2. Calculate loss\n",
        "        #     Step 3. Model backward\n",
        "        #     Step 4. Optimization with Adagrad\n",
        "        random_indices = np.random.permutation(train_num_samples)\n",
        "        x_rand = x_train[random_indices]\n",
        "        y_rand = y_train[random_indices]\n",
        "        \n",
        "        for b in range(0, train_num_samples, batch_size):\n",
        "            x_batch = x_rand[b: b+batch_size] \n",
        "            y_batch = y_rand[b: b+batch_size]\n",
        "            \n",
        "            y_hat, h1 = model.forward(x_batch, return_hiddens=True)\n",
        "            loss = model.categorical_cross_entropy_loss(y_hat, y_batch)\n",
        "\n",
        "            dw1,db1,dw2,db2=model.backward(x_batch, y_hat, y_batch, h1)\n",
        "            model.gradient_descent(dw1,db1,dw2,db2,lr)\n",
        "            optimizer.update(dw1,db1,dw2,db2)          \n",
        "            epoch_loss+=loss*len(x_batch)\n",
        "\n",
        "        # End          \n",
        "        valid_loss = 0.\n",
        "        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
        "        # Start your code here (validation)\n",
        "        #     Step 1. Predict\n",
        "        #     Step 2. Calculate loss\n",
        "        #     Step 3. Calculate metrics\n",
        "        y_hat=model.forward(x_valid)\n",
        "        valid_loss = model.categorical_cross_entropy_loss(y_hat, y_valid)\n",
        "        y_pred = model.predict(y_hat)\n",
        "        accuracy, precision, recall, f1 = get_metrics(y_pred, y_valid)\n",
        "        # End\n",
        "\n",
        "        train_history['loss'].append(epoch_loss / train_num_samples)\n",
        "        for vals, val in zip(valid_history.values(), [valid_loss, accuracy, precision, recall, f1]):\n",
        "            vals.append(val)\n",
        "        log = format_output(epoch, num_epoch, train_history, valid_history)\n",
        "        if epoch % print_every == 0 or epoch == num_epoch - 1:\n",
        "            print(log)\n",
        "        else:\n",
        "            print_line(log)\n",
        "\n",
        "    return train_history, valid_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9273e6-eda3-4a16-8779-810bdc55cf33",
      "metadata": {
        "id": "5f9273e6-eda3-4a16-8779-810bdc55cf33"
      },
      "source": [
        "Run Mini-batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "49874274-247f-43c5-973c-5d8917869b08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49874274-247f-43c5-973c-5d8917869b08",
        "outputId": "d5330d77-7cca-478f-b276-bb315f0f9c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 100: train_loss: 0.8132 - valid_loss: 0.7974 - valid_accuracy: 0.7055 - valid_precision: 0.7055 - valid_recall: 0.7055 - valid_f1: 0.7055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in add\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: overflow encountered in add\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 / 100: train_loss: 0.7208 - valid_loss: 0.7258 - valid_accuracy: 0.7305 - valid_precision: 0.7305 - valid_recall: 0.7305 - valid_f1: 0.7305\n",
            "Epoch 21 / 100: train_loss: 0.6569 - valid_loss: 0.6544 - valid_accuracy: 0.7484 - valid_precision: 0.7484 - valid_recall: 0.7484 - valid_f1: 0.7484\n",
            "Epoch 31 / 100: train_loss: 0.6051 - valid_loss: 0.6034 - valid_accuracy: 0.7638 - valid_precision: 0.7638 - valid_recall: 0.7638 - valid_f1: 0.7638\n",
            "Epoch 41 / 100: train_loss: 0.5625 - valid_loss: 0.5631 - valid_accuracy: 0.7766 - valid_precision: 0.7766 - valid_recall: 0.7766 - valid_f1: 0.7766\n",
            "Epoch 51 / 100: train_loss: 0.5258 - valid_loss: 0.5296 - valid_accuracy: 0.7919 - valid_precision: 0.7919 - valid_recall: 0.7919 - valid_f1: 0.7919\n",
            "Epoch 61 / 100: train_loss: 0.4940 - valid_loss: 0.4985 - valid_accuracy: 0.8009 - valid_precision: 0.8009 - valid_recall: 0.8009 - valid_f1: 0.8009\n",
            "Epoch 71 / 100: train_loss: 0.4639 - valid_loss: 0.4698 - valid_accuracy: 0.8047 - valid_precision: 0.8047 - valid_recall: 0.8047 - valid_f1: 0.8047\n",
            "Epoch 81 / 100: train_loss: 0.4370 - valid_loss: 0.4445 - valid_accuracy: 0.8188 - valid_precision: 0.8188 - valid_recall: 0.8188 - valid_f1: 0.8188\n",
            "Epoch 91 / 100: train_loss: 0.4125 - valid_loss: 0.4224 - valid_accuracy: 0.8310 - valid_precision: 0.8310 - valid_recall: 0.8310 - valid_f1: 0.8310\n",
            "Epoch 100 / 100: train_loss: 0.3915 - valid_loss: 0.4051 - valid_accuracy: 0.8239 - valid_precision: 0.8239 - valid_recall: 0.8239 - valid_f1: 0.8239\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(6666)\n",
        "\n",
        "hidden_dim = 128\n",
        "num_epoch = 100\n",
        "lr = 1e-2\n",
        "batch_size = 128\n",
        "lambda_ = 1e-8\n",
        "print_every = 10\n",
        "\n",
        "model_mbgd = MLP(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
        "mbgd_train_history, mbgd_valid_history = train_mbgd(model_mbgd, x_train, y_train, x_valid, y_valid, lr, num_epoch, batch_size, print_every)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b3020d5-8254-4dce-8034-56be5a5ecb19",
      "metadata": {
        "id": "6b3020d5-8254-4dce-8034-56be5a5ecb19"
      },
      "source": [
        "### 2.5 MLP using Tensorflow (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "bc1bbb3f-8ec7-4582-b590-7bc5b1155f1b",
      "metadata": {
        "id": "bc1bbb3f-8ec7-4582-b590-7bc5b1155f1b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Softmax\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "\n",
        "\n",
        "class MLPTF(Model):\n",
        "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
        "        \"\"\" MLP Model using tensorflow.keras\n",
        "        Args:\n",
        "            feature_dim: feature dimension\n",
        "            hidden_dim: hidden units\n",
        "            num_class: number of class\n",
        "            lambda_: lambda in L2 regularizer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Start your code here (initialize weight and bias)\n",
        "        # self.dense1 =\n",
        "        # self.dense2 =\n",
        "        # self.softmax = \n",
        "        self.dense1 = tf.keras.layers.Dense(units = hidden_dim, activity_regularizer = tf.keras.regularizers.L2(lambda_) , activation=tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(units = num_class, activation=tf.nn.softmax)\n",
        "        self.softmax = tf.keras.layers.Softmax()\n",
        "\n",
        "        # End\n",
        "        \n",
        "    def call(self, x):\n",
        "        \"\"\" Forward function of tf. It should be named 'call'\n",
        "        \n",
        "        Args:\n",
        "            x: (n x feature_dim) tensor\n",
        "        Returns:\n",
        "            y_hat: (n x num_class) tensor\n",
        "        \"\"\"\n",
        "        # Start your code here (Forward)\n",
        "        x = self.dense1(x)\n",
        "        y_hat = self.dense2(x)\n",
        "        # End\n",
        "        return y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "879b1fea-4852-4b0e-994a-9637982717a9",
      "metadata": {
        "id": "879b1fea-4852-4b0e-994a-9637982717a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650be8aa-0971-4a76-b596-acf3ca0c6e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"mlptf\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               multiple                  2145408   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,145,795\n",
            "Trainable params: 2,145,795\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_addons as tfa\n",
        "np.random.seed(6666)\n",
        "tf.random.set_seed(6666)\n",
        "\n",
        "\n",
        "hidden_dim = 128\n",
        "num_epoch = 100\n",
        "lr = 1e-1\n",
        "batch_size = 128\n",
        "lambda_ = 1e-8\n",
        "\n",
        "model_tf = MLPTF(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
        "model_tf.build(input_shape=(None, vocab_size))\n",
        "model_tf.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=lr),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(num_classes=num_class, average='micro')])\n",
        "model_tf.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1541100f-3e5e-4024-82eb-3e2a6d7d3f7a",
      "metadata": {
        "id": "1541100f-3e5e-4024-82eb-3e2a6d7d3f7a",
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a84404-f15c-4f74-bfe4-c45646c0c432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 5s 10ms/step - loss: 0.6835 - categorical_accuracy: 0.7356 - f1_score: 0.7356 - val_loss: 0.5338 - val_categorical_accuracy: 0.7894 - val_f1_score: 0.7894\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.4466 - categorical_accuracy: 0.8178 - f1_score: 0.8178 - val_loss: 0.3708 - val_categorical_accuracy: 0.8483 - val_f1_score: 0.8483\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.3159 - categorical_accuracy: 0.8786 - f1_score: 0.8786 - val_loss: 0.2804 - val_categorical_accuracy: 0.8803 - val_f1_score: 0.8803\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.2377 - categorical_accuracy: 0.9169 - f1_score: 0.9169 - val_loss: 0.2183 - val_categorical_accuracy: 0.9257 - val_f1_score: 0.9257\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.1900 - categorical_accuracy: 0.9386 - f1_score: 0.9386 - val_loss: 0.1874 - val_categorical_accuracy: 0.9321 - val_f1_score: 0.9321\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.1586 - categorical_accuracy: 0.9522 - f1_score: 0.9522 - val_loss: 0.1608 - val_categorical_accuracy: 0.9456 - val_f1_score: 0.9456\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.1367 - categorical_accuracy: 0.9601 - f1_score: 0.9601 - val_loss: 0.1455 - val_categorical_accuracy: 0.9526 - val_f1_score: 0.9526\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.1195 - categorical_accuracy: 0.9667 - f1_score: 0.9667 - val_loss: 0.1281 - val_categorical_accuracy: 0.9641 - val_f1_score: 0.9641\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.1082 - categorical_accuracy: 0.9694 - f1_score: 0.9694 - val_loss: 0.1165 - val_categorical_accuracy: 0.9648 - val_f1_score: 0.9648\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0975 - categorical_accuracy: 0.9723 - f1_score: 0.9723 - val_loss: 0.1083 - val_categorical_accuracy: 0.9680 - val_f1_score: 0.9680\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0892 - categorical_accuracy: 0.9750 - f1_score: 0.9750 - val_loss: 0.1038 - val_categorical_accuracy: 0.9699 - val_f1_score: 0.9699\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0824 - categorical_accuracy: 0.9767 - f1_score: 0.9767 - val_loss: 0.1003 - val_categorical_accuracy: 0.9718 - val_f1_score: 0.9718\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0767 - categorical_accuracy: 0.9791 - f1_score: 0.9791 - val_loss: 0.0909 - val_categorical_accuracy: 0.9744 - val_f1_score: 0.9744\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0724 - categorical_accuracy: 0.9790 - f1_score: 0.9790 - val_loss: 0.0836 - val_categorical_accuracy: 0.9757 - val_f1_score: 0.9757\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0668 - categorical_accuracy: 0.9822 - f1_score: 0.9822 - val_loss: 0.0780 - val_categorical_accuracy: 0.9789 - val_f1_score: 0.9789\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0635 - categorical_accuracy: 0.9819 - f1_score: 0.9819 - val_loss: 0.0785 - val_categorical_accuracy: 0.9782 - val_f1_score: 0.9782\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0598 - categorical_accuracy: 0.9836 - f1_score: 0.9836 - val_loss: 0.0695 - val_categorical_accuracy: 0.9789 - val_f1_score: 0.9789\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0573 - categorical_accuracy: 0.9841 - f1_score: 0.9841 - val_loss: 0.0708 - val_categorical_accuracy: 0.9782 - val_f1_score: 0.9782\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0548 - categorical_accuracy: 0.9841 - f1_score: 0.9841 - val_loss: 0.0620 - val_categorical_accuracy: 0.9846 - val_f1_score: 0.9846\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0519 - categorical_accuracy: 0.9853 - f1_score: 0.9853 - val_loss: 0.0628 - val_categorical_accuracy: 0.9846 - val_f1_score: 0.9846\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0493 - categorical_accuracy: 0.9857 - f1_score: 0.9857 - val_loss: 0.0627 - val_categorical_accuracy: 0.9808 - val_f1_score: 0.9808\n",
            "Epoch 22/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0482 - categorical_accuracy: 0.9866 - f1_score: 0.9866 - val_loss: 0.0563 - val_categorical_accuracy: 0.9834 - val_f1_score: 0.9834\n",
            "Epoch 23/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0458 - categorical_accuracy: 0.9871 - f1_score: 0.9871 - val_loss: 0.0585 - val_categorical_accuracy: 0.9840 - val_f1_score: 0.9840\n",
            "Epoch 24/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0435 - categorical_accuracy: 0.9889 - f1_score: 0.9889 - val_loss: 0.0516 - val_categorical_accuracy: 0.9846 - val_f1_score: 0.9846\n",
            "Epoch 25/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0421 - categorical_accuracy: 0.9880 - f1_score: 0.9880 - val_loss: 0.0495 - val_categorical_accuracy: 0.9866 - val_f1_score: 0.9866\n",
            "Epoch 26/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0418 - categorical_accuracy: 0.9885 - f1_score: 0.9885 - val_loss: 0.0507 - val_categorical_accuracy: 0.9834 - val_f1_score: 0.9834\n",
            "Epoch 27/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0403 - categorical_accuracy: 0.9888 - f1_score: 0.9888 - val_loss: 0.0453 - val_categorical_accuracy: 0.9859 - val_f1_score: 0.9859\n",
            "Epoch 28/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0402 - categorical_accuracy: 0.9885 - f1_score: 0.9885 - val_loss: 0.0422 - val_categorical_accuracy: 0.9885 - val_f1_score: 0.9885\n",
            "Epoch 29/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0388 - categorical_accuracy: 0.9889 - f1_score: 0.9889 - val_loss: 0.0451 - val_categorical_accuracy: 0.9872 - val_f1_score: 0.9872\n",
            "Epoch 30/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0368 - categorical_accuracy: 0.9897 - f1_score: 0.9897 - val_loss: 0.0465 - val_categorical_accuracy: 0.9859 - val_f1_score: 0.9859\n",
            "Epoch 31/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0350 - categorical_accuracy: 0.9904 - f1_score: 0.9904 - val_loss: 0.0425 - val_categorical_accuracy: 0.9846 - val_f1_score: 0.9846\n",
            "Epoch 32/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0357 - categorical_accuracy: 0.9894 - f1_score: 0.9894 - val_loss: 0.0453 - val_categorical_accuracy: 0.9853 - val_f1_score: 0.9853\n",
            "Epoch 33/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0329 - categorical_accuracy: 0.9908 - f1_score: 0.9908 - val_loss: 0.0401 - val_categorical_accuracy: 0.9866 - val_f1_score: 0.9866\n",
            "Epoch 34/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0330 - categorical_accuracy: 0.9905 - f1_score: 0.9905 - val_loss: 0.0362 - val_categorical_accuracy: 0.9885 - val_f1_score: 0.9885\n",
            "Epoch 35/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0316 - categorical_accuracy: 0.9909 - f1_score: 0.9909 - val_loss: 0.0361 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 36/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0320 - categorical_accuracy: 0.9908 - f1_score: 0.9908 - val_loss: 0.0408 - val_categorical_accuracy: 0.9878 - val_f1_score: 0.9878\n",
            "Epoch 37/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0303 - categorical_accuracy: 0.9915 - f1_score: 0.9915 - val_loss: 0.0421 - val_categorical_accuracy: 0.9846 - val_f1_score: 0.9846\n",
            "Epoch 38/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0312 - categorical_accuracy: 0.9908 - f1_score: 0.9908 - val_loss: 0.0375 - val_categorical_accuracy: 0.9898 - val_f1_score: 0.9898\n",
            "Epoch 39/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0304 - categorical_accuracy: 0.9905 - f1_score: 0.9905 - val_loss: 0.0340 - val_categorical_accuracy: 0.9878 - val_f1_score: 0.9878\n",
            "Epoch 40/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0289 - categorical_accuracy: 0.9914 - f1_score: 0.9914 - val_loss: 0.0348 - val_categorical_accuracy: 0.9878 - val_f1_score: 0.9878\n",
            "Epoch 41/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0291 - categorical_accuracy: 0.9915 - f1_score: 0.9915 - val_loss: 0.0362 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 42/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0285 - categorical_accuracy: 0.9916 - f1_score: 0.9916 - val_loss: 0.0323 - val_categorical_accuracy: 0.9904 - val_f1_score: 0.9904\n",
            "Epoch 43/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0280 - categorical_accuracy: 0.9915 - f1_score: 0.9915 - val_loss: 0.0344 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 44/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0272 - categorical_accuracy: 0.9913 - f1_score: 0.9913 - val_loss: 0.0321 - val_categorical_accuracy: 0.9904 - val_f1_score: 0.9904\n",
            "Epoch 45/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0275 - categorical_accuracy: 0.9910 - f1_score: 0.9910 - val_loss: 0.0308 - val_categorical_accuracy: 0.9904 - val_f1_score: 0.9904\n",
            "Epoch 46/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0265 - categorical_accuracy: 0.9917 - f1_score: 0.9917 - val_loss: 0.0332 - val_categorical_accuracy: 0.9898 - val_f1_score: 0.9898\n",
            "Epoch 47/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0266 - categorical_accuracy: 0.9915 - f1_score: 0.9915 - val_loss: 0.0308 - val_categorical_accuracy: 0.9898 - val_f1_score: 0.9898\n",
            "Epoch 48/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0254 - categorical_accuracy: 0.9923 - f1_score: 0.9923 - val_loss: 0.0318 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 49/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0242 - categorical_accuracy: 0.9925 - f1_score: 0.9925 - val_loss: 0.0312 - val_categorical_accuracy: 0.9878 - val_f1_score: 0.9878\n",
            "Epoch 50/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0249 - categorical_accuracy: 0.9923 - f1_score: 0.9923 - val_loss: 0.0229 - val_categorical_accuracy: 0.9942 - val_f1_score: 0.9942\n",
            "Epoch 51/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0253 - categorical_accuracy: 0.9918 - f1_score: 0.9918 - val_loss: 0.0285 - val_categorical_accuracy: 0.9930 - val_f1_score: 0.9930\n",
            "Epoch 52/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0247 - categorical_accuracy: 0.9918 - f1_score: 0.9918 - val_loss: 0.0307 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 53/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0247 - categorical_accuracy: 0.9918 - f1_score: 0.9918 - val_loss: 0.0323 - val_categorical_accuracy: 0.9872 - val_f1_score: 0.9872\n",
            "Epoch 54/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0234 - categorical_accuracy: 0.9918 - f1_score: 0.9918 - val_loss: 0.0309 - val_categorical_accuracy: 0.9885 - val_f1_score: 0.9885\n",
            "Epoch 55/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0244 - categorical_accuracy: 0.9916 - f1_score: 0.9916 - val_loss: 0.0249 - val_categorical_accuracy: 0.9904 - val_f1_score: 0.9904\n",
            "Epoch 56/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0236 - categorical_accuracy: 0.9919 - f1_score: 0.9919 - val_loss: 0.0279 - val_categorical_accuracy: 0.9904 - val_f1_score: 0.9904\n",
            "Epoch 57/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0228 - categorical_accuracy: 0.9928 - f1_score: 0.9928 - val_loss: 0.0299 - val_categorical_accuracy: 0.9904 - val_f1_score: 0.9904\n",
            "Epoch 58/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0229 - categorical_accuracy: 0.9922 - f1_score: 0.9922 - val_loss: 0.0280 - val_categorical_accuracy: 0.9910 - val_f1_score: 0.9910\n",
            "Epoch 59/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0231 - categorical_accuracy: 0.9918 - f1_score: 0.9918 - val_loss: 0.0271 - val_categorical_accuracy: 0.9910 - val_f1_score: 0.9910\n",
            "Epoch 60/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0224 - categorical_accuracy: 0.9923 - f1_score: 0.9923 - val_loss: 0.0265 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 61/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0230 - categorical_accuracy: 0.9921 - f1_score: 0.9921 - val_loss: 0.0276 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 62/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0228 - categorical_accuracy: 0.9921 - f1_score: 0.9921 - val_loss: 0.0242 - val_categorical_accuracy: 0.9898 - val_f1_score: 0.9898\n",
            "Epoch 63/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0219 - categorical_accuracy: 0.9920 - f1_score: 0.9920 - val_loss: 0.0231 - val_categorical_accuracy: 0.9936 - val_f1_score: 0.9936\n",
            "Epoch 64/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0215 - categorical_accuracy: 0.9929 - f1_score: 0.9929 - val_loss: 0.0262 - val_categorical_accuracy: 0.9923 - val_f1_score: 0.9923\n",
            "Epoch 65/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0218 - categorical_accuracy: 0.9925 - f1_score: 0.9925 - val_loss: 0.0226 - val_categorical_accuracy: 0.9923 - val_f1_score: 0.9923\n",
            "Epoch 66/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0210 - categorical_accuracy: 0.9923 - f1_score: 0.9923 - val_loss: 0.0266 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 67/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0209 - categorical_accuracy: 0.9927 - f1_score: 0.9927 - val_loss: 0.0268 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 68/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0205 - categorical_accuracy: 0.9925 - f1_score: 0.9925 - val_loss: 0.0280 - val_categorical_accuracy: 0.9898 - val_f1_score: 0.9898\n",
            "Epoch 69/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0204 - categorical_accuracy: 0.9927 - f1_score: 0.9927 - val_loss: 0.0216 - val_categorical_accuracy: 0.9910 - val_f1_score: 0.9910\n",
            "Epoch 70/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0205 - categorical_accuracy: 0.9929 - f1_score: 0.9929 - val_loss: 0.0260 - val_categorical_accuracy: 0.9910 - val_f1_score: 0.9910\n",
            "Epoch 71/100\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.0205 - categorical_accuracy: 0.9921 - f1_score: 0.9921 - val_loss: 0.0275 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 72/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0215 - categorical_accuracy: 0.9918 - f1_score: 0.9918 - val_loss: 0.0266 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 73/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0203 - categorical_accuracy: 0.9927 - f1_score: 0.9927 - val_loss: 0.0275 - val_categorical_accuracy: 0.9885 - val_f1_score: 0.9885\n",
            "Epoch 74/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0206 - categorical_accuracy: 0.9920 - f1_score: 0.9920 - val_loss: 0.0229 - val_categorical_accuracy: 0.9917 - val_f1_score: 0.9917\n",
            "Epoch 75/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0198 - categorical_accuracy: 0.9930 - f1_score: 0.9930 - val_loss: 0.0241 - val_categorical_accuracy: 0.9898 - val_f1_score: 0.9898\n",
            "Epoch 76/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0195 - categorical_accuracy: 0.9927 - f1_score: 0.9927 - val_loss: 0.0264 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 77/100\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.0199 - categorical_accuracy: 0.9918 - f1_score: 0.9918 - val_loss: 0.0268 - val_categorical_accuracy: 0.9878 - val_f1_score: 0.9878\n",
            "Epoch 78/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.0202 - categorical_accuracy: 0.9923 - f1_score: 0.9923 - val_loss: 0.0204 - val_categorical_accuracy: 0.9917 - val_f1_score: 0.9917\n",
            "Epoch 79/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.0193 - categorical_accuracy: 0.9924 - f1_score: 0.9924 - val_loss: 0.0184 - val_categorical_accuracy: 0.9930 - val_f1_score: 0.9930\n",
            "Epoch 80/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.0197 - categorical_accuracy: 0.9924 - f1_score: 0.9924 - val_loss: 0.0230 - val_categorical_accuracy: 0.9904 - val_f1_score: 0.9904\n",
            "Epoch 81/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.0198 - categorical_accuracy: 0.9921 - f1_score: 0.9921 - val_loss: 0.0226 - val_categorical_accuracy: 0.9898 - val_f1_score: 0.9898\n",
            "Epoch 82/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0192 - categorical_accuracy: 0.9923 - f1_score: 0.9923 - val_loss: 0.0268 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 83/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0202 - categorical_accuracy: 0.9914 - f1_score: 0.9914 - val_loss: 0.0267 - val_categorical_accuracy: 0.9878 - val_f1_score: 0.9878\n",
            "Epoch 84/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0189 - categorical_accuracy: 0.9926 - f1_score: 0.9926 - val_loss: 0.0247 - val_categorical_accuracy: 0.9891 - val_f1_score: 0.9891\n",
            "Epoch 85/100\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.0195 - categorical_accuracy: 0.9920 - f1_score: 0.9920 - val_loss: 0.0244 - val_categorical_accuracy: 0.9930 - val_f1_score: 0.9930\n",
            "Epoch 86/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0190 - categorical_accuracy: 0.9924 - f1_score: 0.9924 - val_loss: 0.0202 - val_categorical_accuracy: 0.9923 - val_f1_score: 0.9923\n",
            "Epoch 87/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0188 - categorical_accuracy: 0.9927 - f1_score: 0.9927 - val_loss: 0.0277 - val_categorical_accuracy: 0.9878 - val_f1_score: 0.9878\n",
            "Epoch 88/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0185 - categorical_accuracy: 0.9926 - f1_score: 0.9926 - val_loss: 0.0245 - val_categorical_accuracy: 0.9910 - val_f1_score: 0.9910\n",
            "Epoch 89/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0192 - categorical_accuracy: 0.9924 - f1_score: 0.9924 - val_loss: 0.0255 - val_categorical_accuracy: 0.9885 - val_f1_score: 0.9885\n",
            "Epoch 90/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0184 - categorical_accuracy: 0.9925 - f1_score: 0.9925 - val_loss: 0.0226 - val_categorical_accuracy: 0.9910 - val_f1_score: 0.9910\n",
            "Epoch 91/100\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.0187 - categorical_accuracy: 0.9920 - f1_score: 0.9920 - val_loss: 0.0248 - val_categorical_accuracy: 0.9904 - val_f1_score: 0.9904\n",
            "Epoch 92/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0183 - categorical_accuracy: 0.9923 - f1_score: 0.9923 - val_loss: 0.0210 - val_categorical_accuracy: 0.9923 - val_f1_score: 0.9923\n",
            "Epoch 93/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0183 - categorical_accuracy: 0.9922 - f1_score: 0.9922 - val_loss: 0.0213 - val_categorical_accuracy: 0.9923 - val_f1_score: 0.9923\n",
            "Epoch 94/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0181 - categorical_accuracy: 0.9927 - f1_score: 0.9927 - val_loss: 0.0183 - val_categorical_accuracy: 0.9917 - val_f1_score: 0.9917\n",
            "Epoch 95/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0188 - categorical_accuracy: 0.9918 - f1_score: 0.9918 - val_loss: 0.0225 - val_categorical_accuracy: 0.9904 - val_f1_score: 0.9904\n",
            "Epoch 96/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0188 - categorical_accuracy: 0.9921 - f1_score: 0.9921 - val_loss: 0.0225 - val_categorical_accuracy: 0.9904 - val_f1_score: 0.9904\n",
            "Epoch 97/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0187 - categorical_accuracy: 0.9925 - f1_score: 0.9925 - val_loss: 0.0241 - val_categorical_accuracy: 0.9885 - val_f1_score: 0.9885\n",
            "Epoch 98/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0184 - categorical_accuracy: 0.9923 - f1_score: 0.9923 - val_loss: 0.0252 - val_categorical_accuracy: 0.9885 - val_f1_score: 0.9885\n",
            "Epoch 99/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0183 - categorical_accuracy: 0.9920 - f1_score: 0.9920 - val_loss: 0.0205 - val_categorical_accuracy: 0.9885 - val_f1_score: 0.9885\n",
            "Epoch 100/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.0187 - categorical_accuracy: 0.9914 - f1_score: 0.9914 - val_loss: 0.0236 - val_categorical_accuracy: 0.9878 - val_f1_score: 0.9878\n"
          ]
        }
      ],
      "source": [
        "tf_history = model_tf.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), batch_size=batch_size, epochs=num_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05bdbd8-bd67-49b5-938a-a71d3dba105e",
      "metadata": {
        "id": "f05bdbd8-bd67-49b5-938a-a71d3dba105e"
      },
      "source": [
        "#### Evaluation with Tensroflow\n",
        "You are required to report the loss, accuracy, precision, recall, and f1 on test set and plot the the curve of them for both SGD and Mini-batch GD on train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "eb577c9f-97b6-4984-8264-7aa811fec827",
      "metadata": {
        "id": "eb577c9f-97b6-4984-8264-7aa811fec827",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4301ae9-8dcc-4349-e61a-4dec5043b9f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mini-batch GD: (0.9951369337087279, 0.9951369337087279, 0.9951369337087279, 0.9951369337087279)\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 0.0391 - categorical_accuracy: 0.9900 - f1_score: 0.9900\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03908565640449524, 0.9900178909301758, 0.9900178909301758]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Calculate the metrics for test set and fill in the table below\n",
        "y_hat = model_mbgd.forward(x_test)\n",
        "y_pred = model_mbgd.predict(y_hat)\n",
        "print('Mini-batch GD:', get_metrics(y_pred, y_test))\n",
        "model_tf.evaluate(x=x_test, y=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "436dfacd-2ad2-41fa-8f23-f4cc86142fe0",
      "metadata": {
        "id": "436dfacd-2ad2-41fa-8f23-f4cc86142fe0"
      },
      "source": [
        "#### Evaluation Metrics on Test set\n",
        "Fill this table with the result you just printed (double click this cell to edit)\n",
        "\n",
        "|     Optimizer                     | Accuracy    | F1 Score    |\n",
        "|:---------------------------------:|-------------|-------------|\n",
        "|      **Your Implementation**      |  0.99513    |   0.99513   |\n",
        "| **Tensorflow**                     |  0.99001    |   0.99001   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fbbb31b-5452-4788-87fc-5d5c7eab144a",
      "metadata": {
        "id": "0fbbb31b-5452-4788-87fc-5d5c7eab144a"
      },
      "source": [
        "##### Please run the following cell to plot the training loss curve for Your implementation and Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "02501623-a585-426f-8e65-fb14704eb13c",
      "metadata": {
        "id": "02501623-a585-426f-8e65-fb14704eb13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "28ed5765-347f-4c1d-f3a9-079520bfbd60"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(4, 3))\n",
        "plt.plot(mbgd_train_history['loss'], label='My MLP')\n",
        "plt.plot(tf_history.history['loss'], label='TF MLP')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b8c5a95-45ab-49d6-b570-25534d15b3b2",
      "metadata": {
        "id": "1b8c5a95-45ab-49d6-b570-25534d15b3b2"
      },
      "source": [
        "##### Please run the following cell to plot the validation metrics curve for SGD and Mini-batch GD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "100f123d-2796-4e48-b204-912bad8c13e6",
      "metadata": {
        "id": "100f123d-2796-4e48-b204-912bad8c13e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "e0464886-9455-48b6-fc3d-14bfe891f551"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
        "ax = axes[0]\n",
        "ax.plot(mbgd_valid_history['accuracy'], label='My MLP')\n",
        "ax.plot(tf_history.history['val_categorical_accuracy'], label='TF MLP')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_title('Validation Accuracy')\n",
        "\n",
        "ax = axes[1]\n",
        "ax.plot(mbgd_valid_history['f1'], label='My MLP')\n",
        "ax.plot(tf_history.history['val_f1_score'], label='TF MLP')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_title('Validation F1')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cea05615-3aa9-4b17-89ee-40ef2eff0bcf",
      "metadata": {
        "id": "cea05615-3aa9-4b17-89ee-40ef2eff0bcf"
      },
      "source": [
        "# 3. Conclusion (5 Points)\n",
        "\n",
        "Provide an analysis for all the results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa38684-e334-4958-8fe9-ac98cfbc7133",
      "metadata": {
        "id": "efa38684-e334-4958-8fe9-ac98cfbc7133"
      },
      "source": [
        "Answer:\n",
        "\n",
        "**Adaptive learning** is an ability to learn how to do tasks based on the data given for training or initial experience.  MLP/Neural networks do not make any assumption regarding the underlying probability density functions or other probabilistic information about the pattern classes under consideration in\n",
        "comparison to other probability based models. They yield the required decision function directly via training. A two layer backpropagation network with sufficient hidden nodes has been proven to be a universal approximator as can be seen in the above implementation.\n",
        "\n",
        "Training model takes significant amount of time, it nearly took 20 min on a gpu, the reason to this behaviour is the weights have to updated at every iteration and for every batch and the parameters are updated in these iteration.\n",
        "\n",
        "It can be seen that the validation accuracy and F1 accuracy increases with more number of epochs. And the opposite can be observed with regards to training loss.\n",
        "\n",
        "we can significatly increase the performance of the model by optimising the parameters using AdaGrad. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "47476cac-f0da-4b92-aa09-8f57e61053e9"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}