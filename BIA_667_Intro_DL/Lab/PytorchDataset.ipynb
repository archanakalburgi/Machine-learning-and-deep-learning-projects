{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PytorchDataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jb0zD0S5BBa"
      },
      "source": [
        "# Pytorch Dataset\n",
        "Haohang Li  \n",
        "09/23/2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0aTwFy65Jvj"
      },
      "source": [
        "### Dependicies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCRZEkeZ49DA"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, random_split, Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aT1PJXZ5kVU"
      },
      "source": [
        "## Generate Fake Data\n",
        "\n",
        "For simplicity, we will use random number to represent our training featuers & label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhS26ytO5i-T"
      },
      "source": [
        "rng = np.random.default_rng()\n",
        "fake_features = rng.normal(size=12000).reshape(1000, 12)  # fake data, assume we have 1000 obs and 12 features\n",
        "fake_label = rng.integers(low=0, high=5, size=1000) # fake label: assume we have 5 different class: [0, 1, 2, 3, 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIsBPkU3BJTi",
        "outputId": "ff9c9709-a5e2-4208-89e3-2f8b55eeeb6e"
      },
      "source": [
        "torch.LongTensor(fake_label).size()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGetc8F_7UaC"
      },
      "source": [
        "## Define Dataset Class\n",
        "The dataset class should inherit from the pytorch's Dataset class, and we need to define:\n",
        "\n",
        "\n",
        "1.   `__init__`: Initialize your parent class and preprocess\n",
        "2.   `__getitem__`: Define how to retrieve your data by index, usually we return both data and corresponding label\n",
        "3.   `__len__`: Define how to get the total length of your data(how many observations/data points/rows) in your dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9nr8zKD7HZ5"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        super(MyDataset, self).__init__()\n",
        "        self.features = torch.Tensor(features)\n",
        "        self.labels = torch.LongTensor(labels)  # label shoul be (long) int (int64)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.features[index], self.labels[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.labels.size()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncHoyd7V-kDf"
      },
      "source": [
        "Create a dataset object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHVi3-Go-bVB"
      },
      "source": [
        "dataset_example = MyDataset(features=fake_features, labels=fake_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJtzE87e-pLu",
        "outputId": "5771a024-fc13-4f4c-a8e5-9b07a3418fdf"
      },
      "source": [
        "a_feature, a_label = dataset_example[0]\n",
        "print(f'A feature:\\n{a_feature}')\n",
        "print(f'A label:\\n{a_label}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A feature:\n",
            "tensor([-0.3835, -1.2055, -2.6225,  0.5530,  0.7653,  0.1032, -0.9299,  0.7310,\n",
            "        -1.2663,  0.0893,  0.9387,  0.7605])\n",
            "A label:\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruEsX7aA_PKT"
      },
      "source": [
        "Sometimes, your dataset might be too large and you my need to load and process it \"on the fly\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Xkfr85-q6d"
      },
      "source": [
        "class MyDataset_onthefly(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        super(MyDataset, self).__init__()\n",
        "        self.size = fake_label.shape[0]\n",
        "        # assume our total dataset is too large to load them at once to our memory\n",
        "        # this actually is a very common case when you deal with audio, image dataset\n",
        "        # it would be impossible for us to load 1 million images to our GPU memory\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        cur_features = torch.Tensor(fake_features[index]) # Instead of put everythin to memory, we load our data and process our data when we actually need it\n",
        "        cur_label = torch.Tensor(fake_label[index])\n",
        "        return cur_features, cur_label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyFw-UyVDdIy"
      },
      "source": [
        "## Split Dataset\n",
        "To split our dataset to train, validation, test datasets, we can use the `random_split` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYnpPd9vDqYF",
        "outputId": "495cc206-a5f2-47d8-a9f0-1ab54c05971b"
      },
      "source": [
        "split_size = (np.array([0.6, 0.2, 0.2]) * len(dataset_example)).astype(np.int)\n",
        "train_data, valid_data, test_data = random_split(dataset_example, lengths=split_size)\n",
        "\n",
        "print(f'Train dataset length: {len(train_data)}')\n",
        "print(f'Validation dataset length: {len(valid_data)}')\n",
        "print(f'Test dataset length: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset length: 600\n",
            "Validation dataset length: 200\n",
            "Test dataset length: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikZC9lkMDI9q"
      },
      "source": [
        "## Data Loader\n",
        "Convert to dataloader so we can use our data in train function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPa0mCmEDLV4",
        "outputId": "2c5a94ad-b86b-478a-f880-ba73ada716ee"
      },
      "source": [
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
        "\n",
        "# one batch example\n",
        "one_batch_features, one_batch_labels = next(train_loader.__iter__())\n",
        "print(one_batch_features.size())  # (batch x num_features)\n",
        "print(one_batch_labels.size())  # (batch x num_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 12])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    }
  ]
}