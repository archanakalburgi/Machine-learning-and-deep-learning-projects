{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"tf2","language":"python","name":"tf2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"Lab_5_gym_tutorial.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"A_L_0iHIr4Wg"},"source":["# <center> GYM Tutorial </center>"]},{"cell_type":"markdown","metadata":{"id":"LQOr2qr5r4Wm"},"source":["`OpenAI Gym` is a Python package comprising a selection of RL environments, ranging from simple “toy” environments to more challenging environments, including simulated robotics environments and Atari video game environments.\n","It was developed with the aim of becoming a standardized environment and benchmark for RL research. These environments have a shared interface, allowing you to write general algorithms."]},{"cell_type":"markdown","metadata":{"id":"lRSxGXh8r4Wn"},"source":["## Environments\n","\n","An environment is a class which defines the `observation` (a.k.a. `state`) and `action` spaces and a set of methods as follows:\n","- Attributes: `action_space` and `observation_space` define allowed states and actions within this environment\n","- Methods\n","    - `reset()`:  initialize an environment object and reset all states to the initial values\n","    - `render()`:  render the environment and show visualization if implemented\n","    - `step(action)`: An `action` is taken, and the environment changes states and return a reward\n","    - `close()`: destroy the environment object\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1-mao85-r4Wo"},"source":["## Demo: CartPole game ##"]},{"cell_type":"markdown","metadata":{"id":"xQ4Afd36r4Wo"},"source":["For details of CartPole game, check https://github.com/openai/gym/wiki/CartPole-v0\n","\n","**Observation (State)**:\n","\n","\n","|Num\t|Observation\t|Min\t|Max|\n","|:------:|:------------|---------:|-----:|\n","|0\t|Cart Position\t|-2.4|\t2.4|\n","|1\t|Cart Velocity|\t-Inf\t|Inf|\n","|2\t|Pole Angle\t|~ -41.8°|\t~ 41.8°|\n","|3\t|Pole Velocity At Tip\t|-Inf\t|Inf|\n","\n","**Actions**\n","\n","|Num\t|Action   |\n","|:------:|:------------|\n","|0\tPush |cart to the left|\n","|1\tPush |cart to the right|\n","\n","\n","Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n","\n","**Reward**\n","\n","Reward is 1 for every step taken, including the termination step."]},{"cell_type":"code","metadata":{"id":"DVJRLRgPr4Wp"},"source":["import gym"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxO8ar9Sr4Wq","outputId":"82bfd2de-7c40-47f8-ad73-2896a1128057"},"source":["env = gym.make('CartPole-v0')\n","env.reset()  # sample a random state as an initial state\n","env.render()\n","#env.close()"],"execution_count":null,"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"Ctg4_5bWr4Ws"},"source":["## Understanding `space`  objects\n","\n","The environments in gym have `Space` objects which describe the valid actions and observations. Every environment comes with an action_space and an observation_space. These attributes are of type Space, and they describe the format of valid actions and observations.\n","\n","Typically, there are two types of spaces:\n","\n","- `Discrete` space allows a fixed range of non-negative numbers. `Discrete(3)` means that the action can take values `{0, 1, 2}`.\n","- `Box` space represents an n-dimensional box, so valid observations will be an array of numbers. You can also check the Box’s bounds."]},{"cell_type":"code","metadata":{"id":"0QLVcfx3r4Wt","outputId":"1cf09868-3c2f-47a4-905b-22dd59d3a29a"},"source":["print(env.action_space)\n","print(env.observation_space)\n","print(env.observation_space.high)\n","print(env.observation_space.low)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Discrete(2)\n","Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n","[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n","[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"]}]},{"cell_type":"code","metadata":{"id":"19CCUxInr4Wt","outputId":"767fee13-d91e-4b42-f6ea-71281651b549"},"source":["# Sample action or observation\n","\n","print(env.observation_space.sample())\n","print(env.action_space.sample())"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[ 3.0832727e+00 -8.5805008e+37  9.8104283e-02  3.1468467e+38]\n","1\n"]}]},{"cell_type":"markdown","metadata":{"id":"vbDlVzmDr4Wu"},"source":["## Understanding env.step"]},{"cell_type":"markdown","metadata":{"id":"kqm6j7pQr4Wu"},"source":["As mentioned in the documentation page, each environment is separated into different episodes, with `done=True` indicating that the specific episode has ended. Thus, we need to call reset there. For this, we need to understand what `env.step(action)` does and returns. `env.step(action)` takes the next step in the environment by performing the action specified by `action` and returns a tuple:\n","- **observation**: This is environment specific and represents our observation of the environment after taking the action specified in `env.step(action)`.\n","- **reward**: The reward we received upon performing the action.\n","- **done**: This is the parameter we discussed about. We need to monitor this and call `env.reset()` when `done=True`.\n","- info: Additional information for debugging"]},{"cell_type":"markdown","metadata":{"id":"IlkDjWL5r4Wv"},"source":["Here’s a bare minimum example of getting something running. This will run an instance of the CartPole-v0 environment for 1000 timesteps, rendering the environment at each step. You should see a window pop up rendering the classic cart-pole problem:"]},{"cell_type":"code","metadata":{"id":"Sh1SwDvVr4Wv","outputId":"3309cfc5-76eb-410c-98d3-91bd4f5ab993"},"source":["from time import sleep\n","env = gym.make('CartPole-v0')\n","for i_episode in range(5):\n","    print(\"\\n episode: \", i_episode)\n","    observation = env.reset()\n","    for t in range(100):\n","        env.render()\n","        sleep(0.03)  \n","        action = env.action_space.sample()\n","        #print(t, observation, action)\n","        observation, reward, done, info = env.step(action)\n","        \n","        if done:\n","            print('Episode #%d finished after %d timesteps' % (i_episode, t))\n","            \n","            break\n","env.close()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," episode:  0\n","Episode #0 finished after 16 timesteps\n","\n"," episode:  1\n","Episode #1 finished after 21 timesteps\n","\n"," episode:  2\n","Episode #2 finished after 20 timesteps\n","\n"," episode:  3\n","Episode #3 finished after 12 timesteps\n","\n"," episode:  4\n","Episode #4 finished after 17 timesteps\n"]}]},{"cell_type":"code","metadata":{"id":"414rIQMUr4Ww"},"source":[""],"execution_count":null,"outputs":[]}]}